[{"content":"Nutanix NKP v2.13.1 Install with private registry NKP 安裝 Lab，安裝在 AHV 環境上 ，透過 cli 方式安裝\n會建立 image registry 來放置 NKP 需要的 image，建置完成後會升級到 2.14\n環境資訊 版本 Prism Central: 2024.2.0.3\nAOS: 6.5.1.1\nNKP: 2.13.1\nKubernetes: 1.30.5\nOS Image: ubuntu-22.04\nKIB: v2.18.0\nIP 與名稱 Prism Central: https://10.38.14.10:9440\nPrism Element (PHX-SPOC014-1): https://10.38.14.7:9440/\nbastion ip : 10.38.14.24\nStorage Container: default\nSubnet: primary\nkubeVIP: 10.38.14.51\nkube Ingress ip Range: 10.38.14.52-10.38.14.54\nSubnet Mask: 255.255.255.192\nGateway IP Address: 10.38.14.1\nBastion 軟體下載 nkp-airgapped, nkp cli , kib 從 Portal 上直接用 copy link 方式下載\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [nkp@nkp-bastion ~]$ curl -o nkp-air-gapped-bundle_v2.13.1_linux_amd64.tar.gz \u0026#34;https://download.nutanix.com/downloads/nkp/v2.13.1/nkp-air-gapped-bundle_v2.13.1_linux_amd64.tar.gz?Expires=1740156924\u0026amp;Key-Pair-xx__\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 14.3G 100 14.3G 0 0 317M 0 0:00:46 0:00:46 --:--:-- 289M [nkp@nkp-bastion ~]$ curl -o nkp_v2.13.1_linux_amd64.tar.gz \u0026#34;https://download.nutanix.com/downloads/nkp/v2.13.1/nkp_v2.13.1_linux_amd64.tar.gz?Expires=1740156983\u0026amp;Key-Pair-xx__\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 97.5M 100 97.5M 0 0 16.5M 0 0:00:05 0:00:05 --:--:-- 17.5M [nkp@nkp-bastion ~]$ curl -o konvoy-image-bundle-v2.18.0_linux_amd64.tar.gz \u0026#34;https://download.nutanix.com/downloads/nkp/v2.13.1/konvoy-image-bundle-v2.18.0_linux_amd64.tar.gz?Expires=1740157007\u0026amp;Key-Pair-xxx__\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 334M 100 334M 0 0 15.7M 0 0:00:21 0:00:21 --:--:-- 15.9M [nkp@nkp-bastion ~]$ ll total 15484132 -rw-r--r--. 1 nkp nkp 350719341 Feb 20 22:57 konvoy-image-bundle-v2.18.0_linux_amd64.tar.gz -rw-r--r--. 1 nkp nkp 15402765171 Feb 20 22:56 nkp-air-gapped-bundle_v2.13.1_linux_amd64.tar.gz -rw-r--r--. 1 nkp nkp 102264118 Feb 20 22:56 nkp_v2.13.1_linux_amd64.tar.gz 軟體安裝 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [nkp@nkp-bastion ~]$ sudo yum install podman yum-utils bzip2 wget tar -y [nkp@nkp-bastion ~]$ sudo yum install git-all -y [nkp@nkp-bastion ~]$ wget https://get.helm.sh/helm-v3.17.1-linux-amd64.tar.gz [nkp@nkp-bastion ~]$ sudo cp linux-amd64/helm /usr/local/bin [nkp@nkp-bastion ~]$ mkdir -p nkptools/nkp [nkp@nkp-bastion ~]$ mkdir -p nkptools/kib [nkp@nkp-bastion ~]$ mkdir -p nkptools/nkp-airgap [nkp@nkp-bastion ~]$ tar xvf nkp_v2.13.1_linux_amd64.tar.gz -C nkptools/nkp [nkp@nkp-bastion ~]$ tar xvf konvoy-image-bundle-v2.18.0_linux_amd64.tar.gz -C nkptools/kib [nkp@nkp-bastion ~]$ tar xvf nkp-air-gapped-bundle_v2.13.1_linux_amd64.tar.gz -C nkptools/nkp-airgap [nkp@nkp-bastion ~]$ sudo cp nkptools/nkp/nkp /usr/local/bin/ [nkp@nkp-bastion ~]$ sudo cp nkptools/kib/konvoy-image /usr/local/bin/ [nkp@nkp-bastion ~]$ sudo cp nkptools/nkp-airgap/nkp-v2.13.1/kubectl /usr/local/bin/ [nkp@nkp-bastion ~]$ nkp version diagnose: v0.10.1 imagebuilder: v0.20.0 kommander: v2.13.1 konvoy: v2.13.1 mindthegap: v1.16.0 nkp: v2.13.1 [nkp@nkp-bastion ~]$ kubectl version Client Version: v1.30.5 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 The connection to the server localhost:8080 was refused - did you specify the right host or port? 基本設定 disable firewalld, selinux\n1 2 3 4 5 6 7 8 9 10 11 [nkp@nkp-bastion ~]$ sudo systemctl disable firewalld --now Removed \u0026#34;/etc/systemd/system/multi-user.target.wants/firewalld.service\u0026#34;. Removed \u0026#34;/etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service\u0026#34;. [nkp@nkp-bastion ~]$ sudo vi /etc/selinux/config SELINUXTYPE=disabled reboot [nkp@nkp-bastion ~]$ getenforce Disabled Registry 啟動 Registry 1 2 3 4 5 6 7 8 9 10 \u0026#34;switch to root\u0026#34; (方便使用，也可以用一般使用者，但要另外設定) [root@nkp-bastion ~] podman login registry-1.docker.io \u0026#34;Start registry\u0026#34; [root@nkp-bastion ~] podman run -d -p 5000:5000 --restart always --name nkp-registry registry:latest [root@nkp-bastion ~] export BOOTSTRAP_HOST_IP=10.38.14.24 \u0026gt;\u0026gt; .bashrc [root@nkp-bastion ~] echo \u0026#34;export REGISTRY_URL=http://10.38.14.24:5000\u0026#34; \u0026gt;\u0026gt; .bashrc [root@nkp-bastion ~] source .bashrc 推送 NKP Image 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026#34;Push Image\u0026#34; [root@nkp-bastion ~] nkp push bundle --bundle /home/nkp/nkptools/nkp-airgap/nkp-v2.13.1/container-images/kommander-image-bundle-v2.13.1.tar --to-registry=$REGISTRY_URL --to-registry-insecure-skip-tls-verify ✓ Creating temporary directory ✓ Unarchiving image bundle \u0026#34;/home/nkp/nkptools/nkp-airgap/nkp-v2.13.1/container-images/kommander-image-bundle-v2.13.1.tar\u0026#34; ✓ Parsing image bundle config ✓ Starting temporary Docker registry ✓ Pushing bundled images [================================\u0026gt;123/123] (time elapsed 57s) [root@nkp-bastion ~] nkp push bundle --bundle /home/nkp/nkptools/nkp-airgap/nkp-v2.13.1/container-images/konvoy-image-bundle-v2.13.1.tar --to-registry=$REGISTRY_URL --to-registry-insecure-skip-tls-verify ✓ Creating temporary directory ✓ Unarchiving image bundle \u0026#34;/home/nkp/nkptools/nkp-airgap/nkp-v2.13.1/container-images/konvoy-image-bundle-v2.13.1.tar\u0026#34; ✓ Parsing image bundle config ✓ Starting temporary Docker registry ✓ Pushing bundled images [================================\u0026gt;115/115] (time elapsed 35s) [root@nkp-bastion ~] nkp push bundle --bundle /home/nkp/nkptools/nkp-airgap/nkp-v2.13.1/container-images/nkp-catalog-applications-image-bundle-v2.13.1.tar --to-registry=$REGISTRY_URL --to-registry-insecure-skip-tls-verify ✓ Creating temporary directory ✓ Unarchiving image bundle \u0026#34;/home/nkp/nkptools/nkp-airgap/nkp-v2.13.1/container-images/nkp-catalog-applications-image-bundle-v2.13.1.tar\u0026#34; ✓ Parsing image bundle config ✓ Starting temporary Docker registry ✓ Pushing bundled images [====================================\u0026gt;8/8] (time elapsed 04s) load image 1 2 3 4 5 6 7 8 podman load -i /home/nkp/nkptools/nkp-airgap/nkp-v2.13.1/konvoy-bootstrap-image-v2.13.1.tar podman load -i /home/nkp/nkptools/nkp-airgap/nkp-v2.13.1/nkp-image-builder-image-v0.20.0.tar [root@nkp-bastion ~] podman images REPOSITORY TAG IMAGE ID CREATED SIZE localhost/mesosphere/konvoy-bootstrap v2.13.1 e7b2ed9a825c 5 weeks ago 2.13 GB docker.io/library/registry latest 26b2eb03618e 16 months ago 26 MB localhost/mesosphere/nkp-image-builder v0.20.0 f8ce7f154c3b 45 years ago 779 MB Create Cluster 上述環境、Bastion 準備完成後，可以開始佈建 NKP Management Cluster\nBootstrap Cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@nkp-bastion ~] nkp create bootstrap ✓ Creating a bootstrap cluster ✓ Initializing new CAPI components ✓ Initializing new CAPI components ✓ Creating ClusterClass resources [root@nkp-bastion ~] kubectl get nodes NAME STATUS ROLES AGE VERSION konvoy-capi-bootstrapper-control-plane Ready control-plane 2m27s v1.30.5 [root@nkp-bastion ~] kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE caaph-system caaph-controller-manager-5c4b55f8b6-n6l5j 1/1 Running 0 93s capa-system capa-controller-manager-6c9654f67d-4cr6z 1/1 Running 0 108s capg-system capg-controller-manager-6f85f89dbc-z4qgv 1/1 Running 0 96s capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-679764b5b7-f5558 1/1 Running 0 111s capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-6db6bbdd85-zbhkg 1/1 Running 0 110s ... Machine Image (if required) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 echo \u0026#34;export NUTANIX_USER=admin\u0026#34; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#34;export NUTANIX_PASSWORD=nx2Tech702!\u0026#34; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#34;export PE_CLUSTER_NAME=PHX-SPOC014-1\u0026#34; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#34;export PC_ENDPOINT=10.38.14.10\u0026#34; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#34;export SUBNET=primary\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc [root@nkp-bastion ~] env REGISTRY_URL=http://10.38.14.24:5000 PE_CLUSTER_NAME=PHX-SPOC014-1 NUTANIX_USER=admin PC_ENDPOINT=10.38.14.10 BOOTSTRAP_HOST_IP=10.38.14.24 ... [root@nkp-bastion ~] nkp create image nutanix ubuntu-22.04 --cluster=$PE_CLUSTER_NAME --endpoint=$PC_ENDPOINT --subnet=$SUBNET --insecure ==\u0026gt; Wait completed after 6 minutes 33 seconds ==\u0026gt; Builds finished. The artifacts of successful builds are: --\u0026gt; nutanix.kib_image: nkp-ubuntu-22.04-1.30.5-20250221084439 --\u0026gt; nutanix.kib_image: nkp-ubuntu-22.04-1.30.5-20250221084439 --\u0026gt; nutanix.kib_image: nkp-ubuntu-22.04-1.30.5-20250221084439 echo \u0026#34;export VM_IMAGE=nkp-ubuntu-22.04-1.30.5-20250221084439\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc Management Cluster Env 設定環境變數 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 env.sh export MGMT_CLUSTER_NAME=nkp-poc export PE_CLUSTER_NAME=PHX-SPOC014-1 export SUBNET=primary export MGMT_CP_ENDPOINT=10.38.14.51 export VM_IMAGE=nkp-ubuntu-22.04-1.30.5-20250221084439 export STORAGE_CONTAINER=default export MGMT_LB_IP_RANGE=10.38.14.52-10.38.14.54 export NUTANIX_USER=admin export NUTANIX_PASSWORD=nx2Tech702! export PC_ENDPOINT=10.38.14.10 export CSI_HYPERVISOR_ATTACHED=\u0026#34;true\u0026#34; export CSI_FILESYSTEM=\u0026#34;ext4\u0026#34; export CONTROL_PLANE_IP=\u0026#34;10.38.14.51\u0026#34; export SSH_PUBLIC_KEY=\u0026#34;/home/nkp/.ssh/id_rsa.pub\u0026#34; export REGISTRY_URL=\u0026#34;http://10.38.14.24:5000\u0026#34; source env.sh Create Cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 nkp create cluster nutanix \\ --cluster-name=$MGMT_CLUSTER_NAME \\ --control-plane-prism-element-cluster=$PE_CLUSTER_NAME \\ --control-plane-subnets=$SUBNET \\ --control-plane-endpoint-ip=$MGMT_CP_ENDPOINT \\ --control-plane-vm-image=$VM_IMAGE \\ --worker-prism-element-cluster=$PE_CLUSTER_NAME \\ --worker-subnets=$SUBNET \\ --worker-vm-image=$VM_IMAGE \\ --worker-memory=16 \\ --csi-storage-container=$STORAGE_CONTAINER \\ --kubernetes-service-load-balancer-ip-range=$MGMT_LB_IP_RANGE \\ --endpoint=\u0026#34;https://$PC_ENDPOINT:9440\u0026#34; \\ --insecure=true \\ --registry-mirror-url=$REGISTRY_URL \\ --ssh-public-key-file=$SSH_PUBLIC_KEY \\ --dry-run \\ --output=yaml \u0026gt; deploy-nkp-$MGMT_CLUSTER_NAME.yaml [root@nkp-bastion ~] ll total 16 -rw-------. 1 root root 997 Feb 20 22:43 anaconda-ks.cfg -rw-r--r--. 1 root root 5913 Feb 21 01:20 deploy-nkp-nkp-poc.yaml -rwxr-xr-x. 1 root root 586 Feb 21 01:17 env.sh [root@nkp-bastion ~] kubectl create -f deploy-nkp-nkp-poc.yaml cluster.cluster.x-k8s.io/nkp-poc created secret/nkp-poc-pc-credentials created secret/nkp-poc-pc-credentials-for-csi created configmap/kommander-bootstrap-configuration created secret/prism-central-metadata created secret/global-nutanix-credentials created watch -n 5 nkp describe cluster --cluster-name $MGMT_CLUSTER_NAME 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [root@nkp-bastion ~] nkp get kubeconfig --cluster-name nkp-poc \u0026gt; nkp-poc.conf [root@nkp-bastion ~] chmod 600 nkp-poc.conf [root@nkp-bastion ~] ll total 32 -rw-------. 1 root root 997 Feb 20 22:43 anaconda-ks.cfg -rw-r--r--. 1 root root 5913 Feb 21 01:20 deploy-nkp-nkp-poc.yaml -rw-r--r--. 1 root root 5913 Feb 21 01:21 deploy-nkp-nkp-poc.yaml_bk -rwxr-xr-x. 1 root root 586 Feb 21 01:17 env.sh -rw-------. 1 root root 5543 Feb 21 01:28 nkp-poc.conf echo \u0026#34;export KUBECONFIG=~/nkp-poc.conf\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source .bashrc [root@nkp-bastion ~] kubectl get nodes NAME STATUS ROLES AGE VERSION nkp-poc-d78fs-dkfnd Ready control-plane 5m43s v1.30.5 nkp-poc-d78fs-gmllf Ready control-plane 8m8s v1.30.5 nkp-poc-d78fs-x989r Ready control-plane 4m25s v1.30.5 nkp-poc-md-0-clx8j-g5pnb-4jbfm Ready \u0026lt;none\u0026gt; 6m17s v1.30.5 nkp-poc-md-0-clx8j-g5pnb-cxlx7 Ready \u0026lt;none\u0026gt; 6m40s v1.30.5 nkp-poc-md-0-clx8j-g5pnb-kwvlj Ready \u0026lt;none\u0026gt; 6m15s v1.30.5 nkp-poc-md-0-clx8j-g5pnb-m9dkw Ready \u0026lt;none\u0026gt; 6m18s v1.30.5 \u0026#34;建立 capi-components\u0026#34; [root@nkp-bastion ~] nkp create capi-components ✓ Initializing new CAPI components ✓ Initializing new CAPI components ✓ Creating ClusterClass resources Delete bootstrap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 \u0026#34;從 bootstratp 移轉 capi-resources\u0026#34; [root@nkp-bastion ~] unset KUBECONFIG [root@nkp-bastion ~] kubectl get nodes NAME STATUS ROLES AGE VERSION konvoy-capi-bootstrapper-control-plane Ready control-plane 60m v1.30.5 [root@nkp-bastion ~] nkp move capi-resources --to-kubeconfig nkp-poc.conf ✓ Moving cluster resources You can now view resources in the moved cluster by using the --kubeconfig flag with kubectl. For example: kubectl --kubeconfig=\u0026#34;nkp-poc.conf\u0026#34; get nodes [root@nkp-bastion ~] nkp delete bootstrap ✓ Deleting bootstrap cluster [root@nkp-bastion ~] source ~/.bashrc [root@nkp-bastion ~] kubectl get nodes NAME STATUS ROLES AGE VERSION nkp-poc-d78fs-dkfnd Ready control-plane 12m v1.30.5 nkp-poc-d78fs-gmllf Ready control-plane 14m v1.30.5 nkp-poc-d78fs-x989r Ready control-plane 10m v1.30.5 nkp-poc-md-0-clx8j-g5pnb-4jbfm Ready \u0026lt;none\u0026gt; 12m v1.30.5 nkp-poc-md-0-clx8j-g5pnb-cxlx7 Ready \u0026lt;none\u0026gt; 13m v1.30.5 nkp-poc-md-0-clx8j-g5pnb-kwvlj Ready \u0026lt;none\u0026gt; 12m v1.30.5 nkp-poc-md-0-clx8j-g5pnb-m9dkw Ready \u0026lt;none\u0026gt; 12m v1.30.5 [root@nkp-bastion ~] kubectl get ns NAME STATUS AGE caaph-system Active 4m56s capa-system Active 5m9s capg-system Active 4m59s capi-kubeadm-bootstrap-system Active 5m10s capi-kubeadm-control-plane-system Active 5m9s capi-system Active 5m11s cappp-system Active 5m1s capv-system Active 5m capvcd-system Active 4m58s capx-system Active 4m57s capz-system Active 5m7s caren-system Active 3m50s cert-manager Active 5m23s default Active 14m git-operator-system Active 88s kommander Active 13m kommander-flux Active 109s kube-node-lease Active 14m kube-public Active 14m kube-system Active 14m metallb-system Active 14m node-feature-discovery Active 13m ntnx-system Active 14m 取得 Dashboard 1 2 3 4 5 6 7 watch kubectl get pods -n kommander [root@nkp-bastion ~] nkp get dashboard Username: clever_euclid Password: 8JW63deYbPpIIrCtTPBXJihIF8xsWxw0YTCSVvcAa1SupyZ14GqEEVFGXzAb7ZB7 URL: https://10.38.14.52/dkp/kommander/dashboard 串接LDAP、License匯入完成後登入畫面\n其他叢集資訊 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 \u0026#34;叢集資訊\u0026#34; [root@nkp-bastion ~] kubectl get namespace kube-system --output jsonpath={.metadata.uid} 7e1c859a-0d30-46a3-83d5-21398e4d7cc4 [root@nkp-bastion ~] kubectl get cluster NAME CLUSTERCLASS PHASE AGE VERSION nkp-poc nkp-nutanix Provisioned 2d20h v1.30.5 \u0026#34;POC License Key\u0026#34; # AEAAN-AAAYY-T9A7V-5UL6S-49UQ2-H89DG-EP2AR [root@nkp-bastion ~] kubectl describe cluster nkp-poc Name: nkp-poc Namespace: default Labels: cluster.x-k8s.io/cluster-name=nkp-poc cluster.x-k8s.io/provider=nutanix konvoy.d2iq.io/cluster-name=nkp-poc konvoy.d2iq.io/provider=nutanix topology.cluster.x-k8s.io/owned= Annotations: caren.nutanix.com/cluster-uuid: 019527cf-0ec5-7b3b-b4aa-3f50c7dc3b4f API Version: cluster.x-k8s.io/v1beta1 Kind: Cluster Metadata: Creation Timestamp: 2025-02-21T09:36:01Z Finalizers: cluster.cluster.x-k8s.io Generation: 2 Resource Version: 10471 UID: 4236d3d9-3d77-4c0b-8494-e67dfe774f92 Spec: Cluster Network: Pods: Cidr Blocks: 192.168.0.0/16 Services: Cidr Blocks: 10.96.0.0/12 Control Plane Endpoint: Host: 10.38.14.51 Port: 6443 Control Plane Ref: API Version: controlplane.cluster.x-k8s.io/v1beta1 Kind: KubeadmControlPlane Name: nkp-poc-d78fs Namespace: default Infrastructure Ref: API Version: infrastructure.cluster.x-k8s.io/v1beta1 Kind: NutanixCluster Name: nkp-poc-5cvwf Namespace: default Topology: Class: nkp-nutanix Control Plane: Metadata: Replicas: 3 Variables: Name: clusterConfig Value: Addons: Ccm: Credentials: Secret Ref: Name: nkp-poc-pc-credentials Strategy: HelmAddon Cluster Autoscaler: Strategy: HelmAddon Cni: Provider: Cilium Strategy: HelmAddon Csi: Default Storage: Provider: nutanix Storage Class Config: volume Providers: Nutanix: Credentials: Secret Ref: Name: nkp-poc-pc-credentials-for-csi Storage Class Configs: Volume: Allow Expansion: true Parameters: csi.storage.k8s.io/fstype: ext4 Description: CSI StorageClass nutanix-volume for nkp-poc Flash Mode: DISABLED Hypervisor Attached: ENABLED Storage Container: default Storage Type: NutanixVolumes Reclaim Policy: Delete Volume Binding Mode: WaitForFirstConsumer Strategy: HelmAddon Snapshot Controller: Strategy: HelmAddon Nfd: Strategy: HelmAddon Service Load Balancer: Configuration: Address Ranges: End: 10.38.14.54 Start: 10.38.14.52 Provider: MetalLB Control Plane: Nutanix: Machine Details: Boot Type: uefi Cluster: Name: PHX-SPOC014-1 Type: name Image: Name: nkp-ubuntu-22.04-1.30.5-20250221084439 Type: name Memory Size: 16Gi Subnets: Name: primary Type: name System Disk Size: 80Gi Vcpu Sockets: 4 Vcpus Per Socket: 1 Dns: Core DNS: Encryption At Rest: Providers: Secretbox: Global Image Registry Mirror: URL: http://10.38.14.24:5000 Nutanix: Control Plane Endpoint: Host: 10.38.14.51 Port: 6443 Virtual IP: Provider: KubeVIP Prism Central Endpoint: Credentials: Secret Ref: Name: nkp-poc-pc-credentials Insecure: true URL: https://10.38.14.10:9440 Version: v1.30.5 Workers: Machine Deployments: Class: default-worker Metadata: Annotations: cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: 4 cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: 4 Name: md-0 Variables: Overrides: Name: workerConfig Value: Nutanix: Machine Details: Boot Type: uefi Cluster: Name: PHX-SPOC014-1 Type: name Image: Name: nkp-ubuntu-22.04-1.30.5-20250221084439 Type: name Memory Size: 16Gi Subnets: Name: primary Type: name System Disk Size: 80Gi Vcpu Sockets: 8 Vcpus Per Socket: 1 Status: Conditions: Last Transition Time: 2025-02-21T09:36:14Z Status: True Type: Ready Last Transition Time: 2025-02-21T09:36:10Z Status: True Type: ControlPlaneInitialized Last Transition Time: 2025-02-21T09:36:14Z Status: True Type: ControlPlaneReady Last Transition Time: 2025-02-21T09:36:11Z Status: True Type: InfrastructureReady Last Transition Time: 2025-02-21T09:37:32Z Status: True Type: KommanderInitialized Last Transition Time: 2025-02-21T09:36:13Z Status: True Type: TopologyReconciled Control Plane Ready: true Infrastructure Ready: true Observed Generation: 2 Phase: Provisioned Events: \u0026lt;none\u0026gt; Upgrade NKP 目前版本 NKP : v2.13.1 預計升級到 v2.14\nKubernetes: v1.30.5 預計升級到 v1.31.4\nOS image : Ubuntu 22.04 , Kernel 5.15.0-119-generic\nKiB : 2.18 預計升級到 2.22.2\n升級前先確認 NKP Release Notes 更新的項目及版本是否會影響現有系統及應用程式 用 nkp version 指令確認 NKP 版本 確認版本相容性 Upgrade Compatibility Tables 重要系統套用 Deploy Pod Disruption Budget (PDB) https://kubernetes.io/docs/concepts/workloads/pods/disruptions/ 使用 Velero 備份現有設定 Upgrade Path Upgrade Order (Nutanix) 先升級 Management Cluster 再升級 Workload Cluster\nBastion 升級 nkp, kib 指令版本 下載 nkp-airgapped bundle 包並升級 kubectl 上傳 Kommander, Konvoy, Catalog Application (Ultimate) 等 image 到 private registry 製作或下載新版本的 OS image ( 請查看版本相容性確認是否需要更新 ) 升級 Kommander 升級 Cluster 並指定 OS Image 在 Nutanix 環境此步驟會直接升級 Kubernetes 版本 Upgrade NKP PRO 1. Bastion 指令升級及bundle 檔案從 Support Portal copy link下載\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [nkp@nkp-bastion nkp-2.14]$ pwd /home/nkp/nkp-2.14 [nkp@nkp-bastion nkp-2.14]$ curl -o nkp_v2.14.0_linux_amd64.tar.gz \u0026#34;https://download.nutanix.com/downloads/nkp/v2.14.0/nkp_v2.14.0_linux_amd64.tar.gz?Expires=17xxxxxQ__\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 90.5M 100 90.5M 0 0 319M 0 --:--:-- --:--:-- --:--:-- 319M [nkp@nkp-bastion nkp-2.14]$ curl -o konvoy-image-bundle-v2.22.2_linux_amd64.tar.gz \u0026#34;https://download.nutanix.com/downloads/nkp/v2.14.0/konvoy-image-bundle-v2.22.2_linux_amd64.tar.gz?Expires=174127xxxxxx__\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 330M 100 330M 0 0 16.1M 0 0:00:20 0:00:20 --:--:-- 15.8M [nkp@nkp-bastion nkp-2.14]$ ls -l total 16674272 -rw-r--r--. 1 nkp nkp 346426397 Mar 5 22:47 konvoy-image-bundle-v2.22.2_linux_amd64.tar.gz -rw-r--r--. 1 nkp nkp 94896342 Mar 5 22:45 nkp_v2.14.0_linux_amd64.tar.gz 解壓縮\n1 2 3 [nkp@nkp-bastion nkp-2.14]$ tar xvf konvoy-image-bundle-v2.22.2_linux_amd64.tar.gz -C ../nkptools/kib [nkp@nkp-bastion nkp-2.14]$ tar xvf nkp_v2.14.0_linux_amd64.tar.gz -C ../nkptools/nkp 將指令複製到 /usr/local/bin\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [nkp@nkp-bastion ~]$ sudo cp nkptools/nkp/nkp /usr/local/bin/ [nkp@nkp-bastion ~]$ sudo cp nkptools/kib/konvoy-image /usr/local/bin/ [nkp@nkp-bastion ~]$ nkp version diagnose: v0.10.1 imagebuilder: v0.22.3 kommander: v2.14.0 konvoy: v2.14.0 mindthegap: v1.16.0 nkp: v2.14.0 [nkp@nkp-bastion ~]$ konvoy-image version Writing manifest to image destination Loaded image: localhost/mesosphere/konvoy-image-builder:v2.22.2 konvoy-image, version v2.22.2 (branch: , revision: ) build date: go version: go1.22.12 platform: linux/amd64 2. 下載 airgapped 包 1 2 3 4 5 6 7 8 9 10 11 12 [nkp@nkp-bastion nkp-2.14]$ curl -o nkp-air-gapped-bundle_v2.14.0_linux_amd64.tar.gz \u0026#34;https://download.nutanix.com/downloads/nkp/v2.14.0/nkp-air-gapped-bundle_v2.14.0_linux_amd64.tar.gz?Expires=174127xxxxx\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 15.4G 100 15.4G 0 0 17.3M 0 0:15:15 0:15:15 --:--:-- 15.1M [nkp@nkp-bastion nkp-2.14]$ tar xvf nkp-air-gapped-bundle_v2.14.0_linux_amd64.tar.gz -C ../nkptools/nkp-airgap [nkp@nkp-bastion ~]$ sudo cp nkptools/nkp-airgap/nkp-v2.14.0/kubectl /usr/local/bin/ [nkp@nkp-bastion ~]$ kubectl version Client Version: v1.31.4 Kustomize Version: v5.4.2 Server Version: v1.30.5 3. 上傳 Image \u0026ldquo;Switch to root\u0026rdquo; 環境變數確認\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [root@nkp-bastion ~]# echo \u0026#34;export REGISTRY_URL=http://10.38.14.24:5000\u0026#34; \u0026gt;\u0026gt; .bashrc [root@nkp-bastion ~]# source .bashrc [root@nkp-bastion ~]# echo $REGISTRY_URL http://10.38.14.24:5000 [root@nkp-bastion ~]# echo \u0026#34;NUTANIX_USERNAME=admin\u0026#34; \u0026gt;\u0026gt; .bashrc [root@nkp-bastion ~]# echo \u0026#34;NUTANIX_PASSWORD=nx2Tech702!\u0026#34; \u0026gt;\u0026gt; .bashrc [root@nkp-bastion ~]# echo \u0026#34;export PE_CLUSTER_NAME=PHX-SPOC014-1\u0026#34; \u0026gt;\u0026gt; ~/.bashrc [root@nkp-bastion ~]# echo \u0026#34;export PC_ENDPOINT=10.38.14.10\u0026#34; \u0026gt;\u0026gt; ~/.bashrc [root@nkp-bastion ~]# echo \u0026#34;export SUBNET=primary\u0026#34; \u0026gt;\u0026gt; ~/.bashrc [root@nkp-bastion ~]# source .bashrc [root@nkp-bastion ~]# ll /home/nkp/nkptools/nkp-airgap/nkp-v2.14.0/container-images/ total 12847076 -rw-r--r--. 1 nkp nkp 81 Mar 4 01:08 NOTICES.txt -rw-r--r--. 1 nkp nkp 8202238464 Mar 4 01:08 kommander-image-bundle-v2.14.0.tar -rw-r--r--. 1 nkp nkp 4069966848 Mar 4 02:46 konvoy-image-bundle-v2.14.0.tar -rw-r--r--. 1 nkp nkp 883191296 Mar 4 00:46 nkp-catalog-applications-image-bundle-v2.14.0.tar 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u0026#34;Push Image\u0026#34; # Kommander [root@nkp-bastion ~] nkp push bundle --bundle /home/nkp/nkptools/nkp-airgap/nkp-v2.14.0/container-images/kommander-image-bundle-v2.14.0.tar --to-registry=$REGISTRY_URL --to-registry-insecure-skip-tls-verify ✓ Creating temporary directory ✓ Unarchiving image bundle \u0026#34;/home/nkp/nkptools/nkp-airgap/nkp-v2.14.0/container-images/kommander-image-bundle-v2.14.0.tar\u0026#34; ✓ Parsing image bundle config ✓ Starting temporary Docker registry ✓ Pushing bundled images [================================\u0026gt;137/137] (time elapsed 1m02s) # Konvoy [root@nkp-bastion ~] nkp push bundle --bundle /home/nkp/nkptools/nkp-airgap/nkp-v2.14.0/container-images/konvoy-image-bundle-v2.14.0.tar --to-registry=$REGISTRY_URL --to-registry-insecure-skip-tls-verify ✓ Creating temporary directory ✓ Unarchiving image bundle \u0026#34;/home/nkp/nkptools/nkp-airgap/nkp-v2.14.0/container-images/konvoy-image-bundle-v2.14.0.tar\u0026#34; ✓ Parsing image bundle config ✓ Starting temporary Docker registry ✓ Pushing bundled images [================================\u0026gt;119/119] (time elapsed 28s) [root@nkp-bastion ~]# # Catalog [root@nkp-bastion ~] nkp push bundle --bundle /home/nkp/nkptools/nkp-airgap/nkp-v2.14.0/container-images/nkp-catalog-applications-image-bundle-v2.14.0.tar --to-registry=$REGISTRY_URL --to-registry-insecure-skip-tls-verify ✓ Creating temporary directory ✓ Unarchiving image bundle \u0026#34;/home/nkp/nkptools/nkp-airgap/nkp-v2.14.0/container-images/nkp-catalog-applications-image-bundle-v2.14.0.tar\u0026#34; ✓ Parsing image bundle config ✓ Starting temporary Docker registry ✓ Pushing bundled images [====================================\u0026gt;8/8] (time elapsed 00s) 4. 製作 OS Image For Rocky 直接在 Prism Central 新增 Support Portal 上的 Image\nAdd Image\n貼上 URL 並點選 Add URL\n點選 save\nFor Others (ubuntu) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 [root@nkp-bastion ~]# nkp create image nutanix ubuntu-22.04 --cluster=$PE_CLUSTER_NAME --endpoint=$PC_ENDPOINT --subnet=$SUBNET --insecure Provisioning and configuring image Manifest files extracted to /root/.nkp-image-builder-3070850318 Trying to pull docker.io/mesosphere/nkp-image-builder:v0.22.3... Getting image source signatures Copying blob sha256:99febc243941c0928306275946d862c586d2bd46be4fc4c4e318938a8662d85f Copying blob sha256:c926b61bad3b94ae7351bafd0c184c159ebf0643b085f7ef1d47ecdc7316833c Copying blob sha256:89fb4fbbd97a1c94aa08f217f0cb914c612179746c6b3706103a99317e65bcda Copying blob sha256:fdc5ed007f9c02f44943008862f7c6b62ad238018c6bf0412ab688445e42bbd2 Copying blob sha256:e9db0040a66483f9870e0f445877fa1352a405a970814dea2aff29598198ff6d Copying blob sha256:acb23a6002388c658931180fb73706eeee11696de5da1bce33df4a8a206de2a0 Copying blob sha256:1166ced2b6761e3267b16e6b82ca07ee8fd00c6626c3507ccc8034ceaf722494 Copying blob sha256:72df65ad9aeb923b7c2cbc5d06a5e16cb2fda0041a0dc04416c02223e54caee4 Copying blob sha256:319333dc043f729fe7dca70c6188d77614743bb43e47693f348f5cafb1425cbe Copying blob sha256:517f3ad3da58ceb19857ea8e995ea92fb76ae37df29d314265f3d0e8da77060f Copying config sha256:2c008cec7e3e56932b4510d51067ff0b604822190f3589d091ec265f58e4bf01 Writing manifest to image destination nutanix.kib_image: output will be in this color. ==\u0026gt; nutanix.kib_image: Creating Packer Builder virtual machine... ... Build \u0026#39;nutanix.kib_image\u0026#39; finished after 7 minutes 28 seconds. ==\u0026gt; Wait completed after 7 minutes 28 seconds ==\u0026gt; Builds finished. The artifacts of successful builds are: --\u0026gt; nutanix.kib_image: nkp-ubuntu-22.04-1.31.4-20250306081227 --\u0026gt; nutanix.kib_image: nkp-ubuntu-22.04-1.31.4-20250306081227 --\u0026gt; nutanix.kib_image: nkp-ubuntu-22.04-1.31.4-20250306081227 OS Image: nkp-ubuntu-22.04-1.31.4-20250306081227 5. 升級 Kommander 原版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 [root@nkp-bastion ~]# kubectl get helmreleases -A NAMESPACE NAME AGE READY STATUS kommander-default-workspace karma-traefik-certs 12d True Helm install succeeded for release kommander-default-workspace/karma-traefik-certs.v1 with chart kommander-cert-federation@0.0.11 kommander-default-workspace kubecost-traefik-certs 12d True Helm install succeeded for release kommander-default-workspace/kubecost-traefik-certs.v1 with chart kommander-cert-federation@0.0.11 kommander-default-workspace prometheus-traefik-certs 12d True Helm install succeeded for release kommander-default-workspace/prometheus-traefik-certs.v1 with chart kommander-cert-federation@0.0.11 kommander ai-navigator-app 10d True Helm install succeeded for release kommander/ai-navigator-cluster-info-api.v1 with chart ai-navigator-cluster-info-api@0.2.8 kommander cluster-observer-2360587938 12d True Helm install succeeded for release kommander/cluster-observer-2360587938.v1 with chart cluster-observer@1.4.1 kommander dex 12d True Helm upgrade succeeded for release kommander/dex.v2 with chart dex@2.14.0 kommander dex-k8s-authenticator 12d True Helm upgrade succeeded for release kommander/dex-k8s-authenticator.v4 with chart dex-k8s-authenticator@1.4.1 kommander fluent-bit 10d True Helm install succeeded for release kommander/kommander-fluent-bit.v1 with chart fluent-bit@0.47.7 kommander gatekeeper 12d True Helm install succeeded for release kommander/kommander-gatekeeper.v1 with chart gatekeeper@3.17.0 kommander gatekeeper-proxy-mutations 12d True Helm install succeeded for release kommander/gatekeeper-proxy-mutations.v1 with chart gatekeeper-proxy-mutations@v0.0.1 kommander grafana-logging 10d True Helm install succeeded for release kommander/grafana-logging.v1 with chart grafana@8.5.8 kommander grafana-loki 10d True Helm install succeeded for release kommander/grafana-loki.v1 with chart loki-distributed@0.79.4 kommander istio 10d True Helm upgrade succeeded for release istio-system/istio.v2 with chart istio@1.23.3 kommander jaeger 10d True Helm install succeeded for release istio-system/jaeger.v1 with chart jaeger-operator@2.56.0 kommander karma-traefik-certs 12d True Helm install succeeded for release kommander/karma-traefik-certs.v1 with chart kommander-cert-federation@0.0.11 kommander kiali 10d True Helm install succeeded for release istio-system/kiali.v1 with chart kiali-operator@1.89.7 kommander kommander 12d True Helm install succeeded for release kommander/kommander.v1 with chart kommander@v2.13.1 kommander kommander-appmanagement 12d True Helm install succeeded for release kommander/kommander-appmanagement.v1 with chart kommander-appmanagement@v2.13.1 kommander kommander-operator 12d True Helm install succeeded for release kommander/kommander-operator.v1 with chart kommander-operator@0.3.2 kommander kommander-ui 12d True Helm install succeeded for release kommander/kommander-kommander-ui.v1 with chart kommander-ui@15.23.17 kommander kube-oidc-proxy 12d True Helm install succeeded for release kommander/kube-oidc-proxy.v1 with chart kube-oidc-proxy@0.3.4 kommander kube-prometheus-stack 10d True Helm install succeeded for release kommander/kube-prometheus-stack.v1 with chart kube-prometheus-stack@65.5.0 kommander kubecost-traefik-certs 12d True Helm install succeeded for release kommander/kubecost-traefik-certs.v1 with chart kommander-cert-federation@0.0.11 kommander kubefed 12d True Helm install succeeded for release kube-federation-system/kubefed.v1 with chart kubefed@0.10.4 kommander kubernetes-dashboard 10d True Helm install succeeded for release kommander/kubernetes-dashboard.v1 with chart kubernetes-dashboard@7.10.0 kommander kubetunnel 10d True Helm install succeeded for release kommander/kubetunnel.v1 with chart kubetunnel@v0.0.38 kommander logging-operator 10d True Helm install succeeded for release kommander/logging-operator.v1 with chart logging-operator@4.2.3 kommander logging-operator-logging 10d True Helm install succeeded for release kommander/logging-operator-logging.v1 with chart logging-operator-logging@4.2.2 kommander object-bucket-claims 10d True Helm install succeeded for release kommander/object-bucket-claims.v1 with chart object-bucket-claim@0.1.11 kommander prometheus-adapter 10d True Helm install succeeded for release kommander/prometheus-adapter.v1 with chart prometheus-adapter@4.11.0 kommander prometheus-traefik-certs 12d True Helm install succeeded for release kommander/prometheus-traefik-certs.v1 with chart kommander-cert-federation@0.0.11 kommander reloader 12d True Helm install succeeded for release kommander/kommander-reloader.v1 with chart reloader@1.1.0 kommander rook-ceph 10d True Helm install succeeded for release kommander/rook-ceph.v1 with chart rook-ceph@v1.14.5 kommander rook-ceph-cluster 10d True Helm install succeeded for release kommander/rook-ceph-cluster.v1 with chart rook-ceph-cluster@v1.14.5 kommander traefik 12d True Helm install succeeded for release kommander/kommander-traefik.v1 with chart traefik@27.0.2 kommander traefik-forward-auth-mgmt 12d True Helm install succeeded for release kommander/traefik-forward-auth-mgmt.v1 with chart traefik-forward-auth@0.3.10 kommander velero 10d True Helm install succeeded for release kommander/velero.v1 with chart velero@7.2.2 執行升級指令 大約20分鐘時間\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [root@nkp-bastion ~]# cd /home/nkp/nkptools/nkp-airgap/nkp-v2.14.0/ [root@nkp-bastion nkp-v2.14.0]# nkp upgrade kommander \\ --charts-bundle ./application-charts/nkp-kommander-charts-bundle-v2.14.0.tar.gz \\ --kommander-applications-repository ./application-repositories/kommander-applications-v2.14.0.tar.gz ✓ Ensuring upgrading conditions are met ✓ Ensuring kubecost can be upgraded ✓ Ensuring DKP licenses are migrated (if any) ✓ Ensuring application definitions are updated ✓ Ensuring helm-mirror implementation is migrated to ChartMuseum ✓ Ensuring root CA duration is upgraded ✓ Ensuring DKA config overrides are cleaned up ✓ Ensuring core Kommander application [kommander-appmanagement] is upgraded ✓ Ensuring core Kommander applications [git-operator kommander kommander-flux kommander-ui] are upgraded ✓ Ensuring new/updated management configOverrides configmaps are applied ✓ Installing COSI controller ✓ Ensuring new default Kommander applications are enabled ✓ Ensuring 23 platform Kommander applications are upgraded [==================================\u0026gt;23/23] (time elapsed 11m07s) ✓ Ensuring new Kommander applications are installed Kommander UI 已升級，並有新的應用程式出現 (Harbor, Nutanix COSI, CloudNativePG, kubecost)\n6. 升級 Cluster 確認環境變數\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 修改 VM_IMAGE_NAME [root@nkp-bastion ~]# vi env.sh [root@nkp-bastion ~]# source env.sh [root@nkp-bastion ~]# echo $MANAGEMENT_CLUSTER_NAME nkp-poc [root@nkp-bastion ~]# echo $VM_IMAGE_NAME nkp-ubuntu-22.04-1.31.4-20250306081227 [root@nkp-bastion nkp-v2.14.0]# kubectl get nodes NAME STATUS ROLES AGE VERSION nkp-poc-d78fs-dkfnd Ready control-plane 12d v1.30.5 nkp-poc-d78fs-gmllf Ready control-plane 12d v1.30.5 nkp-poc-d78fs-x989r Ready control-plane 12d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-4jbfm Ready \u0026lt;none\u0026gt; 12d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-cxlx7 Ready \u0026lt;none\u0026gt; 12d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-kwvlj Ready \u0026lt;none\u0026gt; 12d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-m9dkw Ready \u0026lt;none\u0026gt; 12d v1.30.5 執行升級指令\n1 2 3 4 5 6 7 8 [root@nkp-bastion ~]# nkp upgrade cluster nutanix \\ --cluster-name ${MANAGEMENT_CLUSTER_NAME} \\ --vm-image ${VM_IMAGE_NAME} ✓ Deleting VCD Infrastructure Provider ✓ Upgrading CAPI components ✓ Updating ClusterClass resources cluster.cluster.x-k8s.io/nkp-poc upgraded ⠈⠱ Upgrading the cluster Prism Central 自動開始建立新的虛擬機加入叢集，Control 跟 worker 每一台採取先建後拆的方式自動升級\n所以 IP 數量至少要多2個比較保險\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 用另一個 session 連線到 bastion 確認狀態 [nkp@nkp-bastion ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION nkp-poc-d78fs-czzph NotReady control-plane 47s v1.31.4 nkp-poc-d78fs-dkfnd Ready control-plane 12d v1.30.5 nkp-poc-d78fs-gmllf Ready control-plane 13d v1.30.5 nkp-poc-d78fs-x989r Ready control-plane 12d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-4jbfm Ready \u0026lt;none\u0026gt; 13d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-cxlx7 Ready \u0026lt;none\u0026gt; 13d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-kwvlj Ready \u0026lt;none\u0026gt; 13d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-m9dkw Ready \u0026lt;none\u0026gt; 13d v1.30.5 [nkp@nkp-bastion ~]$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME nkp-poc-d78fs-czzph Ready control-plane 7m12s v1.31.4 10.38.14.43 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-d78fs-nqtx9 Ready control-plane 3m48s v1.31.4 10.38.14.40 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-d78fs-x989r Ready control-plane 13d v1.30.5 10.38.14.30 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-133-generic containerd://1.7.22-d2iq.1 nkp-poc-md-0-clx8j-g5pnb-4jbfm Ready \u0026lt;none\u0026gt; 13d v1.30.5 10.38.14.31 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-133-generic containerd://1.7.22-d2iq.1 nkp-poc-md-0-clx8j-g5pnb-cxlx7 Ready \u0026lt;none\u0026gt; 13d v1.30.5 10.38.14.50 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-133-generic containerd://1.7.22-d2iq.1 nkp-poc-md-0-clx8j-g5pnb-kwvlj Ready \u0026lt;none\u0026gt; 13d v1.30.5 10.38.14.48 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-133-generic containerd://1.7.22-d2iq.1 nkp-poc-md-0-clx8j-g5pnb-m9dkw Ready \u0026lt;none\u0026gt; 13d v1.30.5 10.38.14.47 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-133-generic containerd://1.7.22-d2iq.1 [nkp@nkp-bastion ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION nkp-poc-d78fs-czzph Ready control-plane 12m v1.31.4 nkp-poc-d78fs-mcn2q Ready control-plane 4m59s v1.31.4 nkp-poc-d78fs-nqtx9 Ready control-plane 8m47s v1.31.4 nkp-poc-md-0-clx8j-g5pnb-4jbfm Ready \u0026lt;none\u0026gt; 13d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-cxlx7 Ready \u0026lt;none\u0026gt; 13d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-kwvlj Ready \u0026lt;none\u0026gt; 13d v1.30.5 nkp-poc-md-0-clx8j-g5pnb-m9dkw Ready \u0026lt;none\u0026gt; 13d v1.30.5 nkp-poc-md-0-clx8j-gfbbp-sh8b2 NotReady \u0026lt;none\u0026gt; 13s v1.31.4 [nkp@nkp-bastion ~]$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME nkp-poc-d78fs-czzph Ready control-plane 12m v1.31.4 10.38.14.43 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-d78fs-mcn2q Ready control-plane 5m29s v1.31.4 10.38.14.38 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-d78fs-nqtx9 Ready control-plane 9m17s v1.31.4 10.38.14.40 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-md-0-clx8j-g5pnb-4jbfm Ready \u0026lt;none\u0026gt; 13d v1.30.5 10.38.14.31 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-133-generic containerd://1.7.22-d2iq.1 nkp-poc-md-0-clx8j-g5pnb-cxlx7 Ready \u0026lt;none\u0026gt; 13d v1.30.5 10.38.14.50 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-133-generic containerd://1.7.22-d2iq.1 nkp-poc-md-0-clx8j-g5pnb-kwvlj Ready \u0026lt;none\u0026gt; 13d v1.30.5 10.38.14.48 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-133-generic containerd://1.7.22-d2iq.1 nkp-poc-md-0-clx8j-g5pnb-m9dkw Ready \u0026lt;none\u0026gt; 13d v1.30.5 10.38.14.47 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-133-generic containerd://1.7.22-d2iq.1 nkp-poc-md-0-clx8j-gfbbp-sh8b2 NotReady \u0026lt;none\u0026gt; 43s v1.31.4 10.38.14.19 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 升級完成，大約花費30分鐘左右\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 [nkp@nkp-bastion ~]$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME nkp-poc-d78fs-czzph Ready control-plane 26m v1.31.4 10.38.14.43 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-d78fs-mcn2q Ready control-plane 19m v1.31.4 10.38.14.38 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-d78fs-nqtx9 Ready control-plane 22m v1.31.4 10.38.14.40 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-md-0-clx8j-gfbbp-g6rtw Ready \u0026lt;none\u0026gt; 7m50s v1.31.4 10.38.14.25 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-md-0-clx8j-gfbbp-qzrkh Ready \u0026lt;none\u0026gt; 3m6s v1.31.4 10.38.14.30 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-md-0-clx8j-gfbbp-sh8b2 Ready \u0026lt;none\u0026gt; 14m v1.31.4 10.38.14.19 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 nkp-poc-md-0-clx8j-gfbbp-xlmc7 Ready \u0026lt;none\u0026gt; 11m v1.31.4 10.38.14.27 \u0026lt;none\u0026gt; Ubuntu 22.04.5 LTS 5.15.0-134-generic containerd://1.7.24-d2iq.1 [nkp@nkp-bastion ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION nkp-poc-d78fs-czzph Ready control-plane 26m v1.31.4 nkp-poc-d78fs-mcn2q Ready control-plane 19m v1.31.4 nkp-poc-d78fs-nqtx9 Ready control-plane 23m v1.31.4 nkp-poc-md-0-clx8j-gfbbp-g6rtw Ready \u0026lt;none\u0026gt; 7m59s v1.31.4 nkp-poc-md-0-clx8j-gfbbp-qzrkh Ready \u0026lt;none\u0026gt; 3m15s v1.31.4 nkp-poc-md-0-clx8j-gfbbp-sh8b2 Ready \u0026lt;none\u0026gt; 14m v1.31.4 nkp-poc-md-0-clx8j-gfbbp-xlmc7 Ready \u0026lt;none\u0026gt; 12m v1.31.4 [nkp@nkp-bastion ~]$ kubectl get helmreleases -A NAMESPACE NAME AGE READY STATUS kommander-default-workspace karma-traefik-certs 13d True Helm upgrade succeeded for release kommander-default-workspace/karma-traefik-certs.v2 with chart kommander-cert-federation@0.0.12 kommander-default-workspace prometheus-traefik-certs 13d True Helm upgrade succeeded for release kommander-default-workspace/prometheus-traefik-certs.v2 with chart kommander-cert-federation@0.0.12 kommander ai-navigator-app 10d True Helm install succeeded for release kommander/ai-navigator-cluster-info-api.v1 with chart ai-navigator-cluster-info-api@0.2.8 kommander cluster-observer-2360587938 13d True Helm upgrade succeeded for release kommander/cluster-observer-2360587938.v2 with chart cluster-observer@1.4.1 kommander cosi-driver-ceph-dkp-object-store 20h True Helm install succeeded for release kommander/rook-ceph-cluster-cosi-driver.v1 with chart cosi-bucket-kit@0.0.5 kommander dex 13d True Helm upgrade succeeded for release kommander/dex.v3 with chart dex@2.14.0 kommander dex-k8s-authenticator 13d True Helm upgrade succeeded for release kommander/dex-k8s-authenticator.v5 with chart dex-k8s-authenticator@1.4.1 kommander fluent-bit 10d True Helm upgrade succeeded for release kommander/kommander-fluent-bit.v2 with chart fluent-bit@0.48.5 kommander gatekeeper 13d True Helm upgrade succeeded for release kommander/kommander-gatekeeper.v2 with chart gatekeeper@3.18.2 kommander gatekeeper-proxy-mutations 13d True Helm install succeeded for release kommander/gatekeeper-proxy-mutations.v1 with chart gatekeeper-proxy-mutations@v0.0.1 kommander gateway-api-crds 20h True Helm install succeeded for release kommander/gateway-api-crds.v1 with chart traefik-crds@1.2.0 kommander grafana-logging 10d True Helm upgrade succeeded for release kommander/grafana-logging.v2 with chart grafana@8.9.0 kommander grafana-loki 10d True Helm install succeeded for release kommander/grafana-loki.v1 with chart loki-distributed@0.79.4 kommander istio 10d True Helm upgrade succeeded for release istio-system/istio.v3 with chart istio@1.23.3 kommander jaeger 10d True Helm upgrade succeeded for release istio-system/jaeger.v2 with chart jaeger-operator@2.57.0 kommander karma-traefik-certs 13d True Helm upgrade succeeded for release kommander/karma-traefik-certs.v2 with chart kommander-cert-federation@0.0.12 kommander kiali 10d True Helm upgrade succeeded for release istio-system/kiali.v2 with chart kiali-operator@2.4.0 kommander kommander 13d True Helm upgrade succeeded for release kommander/kommander.v2 with chart kommander@v2.14.0 kommander kommander-appmanagement 13d True Helm upgrade succeeded for release kommander/kommander-appmanagement.v2 with chart kommander-appmanagement@v2.14.0 kommander kommander-operator 13d True Helm upgrade succeeded for release kommander/kommander-operator.v2 with chart kommander-operator@0.3.2 kommander kommander-ui 13d True Helm upgrade succeeded for release kommander/kommander-kommander-ui.v2 with chart kommander-ui@17.42.7 kommander kube-oidc-proxy 13d True Helm upgrade succeeded for release kommander/kube-oidc-proxy.v2 with chart kube-oidc-proxy@0.3.4 kommander kube-prometheus-stack 10d True Helm upgrade succeeded for release kommander/kube-prometheus-stack.v3 with chart kube-prometheus-stack@69.1.2 kommander kubefed 13d True Helm upgrade succeeded for release kube-federation-system/kubefed.v2 with chart kubefed@0.10.4 kommander kubernetes-dashboard 10d True Helm upgrade succeeded for release kommander/kubernetes-dashboard.v2 with chart kubernetes-dashboard@7.10.3 kommander kubetunnel 10d True Helm upgrade succeeded for release kommander/kubetunnel.v2 with chart kubetunnel@v0.0.39 kommander logging-operator 10d True Helm upgrade succeeded for release kommander/logging-operator.v2 with chart logging-operator@5.0.1 kommander logging-operator-logging 10d True Helm upgrade succeeded for release kommander/logging-operator-logging.v2 with chart logging-operator-logging@4.2.2 kommander object-bucket-claims 10d True Helm upgrade succeeded for release kommander/object-bucket-claims.v2 with chart object-bucket-claim@0.1.11 kommander prometheus-adapter 10d True Helm install succeeded for release kommander/prometheus-adapter.v1 with chart prometheus-adapter@4.11.0 kommander prometheus-traefik-certs 13d True Helm upgrade succeeded for release kommander/prometheus-traefik-certs.v2 with chart kommander-cert-federation@0.0.12 kommander reloader 13d True Helm upgrade succeeded for release kommander/kommander-reloader.v2 with chart reloader@1.2.1 kommander rook-ceph 10d True Helm upgrade succeeded for release kommander/rook-ceph.v2 with chart rook-ceph@v1.16.2 kommander rook-ceph-cluster 10d True Helm upgrade succeeded for release kommander/rook-ceph-cluster.v2 with chart rook-ceph-cluster@v1.16.2 kommander traefik 13d True Helm upgrade succeeded for release kommander/kommander-traefik.v2 with chart traefik@34.1.0 kommander traefik-crds 20h True Helm install succeeded for release kommander/traefik-crds.v1 with chart traefik-crds@1.2.0 kommander traefik-forward-auth-mgmt 13d True Helm upgrade succeeded for release kommander/traefik-forward-auth-mgmt.v2 with chart traefik-forward-auth@0.3.10 kommander velero 10d True Helm upgrade succeeded for release kommander/velero.v2 with chart velero@8.3.0 Management 升級完成後之後再升級 workload cluster\n以下為範例\n1 2 3 4 5 6 7 # kubectl get cluster -A # export WORKLOAD_CLUSTER_NAME=demo-prod-01 # export WORKLOAD_CLUSTER_NAMESPACE=demo-zone-c4zz7-qjq6g # nkp upgrade cluster nutanix \\ --cluster-name ${WORKLOAD_CLUSTER_NAME} \\ --vm-image ${VM_IMAGE_NAME} -n WORKLOAD_CLUSTER_NAME ","date":"2025-03-07T05:53:22+08:00","permalink":"https://wangken0129.github.io/p/nutanix_nkp-v2.13_ahv/","title":"Nutanix_NKP-v2.13_AHV"},{"content":"OCP 4.16.3_on_Nutanix (IPI) 整合測試 OCP 4.16.3 透過 IPI (Installer-provisioned Infrastructure) 安裝在 Nutanix\n並且結合 Nutanix CSI、COSI、NDK (Nutanix Data service for kubernetes) 、 Autoscaler 自動擴展Worker\n大致流程 建立Bastion Prism Central 更換憑證 建立 install-config.yaml , CCO 建立 Openshift Cluster 安裝Nutanix CSI 3.0 (beta)連線NUS 安裝測試 Nutanix NDK (For VG Only) 建立Machine Config ，worker node autoscaler Reference Openshift:\nhttps://docs.openshift.com/container-platform/4.16/installing/installing_nutanix/preparing-to-install-on-nutanix.html\nOpenshift Download:\nhttps://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.16.3/\nNutanix NVD for OCP:\nhttps://portal.nutanix.com/page/documents/solutions/details?targetId=NVD-2177-Cloud-Native-6-5-OpenShift:NVD-2177-Cloud-Native-6-5-OpenShift\nhttps://portal.nutanix.com/page/documents/solutions/details?targetId=TN-2030-Red-Hat-OpenShift-on-Nutanix:TN-2030-Red-Hat-OpenShift-on-Nutanix\nNutanix NDK:\nhttps://portal.nutanix.com/page/documents/details?targetId=Nutanix-Data-Services-for-Kubernetes-v1_0:Nutanix-Data-Services-for-Kubernetes-v1_0\nNutanix PC CA:\nhttps://www.nutanix.dev/2023/09/26/generating-a-self-signed-san-certificate-to-use-red-hat-openshift-in-nutanix-marketplace/\n架構與版本 名稱 IP / DNS 版本 AOS / PE 172.16.90.74 6.8 Prism Central 172.16.90.75 pc2024.1.0.1 NDK none 1.0.0 Nutanix CSI none nutanix-csi-storage-3.0.0-beta.1912 Nutanix Object none 5.0 Nutanix IPAM 172.16.90.x none bastion 172.16.90.206 RHEL 9.3 OCP Cluster Name ocplab.nutanixlab.local 4.16.3 OCP API VIP 172.16.90.205 / api.ocplab.nutanixlab.local none OCP Ingress VIP 172.16.90.207 / *.apps.ocplab.nutanixlab.local none OCP Installation Prepare Bastion 準備一台 RHEL VM\n1 2 3 [nutanix@ken-rhel9 ~]$ cat /etc/os-release NAME=\u0026#34;Red Hat Enterprise Linux\u0026#34; VERSION=\u0026#34;9.3 (Plow)\u0026#34; 下載安裝檔 ccoctl , oc , openshift-install ,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 建立安裝目錄 $ mkdir ocp-install-4-16-3 $ cd ocp-install-4-16-3/ # 下載檔案 $ wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.16.3/ccoctl-linux-rhel9-4.16.3.tar.gz $ wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.16.3/openshift-client-linux-amd64-rhel9-4.16.3.tar.gz $ wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.16.3/openshift-install-linux-4.16.3.tar.gz # 解壓縮檔案 $ tar xvf ccoctl-linux-rhel9-4.16.3.tar.gz $ tar xvf openshift-client-linux-amd64-rhel9-4.16.3.tar.gz $ tar xvf openshift-install-linux-4.16.3.tar.gz $ ll total 1689088 -rwxr-xr-x. 1 nutanix nutanix 89185776 Jul 4 05:23 ccoctl -rw-r--r--. 1 nutanix nutanix 35912054 Jul 12 03:52 ccoctl-linux-rhel9-4.16.3.tar.gz -rwxr-xr-x. 2 nutanix nutanix 159916496 Jul 9 04:50 kubectl -rwxr-xr-x. 2 nutanix nutanix 159916496 Jul 9 04:50 oc -rw-r--r--. 1 nutanix nutanix 66698442 Jul 12 03:52 openshift-client-linux-amd64-rhel9-4.16.3.tar.gz -rwxr-xr-x. 1 nutanix nutanix 707719168 Jul 9 20:10 openshift-install -rw-r--r--. 1 nutanix nutanix 510267167 Jul 12 03:52 openshift-install-linux-4.16.3.tar.gz $ ./oc version Client Version: 4.16.3 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 $ ./openshift-install version ./openshift-install 4.16.3 built from commit e1f9f057ce87c1a4a5f3c268812fa4c9dc003cb7 release image quay.io/openshift-release-dev/ocp-release@sha256:3ec3a43ded1decc18134e5677f56037d8929f4442930f5d1156e7a77cdf1b9b3 release architecture amd64 Prism Central CA 產生 CA 憑證\n1 2 3 4 5 6 7 8 9 10 11 12 $ export PC_IP=172.16.90.75 $ export PC_FQDN=ntnxselabpc.nutanixlab.local $ openssl req -x509 -nodes -days 3650 \\ -newkey rsa:2048 -keyout ${PC_IP}.key -out ${PC_IP}.crt \\ -subj \u0026#34;/C=US/ST=CA/L=San Jose/O=Nutanix Inc./OU=Manageability/CN=*.nutanix.local\u0026#34; \\ -addext \u0026#34;subjectAltName=IP:${PC_IP},DNS:${PC_FQDN}\u0026#34; $ ll total 1689096 -rw-r--r--. 1 nutanix nutanix 1444 Jul 16 14:48 172.16.90.75.crt -rw-------. 1 nutanix nutanix 1704 Jul 16 14:48 172.16.90.75.key 置換 Prism Central CA\n檢視憑證\n將憑證匯入至Bastion\n1 2 $ sudo cp 172.16.90.75.* /etc/pki/ca-trust/source/anchors $ sudo update-ca-trust extract Install-config.yaml 建立install-config.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 建立安裝目錄 $ mkdir install # 產生install-config.yaml $ ./openshift-install create install-config --dir /home/nutanix/ocp-install-4-16-3/install ? SSH Public Key /home/nutanix/.ssh/id_rsa.pub ? Platform nutanix ? Prism Central 172.16.90.75 ? Port 9440 ? Username ocpadmin ? Password [? for help] *********** INFO Connecting to Prism Central 172.16.90.75 ? Prism Element NX1365G6PE ? Subnet Netfos_90_Access_IPAM ? Virtual IP Address for API 172.16.90.205 ? Virtual IP Address for Ingress 172.16.90.207 ? Base Domain nutanixlab.local ? Cluster Name ocplab ? Pull Secret [? for help] *****************************************************************************************************************INFO Install-Config created in: /home/nutanix/ocp-install-4-16-3/install $ ll install/ total 8 -rw-r-----. 1 nutanix nutanix 4374 Jul 16 14:59 install-config.yaml 修改檔案\nadditionalTrustBundle: 加入 Prism Central 的 root CA\nadditionalTrustBundlePolicy: 改為 Always\nmachineNetwork: 改為與VM相同網段 xxx.xxx.xxx.xxx/24\nplatform 自定義cpu、memory、disk\n完成後如下，建議備份此檔案以利部署到其他地方或是重新部署\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 $ cat install-config.yaml additionalTrustBundlePolicy: Always additionalTrustBundle: | -----BEGIN CERTIFICATE----- MIID/jCCAuagAwIBAgIUKDpX++EriXrP4L1R+e15qqR3+4owDQYJKoZIhvcNAQEL BQAwdjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAkNBMREwDwYDVQQHDAhTYW4gSm9z ZTEVMBMGxxx.... -----END CERTIFICATE----- apiVersion: v1 baseDomain: nutanixlab.local compute: - architecture: amd64 hyperthreading: Enabled name: worker platform: nutanix: cpus: 2 coresPerSocket: 2 memoryMiB: 8196 osDisk: diskSizeGiB: 120 categories: - key: ocplab value: worker replicas: 3 controlPlane: architecture: amd64 hyperthreading: Enabled name: master platform: nutanix: cpus: 4 coresPerSocket: 2 memoryMiB: 16384 osDisk: diskSizeGiB: 120 categories: - key: ocpalb value: master replicas: 3 credentialsMode: Manual metadata: creationTimestamp: null name: ocplab networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 172.16.90.0/24 networkType: OVNKubernetes serviceNetwork: - 172.30.0.0/16 platform: nutanix: apiVIPs: - 172.16.90.205 ingressVIPs: - 172.16.90.207 prismCentral: endpoint: address: 172.16.90.75 port: 9440 password: password username: ocpadmin prismElements: - endpoint: address: 172.16.90.74 port: 9440 uuid: 00061972-aeba-fee2-0000-000000028e95 subnetUUIDs: - 060837bb-aefc-41f2-aaae-25c95267d87a publish: External pullSecret: \u0026#39;{\u0026#34;auths\u0026#34;:{\u0026#34;cloud.openshift.com\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;b3BlbnN....\u0026#39; sshKey: | ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCy4sUIXB5QMIETJe6+niy4SrO0ly/ymlwBtnCbgAPNtIKfQfkjFrbcUgsdUsjEQ+M0giXfjlgDcAMiasxaFvfhKCNJ1dR5L6hyi/JrWAdG/Ue5ydbc84vFgQRQ1AolnovlxWcM4xdsYYQvpZwo9v1dOIAQ9rasubEiDxHaw7HK+mGChu+6uRli4EdJ7HvRg9Ha5dKAQFawsZ/cBcMZZnW84bfK72naHI7XyVOEPnbQ8NG+Pk55S2za6J7VIIHDr3rSbNYBZUJnTroYMLxXnC0I0xqcXZMDiUm6ZQzE6kJnrFHIdqVEXkgqtbdXQDK+zdeA/vOttq8ojYSFpa9qCHMCHK66OC2P0vE54hI439uEHPJI7aIYNGPxPJtL6k8rDklWyMnkh8jJMtxM+sZkZxI2/+L4szC/S4qJ/OmRJI3TeTob+0kptqwJrAuWK3T/YiCWbyqytr4RyGJTFA5pB0SucJHaqJ4PbiJ633hJ8AJ7KepYHB65KyvHgDga2v3QDf0= nutanix@ken-rhel9.nutanixlab.local Cloud Credential Operator 安裝Cluster需要Cloud Credential Operator (CCO)，讓OCP能與Prism溝通\n大致作法為\n建立帳號密碼檔案\n建立要求帳密的目錄及檔案\n利用上述檔案建立OCP Shared Secret\n建立cred.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 $ vim credentials.yaml credentials: - type: basic_auth data: prismCentral: username: ocpadmin password: password prismElements: - name: username: ocpadmin password: password 設定Image環境變數\n1 $ RELEASE_IMAGE=$(./openshift-install version | awk \u0026#39;/release image/ {print $3}\u0026#39;) 建立credrequests資料夾\n1 $ mkdir credrequests 建立0000_30_machine-api-operator_00_credentials-request.yaml\n1 2 3 4 5 6 7 8 9 $ ./oc adm release extract --credentials-requests --cloud=nutanix --to=./credrequests $RELEASE_IMAGE warning: if you intend to pass CredentialsRequests to ccoctl, you should use --included to filter out requests that your cluster is not expected to need. Extracted release payload created at 2024-07-11T11:19:04Z $ ll credrequests/ total 8 -rw-r--r--. 1 nutanix nutanix 599 Jul 16 15:43 0000_26_cloud-controller-manager-operator_18_credentialsrequest-nutanix.yaml -rw-r--r--. 1 nutanix nutanix 543 Jul 16 15:43 0000_30_machine-api-operator_00_credentials-request.yaml 建立 CCO Secret\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ ./ccoctl nutanix create-shared-secrets --credentials-requests-dir=/home/nutanix/ocp-install-4-16-3/credrequests --output-dir=/home/nutanix/ocp-install-4-16-3/install --credentials-source-filepath=/home/nutanix/ocp-install-4-16-3/install/credentials.yaml 2024/07/16 15:45:45 Saved credentials configuration to: /home/nutanix/ocp-install-4-16-3/install/manifests/openshift-cloud-controller-manager-nutanix-credentials-credentials.yaml 2024/07/16 15:45:45 Saved credentials configuration to: /home/nutanix/ocp-install-4-16-3/install/manifests/openshift-machine-api-nutanix-credentials-credentials.yaml $ tree install install ├── credentials.yaml ├── install-config.yaml └── manifests ├── openshift-cloud-controller-manager-nutanix-credentials-credentials.yaml └── openshift-machine-api-nutanix-credentials-credentials.yaml 1 directory, 4 files Create Cluster Create manifests\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 $ ./openshift-install create manifests --dir /home/nutanix/ocp-install-4-16-3/install INFO Consuming Install Config from target directory INFO Manifests created in: /home/nutanix/ocp-install-4-16-3/install/cluster-api, /home/nutanix/ocp-install-4-16-3/install/manifests and /home/nutanix/ocp-install-4-16-3/install/openshift $ tree install install ├── cluster-api │ ├── 000_capi-namespace.yaml │ ├── 01_capi-cluster.yaml │ ├── 01_nutanix-cluster.yaml │ ├── 01_nutanix-creds.yaml │ └── machines │ ├── 10_inframachine_ocplab-k7zds-bootstrap.yaml │ ├── 10_inframachine_ocplab-k7zds-master-0.yaml │ ├── 10_inframachine_ocplab-k7zds-master-1.yaml │ ├── 10_inframachine_ocplab-k7zds-master-2.yaml │ ├── 10_machine_ocplab-k7zds-bootstrap.yaml │ ├── 10_machine_ocplab-k7zds-master-0.yaml │ ├── 10_machine_ocplab-k7zds-master-1.yaml │ └── 10_machine_ocplab-k7zds-master-2.yaml ├── credentials.yaml ├── manifests │ ├── cloud-provider-config.yaml │ ├── cluster-config.yaml │ ├── cluster-dns-02-config.yml │ ├── cluster-infrastructure-02-config.yml │ ├── cluster-ingress-02-config.yml │ ├── cluster-network-02-config.yml │ ├── cluster-proxy-01-config.yaml │ ├── cluster-scheduler-02-config.yml │ ├── cvo-overrides.yaml │ ├── kube-cloud-config.yaml │ ├── kube-system-configmap-root-ca.yaml │ ├── machine-config-server-tls-secret.yaml │ ├── openshift-cloud-controller-manager-nutanix-credentials-credentials.yaml │ ├── openshift-config-secret-pull-secret.yaml │ ├── openshift-machine-api-nutanix-credentials-credentials.yaml │ └── user-ca-bundle-config.yaml └── openshift ├── 99_feature-gate.yaml ├── 99_kubeadmin-password-secret.yaml ├── 99_openshift-cluster-api_master-machines-0.yaml ├── 99_openshift-cluster-api_master-machines-1.yaml ├── 99_openshift-cluster-api_master-machines-2.yaml ├── 99_openshift-cluster-api_master-user-data-secret.yaml ├── 99_openshift-cluster-api_worker-machineset-0.yaml ├── 99_openshift-cluster-api_worker-user-data-secret.yaml ├── 99_openshift-machine-api_master-control-plane-machine-set.yaml ├── 99_openshift-machineconfig_99-master-ssh.yaml ├── 99_openshift-machineconfig_99-worker-ssh.yaml └── openshift-install-manifests.yaml 4 directories, 41 files Create Cluster\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 $ ./openshift-install create cluster --dir /home/nutanix/ocp-install-4-16-3/install --log-level=debug DEBUG Generating Cluster... INFO Creating infrastructure resources... INFO created the category key \u0026#34;kubernetes-io-cluster-ocplab-k7zds\u0026#34; INFO created the category value \u0026#34;owned\u0026#34; with name \u0026#34;kubernetes-io-cluster-ocplab-k7zds\u0026#34; INFO created the category value \u0026#34;shared\u0026#34; with name \u0026#34;kubernetes-io-cluster-ocplab-k7zds\u0026#34; INFO creating the rhcos image ocplab-k7zds-rhcos (uuid: 1c8db94e-17aa-4eea-bc75-2e05876e8f31). INFO waiting the image data uploading from https://rhcos.mirror.openshift.com/art/storage/prod/streams/4.16-9.4/builds/416.94.202406251923-0/x86_64/rhcos-416.94.202406251923-0-nutanix.x86_64.qcow2?sha256=, taskUUID: cf6ef687-c95d-4648-b97e-c0bccfcb3052. ... INFO Network infrastructure is ready DEBUG No infrastructure ready requirements for the nutanix provider INFO creating the image ocplab-k7zds-bootstrap-ign.iso (uuid: 466b5db5-69e4-4f39-aaac-9ae65594a09f), taskUUID: 04588942-57c9-4b0b-8bb2-321e6eaf60ad INFO created the image ocplab-k7zds-bootstrap-ign.iso (uuid: 466b5db5-69e4-4f39-aaac-9ae65594a09f). used_time 10.829777095s INFO preparing to upload the image ocplab-k7zds-bootstrap-ign.iso (uuid: 466b5db5-69e4-4f39-aaac-9ae65594a09f) data from file /home/nutanix/.cache/openshift-installer/image_cache/ocplab-k7zds-bootstrap-ign.iso ... INFO All cluster operators have completed progressing INFO Checking to see if there is a route at openshift-console/console... DEBUG Route found in openshift-console namespace: console DEBUG OpenShift console route is admitted INFO Install complete! INFO To access the cluster as the system:admin user when using \u0026#39;oc\u0026#39;, run \u0026#39;export KUBECONFIG=/home/nutanix/ocp-install-4-16-3/install/auth/kubeconfig\u0026#39; INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocplab.nutanixlab.local INFO Login to the console with user: \u0026#34;kubeadmin\u0026#34;, and password: \u0026#34;JrHfc-s958g-V3oQ7-eXHxY\u0026#34; DEBUG Time elapsed per stage: DEBUG Infrastructure Pre-provisioning: 2m6s DEBUG Network-infrastructure Provisioning: 54s DEBUG Bootstrap Ignition Provisioning: 18s DEBUG Machine Provisioning: 30s DEBUG Bootstrap Complete: 22m27s DEBUG API: 4m0s DEBUG Bootstrap Destroy: 11s DEBUG Cluster Operators Available: 12m37s DEBUG Cluster Operators Stable: 20s INFO Time elapsed: 39m33s 安裝完畢確認連線\n指令確認\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ cp /home/nutanix/ocp-install-4-16-3/install/auth/kubeconfig ~/.kube/config $ sudo cp oc /usr/local/bin/ [sudo] password for nutanix: $ oc get nodes NAME STATUS ROLES AGE VERSION ocplab-k7zds-master-0 Ready control-plane,master 34m v1.29.6+aba1e8d ocplab-k7zds-master-1 Ready control-plane,master 34m v1.29.6+aba1e8d ocplab-k7zds-master-2 Ready control-plane,master 34m v1.29.6+aba1e8d ocplab-k7zds-worker-27snp Ready worker 19m v1.29.6+aba1e8d ocplab-k7zds-worker-29wr6 Ready worker 19m v1.29.6+aba1e8d ocplab-k7zds-worker-bdgmd Ready worker 19m v1.29.6+aba1e8d $ oc get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ocplab-k7zds-master-0 Ready control-plane,master 34m v1.29.6+aba1e8d 172.16.90.158 \u0026lt;none\u0026gt; Red Hat Enterprise Linux CoreOS 416.94.202407081958-0 5.14.0-427.26.1.el9_4.x86_64 cri-o://1.29.6-3.rhaos4.16.gitfd433b7.el9 ocplab-k7zds-master-1 Ready control-plane,master 34m v1.29.6+aba1e8d 172.16.90.175 \u0026lt;none\u0026gt; Red Hat Enterprise Linux CoreOS 416.94.202407081958-0 5.14.0-427.26.1.el9_4.x86_64 cri-o://1.29.6-3.rhaos4.16.gitfd433b7.el9 ocplab-k7zds-master-2 Ready control-plane,master 34m v1.29.6+aba1e8d 172.16.90.151 \u0026lt;none\u0026gt; Red Hat Enterprise Linux CoreOS 416.94.202407081958-0 5.14.0-427.26.1.el9_4.x86_64 cri-o://1.29.6-3.rhaos4.16.gitfd433b7.el9 ocplab-k7zds-worker-27snp Ready worker 19m v1.29.6+aba1e8d 172.16.90.154 \u0026lt;none\u0026gt; Red Hat Enterprise Linux CoreOS 416.94.202407081958-0 5.14.0-427.26.1.el9_4.x86_64 cri-o://1.29.6-3.rhaos4.16.gitfd433b7.el9 ocplab-k7zds-worker-29wr6 Ready worker 20m v1.29.6+aba1e8d 172.16.90.167 \u0026lt;none\u0026gt; Red Hat Enterprise Linux CoreOS 416.94.202407081958-0 5.14.0-427.26.1.el9_4.x86_64 cri-o://1.29.6-3.rhaos4.16.gitfd433b7.el9 ocplab-k7zds-worker-bdgmd Ready worker 20m v1.29.6+aba1e8d 172.16.90.160 \u0026lt;none\u0026gt; Red Hat Enterprise Linux CoreOS 416.94.202407081958-0 5.14.0-427.26.1.el9_4.x86_64 cri-o://1.29.6-3.rhaos4.16.gitfd433b7.el9 安裝 Nutanix CSI 、COSI Nutanix CSI 3.0 先安裝Helm Chart 在Bastion上面\n1 2 3 4 5 6 7 8 $ sudo curl -L https://mirror.openshift.com/pub/openshift-v4/clients/helm/latest/helm-linux-amd64 -o /usr/local/bin/helm $ sudo chmod +x /usr/local/bin/helm $ helm version WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/nutanix/.kube/config version.BuildInfo{Version:\u0026#34;v3.13.2+35.el9\u0026#34;, GitCommit:\u0026#34;fa6e939d7984e1be0d6fbc2dc920b6bbcf395932\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.20.12\u0026#34;} $ helm repo add nutanix https://nutanix.github.io/helm/ 建立 Nutanix Secret 在 ntnx-system 內\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # Create Namespace ntnx-system $ oc create ns ntnx-system namespace/ntnx-system created # Edit Secret for PE,PC $ vi ntnx-secret.yaml apiVersion: v1 kind: Secret metadata: name: ntnx-secret namespace: ntnx-system stringData: # prism-element-ip:prism-port:admin:password key: pe-ip:9440:admin:password $ vi ntnx-pc-secret.yaml apiVersion: v1 kind: Secret metadata: name: ntnx-pc-secret namespace: ntnx-system stringData: # prism-pc-ip:prism-port:admin:password key: pc-ip:9440:admin:password --- # Create secret $ oc apply -f ntnx-pc-secret.yaml $ oc apply -f ntnx-secret.yaml $ oc get secret -n ntnx-system NAME TYPE DATA AGE ntnx-pc-secret Opaque 1 13s ntnx-secret Opaque 1 9s 下載 Nutanix CSI 3.0 beta\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ wget https://github.com/nutanix/helm-releases/releases/download/nutanix-csi-storage/nutanix-csi-storage-3.0.0-beta.1912.tgz $ tar -xvf nutanix-csi-storage-3.0.0-beta.1912.tgz nutanix-csi-storage/Chart.yaml nutanix-csi-storage/values.yaml nutanix-csi-storage/templates/NOTES.txt nutanix-csi-storage/templates/_helpers.tpl nutanix-csi-storage/templates/csi-driver.yaml nutanix-csi-storage/templates/machine-config.yaml nutanix-csi-storage/templates/ntnx-csi-controller-deployment.yaml nutanix-csi-storage/templates/ntnx-csi-init-configmap.yaml nutanix-csi-storage/templates/ntnx-csi-node-ds.yaml nutanix-csi-storage/templates/ntnx-csi-rbac.yaml nutanix-csi-storage/templates/ntnx-csi-scc.yaml nutanix-csi-storage/templates/ntnx-sc.yaml nutanix-csi-storage/templates/ntnx-secret.yaml nutanix-csi-storage/templates/service-prometheus-csi.yaml nutanix-csi-storage/.helmignore nutanix-csi-storage/Nutanix core_k8s-csi_beta1 Notice.txt nutanix-csi-storage/README.md nutanix-csi-storage/questions.yml Edit value.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ cd nutanix-csi-storage $ vim values.yaml --- # all namespace namespace: ntnx-system createPrismCentralSecret: false # prismCentralEndPoint: 00.00.00.00 # pcUsername: username # pcPassword: password pcSecretName: ntnx-pc-secret createSecret: false # prismEndPoint: 00.00.000.000 # username: username # password: password peSecretName: ntnx-secret #supportedPCVersions: \u0026#34;fraser-2023.4-stable-pc-0\u0026#34; supportedPCVersions: \u0026#34;fraser-2024.1-stable-pc-0.1\u0026#34; -- Install CSI Driver using Helm\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ helm install -n ntnx-system -f nutanix-csi-storage/values.yaml nutanix-csi ./nutanix-csi-storage $ oc get all -n ntnx-system Warning: apps.openshift.io/v1 DeploymentConfig is deprecated in v4.14+, unavailable in v4.10000+ NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nutanix-csi-metrics ClusterIP 172.30.52.86 \u0026lt;none\u0026gt; 9809/TCP,9810/TCP,9811/TCP,9812/TCP 2m43s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/nutanix-csi-node 3 0 0 0 0 kubernetes.io/os=linux 2m43s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nutanix-csi-controller 0/2 0 0 2m43s NAME DESIRED CURRENT READY AGE replicaset.apps/nutanix-csi-controller-8669b97dc8 2 0 0 2m43s $ oc describe daemonset.apps/nutanix-csi-node ---- ------- Warning FailedCreate 12m (x19 over 23m) daemonset-controller Error creating: pods \u0026#34;nutanix-csi-node-\u0026#34; is forbidden: unable to validate against any security context constraint: [provider \u0026#34;anyuid\u0026#34;: Forbidden: not usable by user or serviceaccount Apply privilege to ntnx-system namespace\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ oc adm policy add-scc-to-user privileged -z nutanix-csi-controller -n ntnx-system clusterrole.rbac.authorization.k8s.io/system:openshift:scc:privileged added: \u0026#34;nutanix-csi-controller\u0026#34; $ oc adm policy add-scc-to-user privileged -z nutanix-csi-node -n ntnx-system clusterrole.rbac.authorization.k8s.io/system:openshift:scc:privileged added: \u0026#34;nutanix-csi-node\u0026#34; $ oc adm policy add-scc-to-user privileged -z node-exporter -n ntnx-system $ oc adm policy add-scc-to-user anyuid -z nutanix-csi-controller -n ntnx-system clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \u0026#34;nutanix-csi-controller\u0026#34; $ oc adm policy add-scc-to-user anyuid -z nutanix-csi-node -n ntnx-system clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \u0026#34;nutanix-csi-node\u0026#34; $ oc adm policy add-scc-to-user anyuid -z node-exporter -n ntnx-system clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \u0026#34;node-exporter\u0026#34; # reinstall $ helm uninstall -n ntnx-system nutanix-csi $ helm install -n ntnx-system -f nutanix-csi-storage/values.yaml nutanix-csi ./nutanix-csi-storage 確認安裝完成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ oc get all -n ntnx-system Warning: apps.openshift.io/v1 DeploymentConfig is deprecated in v4.14+, unavailable in v4.10000+ NAME READY STATUS RESTARTS AGE pod/nutanix-csi-controller-8669b97dc8-pwcq4 7/7 Running 0 9s pod/nutanix-csi-controller-8669b97dc8-srkbr 7/7 Running 0 9s pod/nutanix-csi-node-gz278 3/3 Running 0 9s pod/nutanix-csi-node-p6lrb 3/3 Running 0 9s pod/nutanix-csi-node-qdrdp 3/3 Running 0 9s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nutanix-csi-metrics ClusterIP 172.30.34.177 \u0026lt;none\u0026gt; 9809/TCP,9810/TCP,9811/TCP,9812/TCP 9s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/nutanix-csi-node 3 3 3 3 3 kubernetes.io/os=linux 9s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nutanix-csi-controller 2/2 2 2 9s NAME DESIRED CURRENT READY AGE replicaset.apps/nutanix-csi-controller-8669b97dc8 2 2 2 9s 連線 Nutanix Volume Create nutanix-volume.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ vim nutanix-volume.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; name: nutanixvolume provisioner: csi.nutanix.com parameters: csi.storage.k8s.io/provisioner-secret-name: ntnx-secret csi.storage.k8s.io/provisioner-secret-namespace: ntnx-system csi.storage.k8s.io/node-publish-secret-name: ntnx-secret csi.storage.k8s.io/node-publish-secret-namespace: ntnx-system csi.storage.k8s.io/controller-expand-secret-name: ntnx-secret csi.storage.k8s.io/controller-expand-secret-namespace: ntnx-systems csi.storage.k8s.io/fstype: ext4 dataServiceEndPoint: 172.16.90.76:3260 # Prism data service ip storageContainer: NX_1365_AHV_cr # Prism 上建立的Storage Container storageType: NutanixVolumes #whitelistIPMode: ENABLED #chapAuth: ENABLED allowVolumeExpansion: true reclaimPolicy: Delete apply nutanix-volume.yaml\n1 2 3 4 5 6 $ oc apply -f nutanix-volume.yaml -n ntnx-system storageclass.storage.k8s.io/nutanix-volume created $ oc get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE nutanix-volume (default) csi.nutanix.com Delete Immediate true 45s 測試VG\n1 2 3 4 5 6 $ oc get pv,pvc -n ntnx-system NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS VOLUMEATTRIBUTESCLASS REASON AGE persistentvolume/pvc-49b7f489-05a1-479a-ada3-a1cd2ae76bfa 1Gi RWO Delete Bound ntnx-system/test-vg nutanix-volume \u0026lt;unset\u0026gt; 77s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE persistentvolumeclaim/test-vg Bound pvc-49b7f489-05a1-479a-ada3-a1cd2ae76bfa 1Gi RWO nutanix-volume \u0026lt;unset\u0026gt; 110s ","date":"2025-03-06T03:05:48+08:00","permalink":"https://wangken0129.github.io/p/ocp-4.16.3_on_nutanixipi/","title":"OCP-4.16.3_on_Nutanix(IPI)"},{"content":"通過Nutanix實現資料庫與DevOps 的融合 Youtube Link:\nhttps://www.youtube.com/watch?v=cYEByBhtra8\nAgenda 1.通過Nutanix實現資料庫與DevOps 的融合\n講師 鄭兆良 / 逸盈科技 技術經理\n2.demo\n講師 王軍凱 / 逸盈科技 工程師 \u0026ndash;\u0026gt; Ken\n3.新功能分享\n特別來賓 張兆祥 / Nutanix 大中華區關鍵應用系統資深顧問\n","date":"2025-03-03T07:52:01+08:00","permalink":"https://wangken0129.github.io/p/nutanix_webinar_%E9%80%9A%E9%81%8Enutanix%E5%AF%A6%E7%8F%BE%E8%B3%87%E6%96%99%E5%BA%AB%E8%88%87devops%E7%9A%84%E8%9E%8D%E5%90%88/","title":"Nutanix_Webinar_通過Nutanix實現資料庫與DevOps的融合"},{"content":"vSphere on Nutanix 透過Nutanix Foundation安裝vSphere ESXi，並執行後續的設定，\nFoundation步驟因為很簡單此處就不多加贅述，\n原則上會建議先把Nutanix Node加入vCenter，再將Nutanix Cluster建立好，\n因為加入vCenter時Node會需要進入維護模式，如有Nutanix Cluster運作時，\n變成要一台一台加入vCenter，執行速度上會比較慢。\n參考文件 https://portal.nutanix.com/page/documents/details?targetId=vSphere-Admin6-AOS-v6_5:vsp-cluster-introduction-vsphere-c.html\nFoundation 準備vSphere ESXi、vCenter的iso檔 利用mac的foundation安裝nutanix 安裝時改用vSphere的iso檔 如不是使用官方Support的版本,要提供md5 check sum 目前支援到vSphere ESXi 7.0u3c , 實測7.0u3g也可以安裝 安裝時間約50~60min 安裝後的設定 設定DNS Server\n設定NTP Server\n刪除Default Container,再新增一個 (因名稱太長)\n設定Virtual IP、Data Service IP\nDeploy vCenter or Registry to vCenter vCenter Setting\n先在vCenter Create Datacenter並新增Nutanix Cluster Cluster 開啟DRS、HA 確認Checklist都有設定完畢才能加入Nutanix Node HA\nEnable Host Monitoring\nSet the Host Isolation Response of the cluster to Power Off \u0026amp; Restart VMs. Enable admission control (%)\nSet the VM Restart Priority of all CVMs to Disabled. \u0026ndash; After register Set the VM Monitoring for all CVMs to Disabled. \u0026ndash; After register\nDatastore heartbeats \u0026ndash; After register\n(只有一個Datastore時,增加此設定) DRS\nSet the Automation Level on all CVMs to Disabled. \u0026ndash;After register Select Automation Level to accept level 3 recommendations.\nLeave power management disabled. Others\nConfigure advertised capacity for the Nutanix storage container (RF2) Store VM swapfiles in the same directory as the VM.\nhttps://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.resmgmt.doc/GUID-12B8E0FB-CD43-4972-AC2C-4B4E2955A5DA.html\nEnable enhanced vMotion compatibility (EVC) in the cluster. Configure Nutanix CVMs with the appropriate VM overrides. \u0026ndash;After register (HA、DRS、Monitoring Disabled)\nvCenter Adding Nutanix nodes\n實測第一台Node會被順利加到Cluster內,其他兩台需手動移入 補齊Checklist的設定\nRegister Nutanix Cluster to vCenter 安裝後的設定-PC https://portal.nutanix.com/page/documents/details?targetId=Acropolis-Upgrade-Guide:upg-vm-install-wc-r.html\ncreate PC (可用Online下載或是部署時上傳PC的檔案) PC建立好後即可用web連線 預設帳密 admin , Nutanix/4u 若無法登入web,可以在PE開PCVM的console,用nutanix / nutanix/4u的帳密登入 登入後切換身份為root 執行以下指令來重設admin密碼\n1 ncli user reset-password user-name=admin password=password 註冊Nutanix Cluster至Prism Central (PC) 註冊完成即可在PC上看到Nutanix Cluster Prism Central 設定NTP、DNS Server\nEnable Microserivces Infrastructure (可使用更多功能) https://portal.nutanix.com/page/documents/details?targetId=Prism-Central-Guide-vpc_2022_6:mul-cmsp-overview-pc-c.html\nvSphere Settings Checklist https://portal.nutanix.com/page/documents/details?targetId=vSphere-Admin6-AOS-v6_5:vsp-vcenter-settings-r.html\nReview the following checklist of the settings that you have to configure to successfully deploy vSphere virtual environment running Nutanix Enterprise cloud.\nvSphere Availability Settings Enable host monitoring.\nEnable admission control and use the percentage-based policy with a value based on the number of nodes in the cluster.\nFor more information about settings of percentage of cluster resources reserved as failover spare capacity, vSphere HA Admission Control Settings for Nutanix Environment.\nSet the VM Restart Priority of all CVMs to Disabled.\nSet the Host Isolation Response of the cluster to Power Off \u0026amp; Restart VMs.\nSet the VM Monitoring for all CVMs to Disabled.\nEnable datastore heartbeats by clicking Use datastores only from the specified list and choosing the Nutanix NFS datastore.\nIf the cluster has only one datastore, click Advanced Options tab and add das.ignoreInsufficientHbDatastore with Value of true.\nvSphere DRS Settings Set the Automation Level on all CVMs to Disabled. Select Automation Level to accept level 3 recommendations. Leave power management disabled. Other Cluster Settings Configure advertised capacity for the Nutanix storage container (total usable capacity minus the capacity of one node for replication factor 2 or two nodes for replication factor 3). Store VM swapfiles in the same directory as the VM. Enable enhanced vMotion compatibility (EVC) in the cluster. For more information, see vSphere EVC Settings. Configure Nutanix CVMs with the appropriate VM overrides. For more information, see VM Override Settings. Check Nonconfigurable ESXi Components. Modifying the nonconfigurable components may inadvertently constrain performance of your Nutanix cluster or render the Nutanix cluster inoperable. ","date":"2025-03-03T07:39:16+08:00","permalink":"https://wangken0129.github.io/p/vsphere_on_nutanix/","title":"vSphere_on_Nutanix"},{"content":"Nutanix NKP v2.13 Pre-Provisioned 環境 Pre-Provisioned 預配置的方式，可以讓 NKP 裝在任何環境，只要先安裝好 OS 以及基本的設定完成後\n即可部署 NKP 叢集，不論是實體機或是虛擬機都可以安裝。\n安裝大致流程 可調整資源、自定義設定，生產環境建議使用此方法 ，self-managed 僅支援此方法\n1.確認安裝環境\n2.連線 bootstrap host (Bastion Host)\n3.確認 Container Service\n4.確認 Kubernetes CLI\n5.確認 NKP CLI\n6.設定 Local Registry (air-gapped)\n7.推送 NKP Image 到 Local Registry (air-gapped)\n8.建立 bootstrap cluster (air-gapped or pre-provisioned or KIB or NIB )\n9.建立 machine image (optional)\n10.建立 management cluster\n環境資訊 此 LAB 僅用 1 個 control plane 加上 3 個 worker node 來測試，生產環境建議 3 個 control plane 加 4 個 worker node\nNKP Version : 2.13\nOS Image : Rocky 9.5\nBastion Host 準備 安裝主機 創建虛擬機 Rocky 9.1 ，記憶體最少 8GB，最小安裝即可\n環境準備 設定網路、主機名稱、SSH，為簡化安裝流程選擇停用防火牆（Pre-Provisioned的節點需停用） 、 關閉 SELinux，調整完後建議重開機確認，使用root or 一般使用者 + sudo\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 設定網路： # nmcli c s \u0026gt;\u0026gt; 顯示網卡名稱 # nmcli c m ens3 ipv4.method manual ipv4.addresses xxx.xx.xx.xx/24 ipv4.gateway xxx.xx.xx.xxx ipv4.dns 8.8.8.8 \u0026gt;\u0026gt; 設定ip,dns,gateway # nmcli c u ens3 \u0026gt;\u0026gt; 啟用網卡 主機名稱： # hostnamectl set-hostname nkp-bastion SSH Key： $ ssh-keygen \u0026gt;\u0026gt; 產生金鑰 $ ssh-copy-id -i .ssh/id_rsa.pub nkp@xxx.xx.xx.xx \u0026gt;\u0026gt; 複製金鑰到其他 Host 上 停用防火牆： # systemctl disable firewalld --now 關閉SELinux： # vi /etc/selinux/config SELINUX=disable Container Service \u0026amp; 工具 下載安裝NKP CLI、Container Service ( podman 4.0 or docker 20.10 以上 )、其他工具、Kubectl、KIB\n請先確認環境所需的 NKP 以及 Kubernetes 版本，此範例用 NKP 2.13 對應 Kubernetes 1.30.5\n1 2 3 4 5 6 7 8 9 10 Podman 下載安裝： $ sudo yum install -y podman $ podman version or Docker 下載安裝： $ sudo yum install -y docker-ce docker-ce-cli containerd.io 其他工具下載安裝： $ sudo yum install -y yum-utils bzip2 wget tar NKP CLI 下載安裝NKP CLI、Container Service ( podman 4.0 or docker 20.10 以上 )、其他工具、Kubectl、KIB\n請先確認環境所需的 NKP 以及 Kubernetes 版本，此範例用 NKP 2.13 對應 Kubernetes 1.30.5\n1 2 3 4 5 6 7 8 9 10 NKP CLI 下載： $ curl -o nkp_v.2.13.0.tar.gz “https://download.nutanix.com/urlxxx” NKP CLI 安裝： $ tar xvf nkp_v2.13.0.tar.gz $ ls -l $ sudo cp nkp /usr/local/bin $ nkp version Kubectl 1 2 3 4 5 6 7 Kubectl 安裝： https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.30.md#downloads-for-v1305 $ wget \u0026#34;https://dl.k8s.io/v1.30.5/kubernetes-client-linux-amd64.tar.gz\u0026#34; $ tar xvf kubernetes-client-linux-amd64.tar.gz $ sudo cp kubernetes/client/bin/kubectl /usr/local/bin/ $ kubectl version KIB 1 2 3 4 5 6 7 KIB Konvoy Image Builder 下載安裝 ( 有需要再裝 )： $ curl -o konvoy-image-bundle-v2.18.0.tar.gz ”https://url” $ tar xvf konvoy-image-bundle-v2.18.0.tar.gz $ ls -l $ sudo cp konvoy-image /usr/local/bin $ konvoy-image version 預配置主機準備 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Control Plane \u0026amp; Worker Machines 建議： control plane*3 : 4 cores , 16 GiB worker node*4 : 8 cores , 32 GiB root 目錄 15% 可用空間， /var/lib/kubelet 以及 /var/lib/containerd 95 GiB 以上空間 預設Storage Class 是用 localvolume 所以 /mnt/disks 底下至少要有mount 一顆 55GiB 硬碟 (官方建議4顆) 停用防火牆 firewalld、swap 防火牆需求開通 nutanix support portal root 或是不用密碼執行 sudo 的使用者，並先跟Bastion做金鑰交換 至少 1 個 Loadbalancer IP 或是設定外部 Loadbalance ( 含 api 及 Ingress ) OS images: Air-gapped 環境需先準備好用 Konvoy Image Builder 產生出需要的安裝包並用 konvoy upload 上傳至節點 Control Plane * 1： 8 vCPU , 8 GiB RAM , 120 GiB 根目錄 , 60 GiB /mnt/disks/nkp1 一顆 最小安裝,另外安裝 tar , 停用 firewalld , swap , selinux Worker Machines * 3 ： 4 vCPU , 8 GiB RAM , 120 GiB 根目錄 , 60 GiB /mnt/disks/nkp1 一顆 最小安裝,另外安裝 tar , 停用 firewalld , swap , selinux 部署 NKP Pre-provisioned 定義環境變數 此範例使用 root 使用者，如要其他使用者請先設定好Docker or Podman 權限\n並且在 control 與 worker 上要設定 /etc/sudoers “ ALL=(ALL:ALL) NOPASSWD: ALL”\n此為 single control plane 配置，生產環境建議至少 3 個 control plane，4 個 worker node\nStorage Class 建議用外部 Storage\n1 2 3 4 5 6 7 8 9 在 Bastion 上定義環境變數 export CONTROL_PLANE_1_ADDRESS=\u0026#34;172.16.90.131\u0026#34; export WORKER_1_ADDRESS=\u0026#34;172.16.90.132\u0026#34; export WORKER_2_ADDRESS=\u0026#34;172.16.90.133\u0026#34; export WORKER_3_ADDRESS=\u0026#34;172.16.90.134\u0026#34; export SSH_USER=\u0026#34;root\u0026#34; export SSH_PRIVATE_KEY_SECRET_NAME=\u0026#34;$CLUSTER_NAME-ssh-key\u0026#34; export CLUSTER_NAME=\u0026#34;nkp-pro-preprovisioned” \u0026gt;\u0026gt; .bashrc source .bashrc inventory.yaml 從 NKP 文件複製下來\n確認 preprovisioned_inventory.yaml 內容是否正確 vi preprovisioned_inventory.yaml\n確認control plane 、worker 數量及IP、secret 名稱等等\nbootstrap 1 2 3 4 5 6 7 建立 bootstrap cluster 並部署 preprovisioned_inventory.yaml # nkp create bootstrap # kubectl get pods -A # kubectl apply -f preprovisioned_inventory.yaml 建立 override.yaml 設定 docker.io or Registry 及帳號密碼 # kubectl create secret generic $CLUSTER_NAME-user-overrides --from-file=overrides.yaml=overrides.yaml # kubectl label secret $CLUSTER_NAME-user-overrides clusterctl.cluster.x-k8s.io/move= Dry run 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 部署NKP Preprovisioned Cluster (non-airgapped)，使用單節點或是MetalLB，依需求修改下方資訊 Kube-VIP 方式部署則需要加上--virtual-ip-interface eth1 nkp create cluster preprovisioned \\ --cluster-name=${CLUSTER_NAME} \\ --control-plane-endpoint-host=\u0026lt;control plane endpoint host\u0026gt; \\ --control-plane-replicas=1 \\ --worker-replicas=3 \\ --ssh-private-key-file=\u0026lt;path-to-ssh-private-key\u0026gt; \\ --registry-mirror-url=https://registry-1.docker.io \\ --registry-mirror-username=xxx \\ --registry-mirror-password=xxx \\ --dry-run \\ --output=yaml \u0026gt; deploy-nkp-$CLUSTER_NAME.yaml 檢查 deploy-nkp-$CLUSTER_NAME.yaml # vi deploy-nkp-nkp-pro-preprovisioned.yaml Create Cluster 1 2 3 4 5 6 7 部署 nkp cluster # kubectl create -f deploy-nkp-nkp-pro-preprovisioned.yaml 觀察狀態 # kubectl wait --for=condition=ControlPlaneReady \u0026#34;clusters/${CLUSTER_NAME}\u0026#34; --timeout=30m 確認 Cluster\n1 2 # kubectl get cluster # kubectl describe cluster 取得kubeconfig 1 2 3 4 取得kubeconfig檔案 # nkp get kubeconfig -c ${CLUSTER_NAME} \u0026gt; ${CLUSTER_NAME}.conf # kubectl get nodes --kubeconfig=${CLUSTER_NAME}.conf # kubectl get pods -A --kubeconfig=${CLUSTER_NAME}.conf Pivot 原bootstrap 部署出來只是 workload cluster，此動作是要將此叢集變成 Self-Manged or Management\n1 2 3 4 5 6 將 Cluster API 元件部署到 workload cluster # nkp create capi-components --kubeconfig=${CLUSTER_NAME}.conf 再將 life cycle services 從 bootstrap 移轉到 workload cluster，並確認NKP describe cluster # nkp move capi-resources --to-kubeconfig ${CLUSTER_NAME}.conf # nkp describe cluster --kubeconfig ${CLUSTER_NAME}.conf -c ${CLUSTER_NAME} MetalLB 如果沒有外部 LoadBalancer 的話，可以使用 MetalLB ，需設定 Loadbalancer IP Pool\nIP 網段需與 Machine 同網段，可以只設定一個 Ex. 192.168.1.240-192.168.1.240\n原廠文件提供範例，調整 addresses 後 apply 該設定檔\n# kubectl apply -f metallb-conf.yaml \u0026ndash;kubeconfig ${CLUSTER_NAME}.conf\nInstall Kommander 移除 bootstrap cluster\n# nkp delete bootstrap \u0026ndash;kubeconfig $HOME/.kube/config\n取得 kommander.yaml\n# nkp install kommander \u0026ndash;init \u0026gt; kommander.yaml\n預設 storage class 是 localvolumeprovisioner，volume mode 無法為 Block **， ** 故需修改 yaml 調整 rook-ceph-cluster **設定，**原則上建議使用外部 Storage Class ( NFS or Nutanix CSI )\n下方服務不需要的可以設成 false 最小安裝參照 https://portal.nutanix.com/page/documents/details?targetId=Nutanix-Kubernetes-Platform-v2_13:top-addl-kommander-config-c.html\n設定 **NKP Catalog Applications，並部署 **Kommander\n# nkp install kommander \u0026ndash;installer-config kommander.yaml \u0026ndash;kubeconfig=${CLUSTER_NAME}.conf \u0026ndash;wait-timeout=1h\n觀察安裝過程\n# watch kubectl get pod \u0026ndash;n kommander\n# kubectl -n kommander wait \u0026ndash;for condition=Ready helmreleases \u0026ndash;all \u0026ndash;timeout 15m\n取得 UI 連結、帳號密碼\n# nkp open dashboard \u0026ndash;kubeconfig=${CLUSTER_NAME}.conf\n","date":"2025-02-27T09:42:42+08:00","permalink":"https://wangken0129.github.io/p/nutanix_nkp-v2.13_preprovisioned/","title":"Nutanix_NKP-v2.13_Preprovisioned"},{"content":"Nutanix NKP v2.12 For AHV Nutanix 在2024年收購了D2iQ這間公司，並且將DKP重新調整為NKP\n讓Nutanix的客戶可以在原來的NCI基礎上使用多叢集管理的Kubernetes平台也取代了原本的NKE\n平台功能有包含叢集的監控與Log、負載平衡、SSO、生命週期管理、Service Mesh等等\n主要的競爭者應該為Rancher、Anthos 等等，並沒有像是OCP有開發者介面，所以還是著重在管理上。\n安裝選擇 NKP on Nutanix AHV (non-airgap)\nReference https://www.nutanix.com/tech-center/blog/nkp-kubernetes-done-the-nutanix-way\nhttps://www.nutanix.com/blog/nutanix-announces-nutanix-kubernetes-platform\nhttps://www.nutanix.com/products/kubernetes-management-platform\nhttps://portal.nutanix.com/page/documents/details?targetId=Nutanix-Kubernetes-Platform-v2_12:Nutanix-Kubernetes-Platform-v2_12\n（非官方）\nhttps://jonashogman.com/nkp-post-1-preparing-the-setup-environment/\nhttps://blog.ntnx.jp/entry/2024/09/16/235808\nArchitecture NKP 以目前的文件來看，可以安裝在Nutanix、vSphere、Bare metal (pre-provisioned)、EKS、AKS、Azure、GCP等等\n各安裝的方法以及需求皆有所不同 (airgap or non-airgap 這些都要考慮到)。\n架構主要分為三種型態，以及另一種 Self-Managed Cluster\nManagement Cluster：\n安裝 NKP 的上游叢集 (Upstream Cluster)，負責管理多個叢集，不負責使用者工作負載，\n可以管理部署出來的Managed Cluster，也可以管理連接上來的 Attached Cluster。\nManaged Cluster：\n由 NKP Management Cluster 部署出來的叢集，又稱 NKP Cluster ，主要負責使用者工作負載，\n並透過Management Cluster來管理生命週期跟應用程式。\nAttached Cluster：\n這是非NKP Management Cluster 部署出來的外部叢集，像是 EKS、AKS 或是其他認證的 Kubernetes 叢集(目前無認證清單)，\n以D2iQ的文件來說是All Kubernetes Conformant clusters (符合、一致的？)\n透過Management Cluster來管理應用程式，生命週期由原來的地方管理。\nSelf-Managed Cluster：\n一樣是裝 NKP，但非上游叢集，可以跑使用者工作負載，不能 Attach 其他叢集做管理。\n整體架構圖：\n多叢集環境架構圖：\nNKP Self-Managed Cluster：\nPrerequisites Prism Central Role (LAB環境建議是用admin)\n預設已有Kubernetes Infrastructure Provisions role\n需要新增Attach\u0026amp;Detach Volume Group To AHV VM、Update VM Disk、view container的權限、Create Volume Group Disk、\nView VM Stats、Category(預設的有此權限，但實際安裝發現是 CSI 使用API v4的去呼叫)、View Prism Central 等等\n並且新增一個 local user 設定為此角色即可\n這裡為 k8sadmin 這個user account\n新增Category權限(apiv4)\nResource Requirements 以下為各環境所需的資源，基本上還需另外一台Bastion作為安裝使用。\nNKP PRO / Ultimate Management Cluster： 3 control plane 、 4 worker node NKP PRO / Ultimate Managed Cluster： 3 control plane 、 4 worker node 3 worker node 適用於不啟用 velero or logging stack 的叢集。\nNKP Starter Cluster（僅適用於Nutanix 的環境）： 3 control plane、最低 2 worker node control plane 文件沒特別寫數量，但還是要三個，另外有支援single control plane的叢集，僅適用於測試環境。\nSofware Requirements For AHV 環境，NKP 相關的軟體可以在Nutanix Support Portal 上下載\nNKP Node Image\nNutanix 有包好 Rocky Linux for NKP 的 image，另外也可以選用Ubuntu 要另外用Konvoy Image Builder去做\n差別在於GPU Passthrough 是否支援，此 Lab 就用Rocky Linux實作\n在Support Portal 複製Download Link 上傳到Prism Central 即可 An x86_64 Linux or MacOS machine\nNKP Binary File\ncontainer runtime\nkubectl\nKonvoy Image Builder in KIB\nNKP air-gapped bundle -\u0026gt; Air-gapped Only (additional prerequisites)\nLocal image registry -\u0026gt; Air-gapped Only (additional prerequisites)\nNutanix prerequisites Prism Central : 2024.1 later\nAOS: 6.5 , 6.8 later\nIP Address for nodes : 7\nIP Address for API : 1\nIP Address for Traefik (metallb) : 1\nBastion Software 已預先安裝好RHEL v9.3 、Podman v4.2.0、kubectl v5.0.4 for v.1.28.0 ，並下載 nkp binary、konvoy-image-bundle\n另外要準備docker image的帳號密碼，不然下載檔案太多會被擋下來\nSSH-Key 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [nkp@ken-rhel9 ~]$ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/home/nkp/.ssh/id_rsa): Created directory \u0026#39;/home/nkp/.ssh\u0026#39;. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/nkp/.ssh/id_rsa Your public key has been saved in /home/nkp/.ssh/id_rsa.pub The key fingerprint is: SHA256:r5lyQDXM972bfYgElhrfz1bkTR2x9GwGIeIP918fbiI nkp@ken-rhel9.nutanixlab.local The key\u0026#39;s randomart image is: +---[RSA 3072]----+ | o . . o+.| | =... ..+o| | . ooo.. .B| | . . ++...o+| | . S= o. .*o| | . ... o o.B| | . .E = Oo| | . .+ o X o| | o+ . .| +----[SHA256]-----+ untar files 1 2 3 4 5 6 7 8 9 10 11 12 [nkp@ken-rhel9 ~]$ tar -xvf nkp_v2.12.0_linux_amd64.tar.gz NOTICES nkp [nkp@ken-rhel9 ~]$ tar -xvf konvoy-image-bundle-v2.13.2_linux_amd64.tar.gz LICENSE README.md ansible/.tool-versions ansible/.yamllint ... [nkp@ken-rhel9 ~]$ sudo mv nkp konvoy-image /usr/local/bin/ Installation nkp create cluster nutanix nkp create cluster\n1 2 3 4 5 [nkp@ken-rhel9 ~]$ nkp create cluster nutanix image registry: https://registry-1.docker.io Subnet 需要IPAM 切換成root、PC 新增container 權限\nCheck logs\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 [nkp@ken-rhel9 ~]$ kubectl --kubeconfig=\u0026#34;./nkp-mgmt.conf\u0026#34; get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE caaph-system caaph-controller-manager-7557bfbd76-25psr 1/1 Running 1 (149m ago) 15h capa-system capa-controller-manager-549fff5698-4js7x 1/1 Running 0 15h capg-system capg-controller-manager-54df78c867-ngvkg 1/1 Running 0 15h capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-589f87d995-ph2ll 1/1 Running 0 15h capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-869b46bd8d-ccbl4 1/1 Running 0 15h capi-system capi-controller-manager-7bd8c69994-6562l 1/1 Running 0 15h cappp-system cappp-controller-manager-6cc595974c-nphps 1/1 Running 0 15h capv-system capv-controller-manager-7c6679579f-qdvmx 1/1 Running 0 15h capvcd-system capvcd-controller-manager-647bbc5685-8x8p8 1/1 Running 0 15h capx-system capx-controller-manager-6b4c9976f6-zjw5r 1/1 Running 0 15h capz-system azureserviceoperator-controller-manager-697d757c4b-ffrcs 2/2 Running 0 15h capz-system capz-controller-manager-75c684766c-7z29w 1/1 Running 0 15h caren-system cluster-api-runtime-extensions-nutanix-869f6bc85d-wr864 1/1 Running 0 15h caren-system helm-repository-86c695db8f-w6xwc 1/1 Running 0 15h cert-manager cert-manager-c49657b87-hkgdf 1/1 Running 0 15h cert-manager cert-manager-cainjector-7b9b545679-l6pjf 1/1 Running 0 15h cert-manager cert-manager-webhook-647c6946df-cdtl9 1/1 Running 0 15h default cluster-autoscaler-01924266-3064-7c78-9b30-7e4445c87fa1-85txw8 l 1/1 Running 0 15h git-operator-system git-operator-admin-credentials-rotate-28795680-7fpm9 0/1 Completed 0 81m git-operator-system git-operator-controller-manager-585cc87d78-h47zp 2/2 Running 1 (80m ago) 15h git-operator-system git-operator-git-0 0/3 Pending 0 15h kommander-flux helm-controller-dc4455cd5-rlpdg 1/1 Running 0 15h kommander-flux kustomize-controller-755bbdfc55-dmf4z 1/1 Running 0 15h kommander-flux notification-controller-86c44d47f9-bbm9b 1/1 Running 0 15h kommander-flux source-controller-68bc9cbf4d-zgfd7 1/1 Running 0 15h kommander kommander-bootstrap-mmq8g 1/1 Running 0 15h kommander runtime-extension-kommander-59d9c676b7-fjqrv 1/1 Running 0 15h kube-system cilium-4vt4l 1/1 Running 0 15h kube-system cilium-7gjsw 1/1 Running 0 15h kube-system cilium-gkbcm 1/1 Running 0 15h kube-system cilium-operator-58c5d5d6d-dtknv 1/1 Running 0 15h kube-system cilium-operator-58c5d5d6d-f7x2w 1/1 Running 0 15h kube-system cilium-q7bnb 1/1 Running 0 15h kube-system cilium-qr94t 1/1 Running 0 15h kube-system cilium-r8k72 1/1 Running 0 15h kube-system cilium-wlvds 1/1 Running 0 15h kube-system coredns-76f75df574-wrg54 1/1 Running 0 15h kube-system coredns-76f75df574-zqcr8 1/1 Running 0 15h kube-system etcd-nkp-mgmt-xxl2n-7fdv6 1/1 Running 0 15h kube-system etcd-nkp-mgmt-xxl2n-cc69v 1/1 Running 0 15h kube-system etcd-nkp-mgmt-xxl2n-z5c49 1/1 Running 0 15h kube-system kube-apiserver-nkp-mgmt-xxl2n-7fdv6 1/1 Running 0 15h kube-system kube-apiserver-nkp-mgmt-xxl2n-cc69v 1/1 Running 0 15h kube-system kube-apiserver-nkp-mgmt-xxl2n-z5c49 1/1 Running 0 15h kube-system kube-controller-manager-nkp-mgmt-xxl2n-7fdv6 1/1 Running 0 15h kube-system kube-controller-manager-nkp-mgmt-xxl2n-cc69v 1/1 Running 0 15h kube-system kube-controller-manager-nkp-mgmt-xxl2n-z5c49 1/1 Running 0 15h kube-system kube-proxy-45pft 1/1 Running 0 15h kube-system kube-proxy-5dv8f 1/1 Running 0 15h kube-system kube-proxy-bh7hq 1/1 Running 0 15h kube-system kube-proxy-fm4gj 1/1 Running 0 15h kube-system kube-proxy-hkx6m 1/1 Running 0 15h kube-system kube-proxy-hmzqk 1/1 Running 0 15h kube-system kube-proxy-zvj58 1/1 Running 0 15h kube-system kube-scheduler-nkp-mgmt-xxl2n-7fdv6 1/1 Running 0 15h kube-system kube-scheduler-nkp-mgmt-xxl2n-cc69v 1/1 Running 0 15h kube-system kube-scheduler-nkp-mgmt-xxl2n-z5c49 1/1 Running 0 15h kube-system kube-vip-nkp-mgmt-xxl2n-7fdv6 1/1 Running 0 15h kube-system kube-vip-nkp-mgmt-xxl2n-cc69v 1/1 Running 0 15h kube-system kube-vip-nkp-mgmt-xxl2n-z5c49 1/1 Running 0 15h kube-system nutanix-cloud-controller-manager-598b4c7669-dd2nl 1/1 Running 0 15h kube-system snapshot-controller-5c7f9fc58-dlbm2 1/1 Running 0 15h metallb-system metallb-controller-94f95d674-24tnf 1/1 Running 0 15h metallb-system metallb-speaker-587jl 4/4 Running 0 15h metallb-system metallb-speaker-h4g7r 4/4 Running 0 15h metallb-system metallb-speaker-kxbp4 4/4 Running 0 15h metallb-system metallb-speaker-qxhsr 4/4 Running 0 15h metallb-system metallb-speaker-v8r8j 4/4 Running 0 15h metallb-system metallb-speaker-wdkbx 4/4 Running 0 15h metallb-system metallb-speaker-xqvbj 4/4 Running 0 15h node-feature-discovery node-feature-discovery-gc-7f54d58d99-slpr2 1/1 Running 0 15h node-feature-discovery node-feature-discovery-master-ccf75997b-86x9c 1/1 Running 0 15h node-feature-discovery node-feature-discovery-worker-6t9lf 1/1 Running 0 15h node-feature-discovery node-feature-discovery-worker-78c8h 1/1 Running 0 15h node-feature-discovery node-feature-discovery-worker-f25gw 1/1 Running 0 15h node-feature-discovery node-feature-discovery-worker-l6q5k 1/1 Running 0 15h node-feature-discovery node-feature-discovery-worker-nk99v 1/1 Running 0 15h node-feature-discovery node-feature-discovery-worker-rjj8w 1/1 Running 0 15h node-feature-discovery node-feature-discovery-worker-rq5dm 1/1 Running 0 15h ntnx-system nutanix-csi-precheck-job-nmkj7 0/1 Error 0 149m [nkp@ken-rhel9 ~]$ kubectl --kubeconfig=\u0026#34;./nkp-mgmt.conf\u0026#34; logs -f ^C [nkp@ken-rhel9 ~]$ kubectl --kubeconfig=\u0026#34;./nkp-mgmt.conf\u0026#34; logs nutanix-csi-precheck-job-nmkj7 -n ntnx-system W0930 22:51:08.489458 1 client_config.go:614] Neither --kubeconfig nor --master was specified. Using the inClust erConfig. This might not work. 2024-09-30T22:51:08.493Z client.go:227: [INFO] nutanix_files: rest request method:GET path: /cluster/version, data: \u0026lt;ni l\u0026gt; 2024-09-30T22:51:08.726Z prism_central.go:31: [INFO] PC Version Info pc.2024.1.0.1 2024-09-30T22:51:08.726Z main.go:252: [INFO] PC version:pc.2024.1.0.1 is supported 2024-09-30T22:51:08.74Z main.go:145: [WARN] Error getting the data from ConfigMap ntnx-cluster-configmap/ntnx-system : configmaps \u0026#34;ntnx-cluster-configmap\u0026#34; not found 2024-09-30T22:51:08.74Z main.go:164: [WARN] Failed to get categories from ntnx-system/ntnx-cluster-configmap, err: conf igmaps \u0026#34;ntnx-cluster-configmap\u0026#34; not found, will use kube-system uuid as category 2024-09-30T22:51:08.745Z main.go:174: [INFO] Cluster UUID: k8s-d92bdb3c-dc75-4132-a59d-61d5f3531913 2024-09-30T22:51:08.746Z categories.go:48: [INFO] creating a new category api client 2024-09-30T22:51:08.746Z main.go:200: [INFO] Category to be associated to CSI Provisioned Vol KubernetesClusterUUID/k8s -d92bdb3c-dc75-4132-a59d-61d5f3531913 2024-09-30T22:51:08.746Z categories_idempotent.go:35: [INFO] Retrieving categories for key \u0026#34;KubernetesClusterUUID\u0026#34; 2024-09-30 22:51:23.968 INFO - GET https://172.16.90.75:9440/api/prism/v4.0.b1/config/categories?%24filter=key+in+%28%2 7KubernetesClusterUUID%27%29 2024-09-30 22:51:23.979 INFO - HTTP/1.1 403 FORBIDDEN 2024-09-30T22:51:23.98Z categories.go:115: [ERROR] Failed to get all categories with status: 403 FORBIDDEN, error: {\u0026#34;da ta\u0026#34;:{\u0026#34;error\u0026#34;:[{\u0026#34;message\u0026#34;:\u0026#34;Access Denied because no or incorrect permissions were set in the request http://172.16.90.75 /api/prism/v4.0.b1/config/categories\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;RBA-10001\u0026#34;,\u0026#34;locale\u0026#34;:\u0026#34;en_US\u0026#34;,\u0026#34;errorGroup\u0026#34;:\u0026#34;RBAC_AUTHORIZATION_ERROR\u0026#34;,\u0026#34;seve rity\u0026#34;:\u0026#34;ERROR\u0026#34;,\u0026#34;$objectType\u0026#34;:\u0026#34;prism.v4.error.AppMessage\u0026#34;}],\u0026#34;$errorItemDiscriminator\u0026#34;:\u0026#34;List\u0026lt;prism.v4.error.AppMessage\u0026gt;\u0026#34;,\u0026#34; $objectType\u0026#34;:\u0026#34;prism.v4.error.ErrorResponse\u0026#34;},\u0026#34;$dataItemDiscriminator\u0026#34;:\u0026#34;prism.v4.error.ErrorResponse\u0026#34;} 2024-09-30T22:51:23.98Z categories_idempotent.go:94: [ERROR] failed to get category for fqName \u0026#34;KubernetesClusterUUID/k 8s-d92bdb3c-dc75-4132-a59d-61d5f3531913\u0026#34;, err: %!w(*fmt.wrapError=\u0026amp;{Failed to get all categories with status: 403 FORBI DDEN, error: {\u0026#34;data\u0026#34;:{\u0026#34;error\u0026#34;:[{\u0026#34;message\u0026#34;:\u0026#34;Access Denied because no or incorrect permissions were set in the request ht tp://172.16.90.75/api/prism/v4.0.b1/config/categories\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;RBA-10001\u0026#34;,\u0026#34;locale\u0026#34;:\u0026#34;en_US\u0026#34;,\u0026#34;errorGroup\u0026#34;:\u0026#34;RBAC_AUTHORIZA TION_ERROR\u0026#34;,\u0026#34;severity\u0026#34;:\u0026#34;ERROR\u0026#34;,\u0026#34;$objectType\u0026#34;:\u0026#34;prism.v4.error.AppMessage\u0026#34;}],\u0026#34;$errorItemDiscriminator\u0026#34;:\u0026#34;List\u0026lt;prism.v4.err or.AppMessage\u0026gt;\u0026#34;,\u0026#34;$objectType\u0026#34;:\u0026#34;prism.v4.error.ErrorResponse\u0026#34;},\u0026#34;$dataItemDiscriminator\u0026#34;:\u0026#34;prism.v4.error.ErrorResponse\u0026#34;} {[123 34 100 97 116 97 34 58 123 34 101 114 114 111 114 34 58 91 123 34 109 101 115 115 97 103 101 34 58 34 65 99 99 10 1 115 115 32 68 101 110 105 101 100 32 98 101 99 97 117 115 101 32 110 111 32 111 114 32 105 110 99 111 114 114 101 99 116 32 112 101 114 109 105 115 115 105 111 110 115 32 119 101 114 101 32 115 101 116 32 105 110 32 116 104 101 32 114 1 01 113 117 101 115 116 32 104 116 116 112 58 47 47 49 55 50 46 49 54 46 57 48 46 55 53 47 97 112 105 47 112 114 105 115 109 47 118 52 46 48 46 98 49 47 99 111 110 102 105 103 47 99 97 116 101 103 111 114 105 101 115 34 44 34 99 111 100 10 1 34 58 34 82 66 65 45 49 48 48 48 49 34 44 34 108 111 99 97 108 101 34 58 34 101 110 95 85 83 34 44 34 101 114 114 111 114 71 114 111 117 112 34 58 34 82 66 65 67 95 65 85 84 72 79 82 73 90 65 84 73 79 78 95 69 82 82 79 82 34 44 34 115 1 01 118 101 114 105 116 121 34 58 34 69 82 82 79 82 34 44 34 36 111 98 106 101 99 116 84 121 112 101 34 58 34 112 114 10 5 115 109 46 118 52 46 101 114 114 111 114 46 65 112 112 77 101 115 115 97 103 101 34 125 93 44 34 36 101 114 114 111 1 14 73 116 101 109 68 105 115 99 114 105 109 105 110 97 116 111 114 34 58 34 76 105 115 116 60 112 114 105 115 109 46 11 8 52 46 101 114 114 111 114 46 65 112 112 77 101 115 115 97 103 101 62 34 44 34 36 111 98 106 101 99 116 84 121 112 101 34 58 34 112 114 105 115 109 46 118 52 46 101 114 114 111 114 46 69 114 114 111 114 82 101 115 112 111 110 115 101 34 125 44 34 36 100 97 116 97 73 116 101 109 68 105 115 99 114 105 109 105 110 97 116 111 114 34 58 34 112 114 105 115 109 46 118 52 46 101 114 114 111 114 46 69 114 114 111 114 82 101 115 112 111 110 115 101 34 125] \u0026lt;nil\u0026gt; 403 FORBIDDEN}}) 2024-09-30T22:51:23.98Z categories_idempotent.go:74: [ERROR] Failed to create category key for fqName \u0026#34;KubernetesCluste rUUID/k8s-d92bdb3c-dc75-4132-a59d-61d5f3531913\u0026#34; ,err: %!w(string=Max retries done: Failed to get all categories with st atus: 403 FORBIDDEN, error: {\u0026#34;data\u0026#34;:{\u0026#34;error\u0026#34;:[{\u0026#34;message\u0026#34;:\u0026#34;Access Denied because no or incorrect permissions were set in the request http://172.16.90.75/api/prism/v4.0.b1/config/categories\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;RBA-10001\u0026#34;,\u0026#34;locale\u0026#34;:\u0026#34;en_US\u0026#34;,\u0026#34;errorGroup\u0026#34;: \u0026#34;RBAC_AUTHORIZATION_ERROR\u0026#34;,\u0026#34;severity\u0026#34;:\u0026#34;ERROR\u0026#34;,\u0026#34;$objectType\u0026#34;:\u0026#34;prism.v4.error.AppMessage\u0026#34;}],\u0026#34;$errorItemDiscriminator\u0026#34;:\u0026#34;Li st\u0026lt;prism.v4.error.AppMessage\u0026gt;\u0026#34;,\u0026#34;$objectType\u0026#34;:\u0026#34;prism.v4.error.ErrorResponse\u0026#34;},\u0026#34;$dataItemDiscriminator\u0026#34;:\u0026#34;prism.v4.error.E rrorResponse\u0026#34;}) 2024-09-30T22:51:23.98Z main.go:209: [ERROR] Error creating categories, err: %!w(*errors.Error=\u0026amp;{1 0xc0003a2e80 [0xc000 39e2c0]}) 調整權限後再跑一次\n新增View Prism Central、update VM basic config、Volume Group權限\n完成\n連線NKP Dashboard\n1 2 3 4 5 6 7 8 [root@ken-rhel9 ~]# nkp get dashboard --kubeconfig=\u0026#34;/root/nkp-mgmt.conf\u0026#34; Username: crazy_boyd Password: HYokkgBhIU5WnjBfWoCfxwtBQ4WwP8APuw13fnDKA4MZ9eGz1Gbq3ddyiUkT4OiW URL: https://172.16.90.209/dkp/kommander/dashboard 編輯cluster nkp --kubeconfig=/root/nkp-mgmt.conf edit cluster nkp-workload -n nkp-workload-9x6hx-2k79n 登入測試\nNKP Starter 畫面擷取 Dashboard\n可以新增Cluster\nInfrastructure Providers\n這裡不能修改或新增\nIdentity Providers\n可以整合Github、LDAP、OIDC、SAML等\nAccess Control 預設有 Admin 跟 View Role Role Bindings是使用跟LDAP或是OIDC等等Group結合\nLicensing，安裝在 Nutanix 上都有 NKP Starter ，但功能陽春\nResource Alerts 可以設定CPU、Memory、Disk的使用量閥值 但在Starter看起來只會顯示在 UI 上面，UI 沒有其他可以設定寄送告警的地方 這功能可以在 PC 上面設定閥值\nPC Project畫面\n點進去Management Cluster\n沒有kubernetes Dashboard 只能看Namespaces有哪些，無法編輯\nCreate Workload Cluster 用Control plane + worker node 各一台部署一個測試環境叢集\nCreate Cluster，可以選擇 UI或是 yaml\n輸入Cluster name : nkp-workload 添加Labels 、其他無法選擇\n數入 SSH Key、 PC Project、PE Subnets 資訊 這裏PC Project 我有建立但沒抓到，不確定多久會抓一次資訊\nOS Image 、 Control Plan IP 及數量\nWorker Node 資訊、Storage\n容器網路\nImage Registry 先留空白\n開始部署\n安裝完成\n整體畫面，UI 沒有升級的地方\n要從指令去操作，nkp 指令有提供 update, upgrade\nManagement 整個叢集的pod數量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 [root@ken-rhel9 ~]# kubectl --kubeconfig=nkp-mgmt.conf get nodes NAME STATUS ROLES AGE VERSION nkp-mgmt-hzxnp-28zsd Ready control-plane 127m v1.29.6 nkp-mgmt-hzxnp-cnjn6 Ready control-plane 125m v1.29.6 nkp-mgmt-hzxnp-gg88v Ready control-plane 124m v1.29.6 nkp-mgmt-md-0-l965p-fczv8-mspnw Ready \u0026lt;none\u0026gt; 125m v1.29.6 nkp-mgmt-md-0-l965p-fczv8-n6tlc Ready \u0026lt;none\u0026gt; 126m v1.29.6 nkp-mgmt-md-0-l965p-fczv8-t5xmd Ready \u0026lt;none\u0026gt; 125m v1.29.6 nkp-mgmt-md-0-l965p-fczv8-w7sxk Ready \u0026lt;none\u0026gt; 125m v1.29.6 [root@ken-rhel9 ~]# kubectl --kubeconfig=nkp-mgmt.conf get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE caaph-system caaph-controller-manager-7557bfbd76-cdx7d 1/1 Running 0 120m capa-system capa-controller-manager-549fff5698-9zdlh 1/1 Running 0 120m capg-system capg-controller-manager-54df78c867-7zqlw 1/1 Running 0 120m capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-589f87d995-gqjh6 1/1 Running 0 120m capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-869b46bd8d-tqmr5 1/1 Running 0 120m capi-system capi-controller-manager-7bd8c69994-4c5h9 1/1 Running 0 120m cappp-system cappp-controller-manager-6cc595974c-dlj27 1/1 Running 0 120m capv-system capv-controller-manager-7c6679579f-sj4vc 1/1 Running 0 120m capvcd-system capvcd-controller-manager-647bbc5685-74qnv 1/1 Running 0 120m capx-system capx-controller-manager-6b4c9976f6-f29bg 1/1 Running 0 120m capz-system azureserviceoperator-controller-manager-697d757c4b-bpwmm 2/2 Running 0 120m capz-system capz-controller-manager-75c684766c-mp5bb 1/1 Running 0 120m caren-system cluster-api-runtime-extensions-nutanix-869f6bc85d-clcgv 1/1 Running 0 120m caren-system helm-repository-86c695db8f-t7cgs 1/1 Running 0 120m cert-manager cert-manager-c49657b87-kvv2q 1/1 Running 0 122m cert-manager cert-manager-cainjector-7b9b545679-w867w 1/1 Running 0 122m cert-manager cert-manager-webhook-647c6946df-qwvnk 1/1 Running 0 122m default cluster-autoscaler-01924623-02df-76cb-a348-158120974c65-8ff7rp7 1/1 Running 0 126m git-operator-system git-operator-controller-manager-585cc87d78-bgl8w 2/2 Running 0 115m git-operator-system git-operator-git-0 3/3 Running 0 115m kommander-default-workspace karma-traefik-certs-kommander-default-workspace-cert-federj7xjn 1/1 Running 0 107m kommander-default-workspace kubecost-traefik-certs-kommander-default-workspace-cert-fe25f2t 1/1 Running 0 107m kommander-default-workspace prometheus-traefik-certs-kommander-default-workspace-cert-fxpv6 1/1 Running 0 107m kommander-flux helm-controller-dc4455cd5-frdrw 1/1 Running 0 116m kommander-flux kustomize-controller-755bbdfc55-kd8rh 1/1 Running 0 116m kommander-flux notification-controller-86c44d47f9-nzvf7 1/1 Running 0 116m kommander-flux source-controller-68bc9cbf4d-fn7x5 1/1 Running 0 116m kommander cluster-observer-2360587938-5fc8b86cd6-9kpgn 1/1 Running 0 106m kommander dex-5ff688ddc5-bs87d 1/1 Running 0 13m kommander dex-dex-controller-84656677b7-t7jtq 2/2 Running 0 105m kommander dex-k8s-authenticator-cd94b9bfb-t6jq5 1/1 Running 0 13m kommander gatekeeper-audit-57b899b497-msjzc 1/1 Running 0 113m kommander gatekeeper-controller-manager-84456b75f-5r8h4 1/1 Running 0 113m kommander gatekeeper-controller-manager-84456b75f-nf66v 1/1 Running 0 113m kommander karma-traefik-certs-kommander-cert-federation-7b7bbdf54f-jdcfw 1/1 Running 0 107m kommander kommander-appmanagement-677d49d6d4-bwgrw 2/2 Running 0 111m kommander kommander-appmanagement-webhook-765c8c99f-cknr5 1/1 Running 0 111m kommander kommander-authorizedlister-69586f8664-tl5wq 1/1 Running 0 108m kommander kommander-bootstrap-kjrcc 0/1 Completed 0 126m kommander kommander-capimate-68db846987-d9br9 1/1 Running 0 108m kommander kommander-capimate-68db846987-kp44s 1/1 Running 0 107m kommander kommander-cm-6cdb956ddc-pbqcn 2/2 Running 0 108m kommander kommander-flux-operator-84976cdbd7-l6mkt 2/2 Running 0 108m kommander kommander-kommander-ui-594c784fb9-fb5rp 1/1 Running 0 107m kommander kommander-licensing-cm-79db86b6f9-hfh9b 2/2 Running 0 108m kommander kommander-licensing-webhook-8f48d69f4-n8n2t 1/1 Running 0 108m kommander kommander-operator-5ddcf7797f-9mw9k 1/1 Running 0 113m kommander kommander-reloader-reloader-68fb77b665-nw854 1/1 Running 0 109m kommander kommander-traefik-7bbc46f4f5-6bvxs 1/1 Running 0 109m kommander kommander-traefik-7bbc46f4f5-7nxt8 1/1 Running 0 109m kommander kommander-webhook-7b758645bd-xkk57 1/1 Running 0 108m kommander kube-oidc-proxy-7b8ccdf676-svvqs 1/1 Running 0 105m kommander kubecost-traefik-certs-kommander-cert-federation-c455687b-q8dzq 1/1 Running 0 107m kommander prometheus-traefik-certs-kommander-cert-federation-677d4b4dlnmz 1/1 Running 0 107m kommander runtime-extension-kommander-59d9c676b7-vdk76 1/1 Running 0 120m kommander traefik-forward-auth-mgmt-7b4848595b-z4kpq 1/1 Running 0 100m kube-federation-system kubefed-admission-webhook-575c45986d-9c4ff 1/1 Running 0 109m kube-federation-system kubefed-controller-manager-c6d6989b8-77bwd 1/1 Running 0 108m kube-federation-system kubefed-controller-manager-c6d6989b8-7m48r 1/1 Running 0 108m kube-system cilium-2s6jn 1/1 Running 0 126m kube-system cilium-5rssh 1/1 Running 0 124m kube-system cilium-mswcf 1/1 Running 0 124m kube-system cilium-n7h7f 1/1 Running 0 124m kube-system cilium-operator-58c5d5d6d-sxxp8 1/1 Running 0 126m kube-system cilium-operator-58c5d5d6d-vknvd 1/1 Running 0 126m kube-system cilium-p89dz 1/1 Running 0 123m kube-system cilium-ph4px 1/1 Running 0 125m kube-system cilium-prc67 1/1 Running 0 124m kube-system coredns-76f75df574-tnk7v 1/1 Running 0 126m kube-system coredns-76f75df574-w84hk 1/1 Running 0 126m kube-system etcd-nkp-mgmt-hzxnp-28zsd 1/1 Running 0 126m kube-system etcd-nkp-mgmt-hzxnp-cnjn6 1/1 Running 0 124m kube-system etcd-nkp-mgmt-hzxnp-gg88v 1/1 Running 0 123m kube-system kube-apiserver-nkp-mgmt-hzxnp-28zsd 1/1 Running 0 126m kube-system kube-apiserver-nkp-mgmt-hzxnp-cnjn6 1/1 Running 0 124m kube-system kube-apiserver-nkp-mgmt-hzxnp-gg88v 1/1 Running 0 123m kube-system kube-controller-manager-nkp-mgmt-hzxnp-28zsd 1/1 Running 0 126m kube-system kube-controller-manager-nkp-mgmt-hzxnp-cnjn6 1/1 Running 0 124m kube-system kube-controller-manager-nkp-mgmt-hzxnp-gg88v 1/1 Running 0 123m kube-system kube-proxy-76jcm 1/1 Running 0 124m kube-system kube-proxy-96pjt 1/1 Running 0 124m kube-system kube-proxy-9qrlm 1/1 Running 0 123m kube-system kube-proxy-cxw6r 1/1 Running 0 125m kube-system kube-proxy-jkl95 1/1 Running 0 124m kube-system kube-proxy-kr5wg 1/1 Running 0 126m kube-system kube-proxy-ljlhh 1/1 Running 0 124m kube-system kube-scheduler-nkp-mgmt-hzxnp-28zsd 1/1 Running 0 126m kube-system kube-scheduler-nkp-mgmt-hzxnp-cnjn6 1/1 Running 0 124m kube-system kube-scheduler-nkp-mgmt-hzxnp-gg88v 1/1 Running 0 123m kube-system kube-vip-nkp-mgmt-hzxnp-28zsd 1/1 Running 1 (126m ago) 126m kube-system kube-vip-nkp-mgmt-hzxnp-cnjn6 1/1 Running 0 124m kube-system kube-vip-nkp-mgmt-hzxnp-gg88v 1/1 Running 0 123m kube-system nutanix-cloud-controller-manager-598b4c7669-4q8j8 1/1 Running 0 126m kube-system snapshot-controller-5c7f9fc58-7zjjg 1/1 Running 0 126m metallb-system metallb-controller-94f95d674-48z4s 1/1 Running 0 126m metallb-system metallb-speaker-6lclp 4/4 Running 0 122m metallb-system metallb-speaker-c78xt 4/4 Running 0 123m metallb-system metallb-speaker-cnljr 4/4 Running 0 123m metallb-system metallb-speaker-dklnr 4/4 Running 0 124m metallb-system metallb-speaker-fmbzv 4/4 Running 0 123m metallb-system metallb-speaker-nwq8b 4/4 Running 0 125m metallb-system metallb-speaker-nxhz6 4/4 Running 0 124m nkp-workload-9x6hx-2k79n cluster-autoscaler-0192467d-9888-7536-8266-ed9e557d46a1-66qkh7z 1/1 Running 1 (15m ago) 27m nkp-workload-9x6hx-2k79n cluster-observer-1774060518-68b9989d54-dzcqt 1/1 Running 0 27m nkp-workload-9x6hx-2k79n karma-traefik-certs-nkp-workload-9x6hx-2k79n-cert-federatish7kf 1/1 Running 0 28m nkp-workload-9x6hx-2k79n kubecost-traefik-certs-nkp-workload-9x6hx-2k79n-cert-federn6pf9 1/1 Running 0 28m nkp-workload-9x6hx-2k79n prometheus-traefik-certs-nkp-workload-9x6hx-2k79n-cert-fedh9vpc 1/1 Running 0 29m node-feature-discovery node-feature-discovery-gc-7f54d58d99-zvgnb 1/1 Running 0 126m node-feature-discovery node-feature-discovery-master-ccf75997b-dp7q4 1/1 Running 0 126m node-feature-discovery node-feature-discovery-worker-42bxq 1/1 Running 0 124m node-feature-discovery node-feature-discovery-worker-478dl 1/1 Running 0 124m node-feature-discovery node-feature-discovery-worker-79lmw 1/1 Running 0 123m node-feature-discovery node-feature-discovery-worker-7kvw6 1/1 Running 0 123m node-feature-discovery node-feature-discovery-worker-7tvvn 1/1 Running 0 122m node-feature-discovery node-feature-discovery-worker-dlj8p 1/1 Running 0 123m node-feature-discovery node-feature-discovery-worker-hpbmp 1/1 Running 0 125m ntnx-system nutanix-csi-controller-754fcf5f85-c9vtt 7/7 Running 3 (119m ago) 122m ntnx-system nutanix-csi-controller-754fcf5f85-sfrpb 7/7 Running 3 (119m ago) 122m ntnx-system nutanix-csi-node-5d9gs 3/3 Running 0 122m ntnx-system nutanix-csi-node-8qhkx 3/3 Running 1 (121m ago) 122m ntnx-system nutanix-csi-node-dbxm8 3/3 Running 1 (121m ago) 122m ntnx-system nutanix-csi-node-qvkv9 3/3 Running 0 122m ntnx-system nutanix-csi-node-s2wn7 3/3 Running 1 (122m ago) 122m ntnx-system nutanix-csi-node-tq5bt 3/3 Running 1 (120m ago) 122m ntnx-system nutanix-csi-node-vz87g 3/3 Running 0 122m 其他NKP , kubectl 指令、加金鑰\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 [root@ken-rhel9 ~]# nkp get workspace --kubeconfig=\u0026#34;/root/nkp-mgmt.conf\u0026#34; NAME NAMESPACE default-workspace kommander-default-workspace kommander-workspace kommander nkp-workload-9x6hx nkp-workload-9x6hx-2k79n [root@ken-rhel9 ~]# nkp get cluster -w kommander-workspace --kubeconfig=\u0026#34;/root/nkp-mgmt.conf\u0026#34; WORKSPACE NAME KUBECONFIG STATUS kommander-workspace host-cluster kommander-self-attach-kubeconfig Joined [root@ken-rhel9 ~]# nkp get cluster -w default-workspace --kubeconfig=\u0026#34;/root/nkp-mgmt.conf\u0026#34; WORKSPACE NAME KUBECONFIG STATUS [root@ken-rhel9 ~]# nkp get cluster -w nkp-workload-9x6hx --kubeconfig=\u0026#34;/root/nkp-mgmt.conf\u0026#34; WORKSPACE NAME KUBECONFIG STATUS nkp-workload-9x6hx nkp-workload nkp-workload-kubeconfig Joined 編輯cluster nkp --kubeconfig=/root/nkp-mgmt.conf edit cluster nkp-workload -n nkp-workload-9x6hx-2k79n [root@ken-rhel9 ~]# nkp get cluster -w nkp-workload-lab --kubeconfig=\u0026#34;/root/nkp-mgmt.conf\u0026#34; WORKSPACE NAME KUBECONFIG STATUS nkp-workload-lab\tnkp-cluster01\tnkp-cluster01-kubeconfig\tJoined\t[root@ken-rhel9 ~]# nkp describe cluster -w nkp-workload-lab --kubeconfig=\u0026#34;/root/nkp-mgmt.conf\u0026#34; [root@ken-rhel9 ~]# nkp edit cluster nkp-cluster01 -n nkp-workload-lab --kubeconfig=\u0026#34;/root/nkp-mgmt.conf\u0026#34; Users: Name: nkp Ssh Authorized Keys: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCVN+q1hhDCFrbW4Gw+wPryGl2BQbLi9NNOv+hXLgI+NXJqhTT0XEL6Uun5WKd7ceNznfiCHee2AkQXm9nrvVyPpFU5zQ91j9zQbMsFylSDv7RALFyEaXO45u6bzKRvVnL4lsKygBcMwxSW8yIrv0qTeDl3rChfkxjhpWR+C7gbn6uIVINRoC/5L1xNGcThbF0CiHgOt5P03ZV4tsm6C7Z65Wj41vczaH460qqq//kaf2uNveOZhi4juIQMc0FcE4kvGwtqRJ5HIs8CRq1N/wX+uIIQEKsqtmQqSujcHOwosYPSNvVXvVgqvy9t0jfEU4Y3bcBkoF5zfq98OCG0978ewjb8yR909bQM2s2QRoQML8Gb6Be+EV7+xtUcqZOpAkyZOmWXN8yoL5EMcelgDZoyK3yH9FgKuz1sbB0f+4b1B8LM/SULTq8Lv7ww6ps3EIvKDwbH5rMQIxf3IEIuk7qvjyQlAZiE2nxiU88fcPB54WUoQBNd3uUjr0YVt1i5GM0= Sudo: ALL=(ALL) NOPASSWD:ALL Version: v1.29.6 Workers: Machine Deployments: Class: default-worker Metadata: Annotations: cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: 4 cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: 4 Name: md-0 Variables: Overrides: Name: workerConfig Value: Nutanix: Machine Details: Boot Type: uefi Cluster: Name: NX1365G6PE Type: name Image: Name: nkp-rocky-9.4-release-1.29.6-20240816215147.qcow2 Type: name Memory Size: 32Gi Project: Name: nkp-management Type: name Subnets: Name: Netfos_90_Access_IPAM Type: name System Disk Size: 80Gi Vcpu Sockets: 8 Vcpus Per Socket: 1 [root@ken-rhel9 ~]# cat /home/nkp/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCVN+q1hhDCFrbW4Gw+wPryGl2BQbLi9NNOv+hXLgI+NXJqhTT0XEL6Uun5WKd7ceNznfiCHee2AkQXm9nrvVyPpFU5zQ91j9zQbMsFylSDv7RALFyEaXO45u6bzKRvVnL4lsKygBcMwxSW8yIrv0qTeDl3rChfkxjhpWR+C7gbn6uIVINRoC/5L1xNGcThbF0CiHgOt5P03ZV4tsm6C7Z65Wj41vczaH460qqq//kaf2uNveOZhi4juIQMc0FcE4kvGwtqRJ5HIs8CRq1N/wX+uIIQEKsqtmQqSujcHOwosYPSNvVXvVgqvy9t0jfEU4Y3bcBkoF5zfq98OCG0978ewjb8yR909bQM2s2QRoQML8Gb6Be+EV7+xtUcqZOpAkyZOmWXN8yoL5EMcelgDZoyK3yH9FgKuz1sbB0f+4b1B8LM/SULTq8Lv7ww6ps3EIvKDwbH5rMQIxf3IEIuk7qvjyQlAZiE2nxiU88fcPB54WUoQBNd3uUjr0YVt1i5GM0= nkp@ken-rhel9.nutanixlab.local [root@ken-rhel9 ~]# kubectl --kubeconfig=nkp-mgmt.conf describe cluster nkp-mgmt Name: nkp-mgmt Namespace: default Labels: cluster.x-k8s.io/cluster-name=nkp-mgmt cluster.x-k8s.io/provider=nutanix konvoy.d2iq.io/cluster-name=nkp-mgmt konvoy.d2iq.io/provider=nutanix topology.cluster.x-k8s.io/owned= Annotations: caren.nutanix.com/cluster-uuid: 01924623-02df-76cb-a348-158120974c65 API Version: cluster.x-k8s.io/v1beta1 Kind: Cluster Metadata: Creation Timestamp: 2024-10-01T03:44:26Z Finalizers: cluster.cluster.x-k8s.io Generation: 2 Resource Version: 9910 UID: b1e6c594-7132-40cd-ae06-80623ce81de0 Spec: Cluster Network: Pods: Cidr Blocks: 192.168.0.0/16 Services: Cidr Blocks: 10.96.0.0/12 Control Plane Endpoint: Host: 172.16.90.208 Port: 6443 Control Plane Ref: API Version: controlplane.cluster.x-k8s.io/v1beta1 Kind: KubeadmControlPlane Name: nkp-mgmt-hzxnp Namespace: default Infrastructure Ref: API Version: infrastructure.cluster.x-k8s.io/v1beta1 Kind: NutanixCluster Name: nkp-mgmt-lbhrg Namespace: default Topology: Class: nkp-nutanix Control Plane: Metadata: Replicas: 3 Variables: Name: clusterConfig Value: Addons: Ccm: Credentials: Secret Ref: Name: nkp-mgmt-pc-credentials Strategy: HelmAddon Cluster Autoscaler: Strategy: HelmAddon Cni: Provider: Cilium Strategy: HelmAddon Csi: Default Storage: Provider: nutanix Storage Class Config: volume Providers: Nutanix: Credentials: Secret Ref: Name: nkp-mgmt-pc-credentials-for-csi Storage Class Configs: Volume: Allow Expansion: false Parameters: csi.storage.k8s.io/fstype: ext4 Description: CSI StorageClass nutanix-volume for nkp-mgmt Flash Mode: DISABLED Hypervisor Attached: ENABLED Storage Container: NX_1365_AHV_cr Storage Type: NutanixVolumes Reclaim Policy: Delete Volume Binding Mode: WaitForFirstConsumer Strategy: HelmAddon Snapshot Controller: Strategy: HelmAddon Nfd: Strategy: HelmAddon Service Load Balancer: Configuration: Address Ranges: End: 172.16.90.209 Start: 172.16.90.209 Provider: MetalLB Control Plane: Nutanix: Machine Details: Boot Type: uefi Cluster: Name: NX1365G6PE Type: name Image: Name: nkp-rocky-9.4-release-1.29.6-20240816215147.qcow2 Type: name Memory Size: 16Gi Project: Name: nkp-management Type: name Subnets: Name: Netfos_90_Access_IPAM Type: name System Disk Size: 80Gi Vcpu Sockets: 4 Vcpus Per Socket: 1 Encryption At Rest: Providers: Secretbox: Image Registries: Credentials: Secret Ref: Name: nkp-mgmt-image-registry-credentials URL: https://registry-1.docker.io Nutanix: Control Plane Endpoint: Host: 172.16.90.208 Port: 6443 Virtual IP: Provider: KubeVIP Prism Central Endpoint: Credentials: Secret Ref: Name: nkp-mgmt-pc-credentials Insecure: true URL: https://172.16.90.75:9440 Users: Name: nkp Ssh Authorized Keys: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCVN+q1hhDCFrbW4Gw+wPryGl2BQbLi9NNOv+hXLgI+NXJqhTT0XEL6Uun5WKd7ceNznfiCHee2AkQXm9nrvVyPpFU5zQ 91j9zQbMsFylSDv7RALFyEaXO45u6bzKRvVnL4lsKygBcMwxSW8yIrv0qTeDl3rChfkxjhpWR+C7gbn6uIVINRoC/5L1xNGcThbF0CiHgOt5P03ZV4tsm6C7Z65Wj41vczaH460qqq//ka f2uNveOZhi4juIQMc0FcE4kvGwtqRJ5HIs8CRq1N/wX+uIIQEKsqtmQqSujcHOwosYPSNvVXvVgqvy9t0jfEU4Y3bcBkoF5zfq98OCG0978ewjb8yR909bQM2s2QRoQML8Gb6Be+EV7+xt UcqZOpAkyZOmWXN8yoL5EMcelgDZoyK3yH9FgKuz1sbB0f+4b1B8LM/SULTq8Lv7ww6ps3EIvKDwbH5rMQIxf3IEIuk7qvjyQlAZiE2nxiU88fcPB54WUoQBNd3uUjr0YVt1i5GM0= Sudo: ALL=(ALL) NOPASSWD:ALL Version: v1.29.6 Workers: Machine Deployments: Class: default-worker Metadata: Annotations: cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: 4 cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: 4 Name: md-0 Variables: Overrides: Name: workerConfig Value: Nutanix: Machine Details: Boot Type: uefi Cluster: Name: NX1365G6PE Type: name Image: Name: nkp-rocky-9.4-release-1.29.6-20240816215147.qcow2 Type: name Memory Size: 32Gi Project: Name: nkp-management Type: name Subnets: Name: Netfos_90_Access_IPAM Type: name System Disk Size: 80Gi Vcpu Sockets: 8 Vcpus Per Socket: 1 Status: Conditions: Last Transition Time: 2024-10-01T03:44:45Z Status: True Type: Ready Last Transition Time: 2024-10-01T03:44:39Z Status: True Type: ControlPlaneInitialized Last Transition Time: 2024-10-01T03:44:45Z Status: True Type: ControlPlaneReady Last Transition Time: 2024-10-01T03:44:42Z Status: True Type: InfrastructureReady Last Transition Time: 2024-10-01T03:46:51Z Status: True Type: KommanderInitialized Last Transition Time: 2024-10-01T03:44:41Z Status: True Type: TopologyReconciled Control Plane Ready: true Infrastructure Ready: true Observed Generation: 2 Phase: Provisioned Events: \u0026lt;none\u0026gt; [root@ken-rhel9 ~]# kubectl --kubeconfig=nkp-mgmt.conf describe cluster nkp-mgmt |grep UID UID: b1e6c594-7132-40cd-ae06-80623ce81de0 License 在Support Portal 可以看到有購買的License\n點選Manage Licenses ，選擇 Nutanix Kubernetes Plaform\n輸入NKP 叢集名稱、UUID (optional)\n選擇License NKP Ultimate\n選擇License的種類，這裡是POC License\n選擇叢集\n選擇 vCPU 數量\n點選Save 然後 Next\n最後確認完成\n點選Generate license keys\n即可產生 License key\n1 2 3 AEAAQ-AAAQS-L5N22-FU6P2-6NGKV-9QKLU-UF8S7 renew AEAAQ-AAASU-57YDC-4PVGR-EJ233-N5AKR-3T5XG renew\nSupport Portal 即可看到使用的License\n回到NKP Dashboard 的 Licensing 頁面，點選移除License\nrenew\n未授權狀態，可以看到的畫面與NKP Starter 一樣\nrenew ，機器人的button會被拿掉，但其他正常\n點選 Activate License，輸入 License key\nrenew\n授權匯入完成\nrenew 完成，機器人就正常了\nLicense 到期前告警\nLicesne 到期後影響：AI機器人無法使用，其他應用程式看起來正常，按鈕沒被拔掉，Dashboard 也正常\nAI 機器人壞掉，服務應該跟License無關，但修復後有跳出License異常，所以到期後應該是無法使用AI機器人\n修復動作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # 先將scale 調整成0 [nkp@ken-rhel9 ~]$ kubectl scale deployment ai-navigator-cluster-info-api -n kommander --replicas=0 # 刪除失敗的pods 用Label來表示所有相關的pods [nkp@ken-rhel9 ~]$ kubectl delete pod -l app.kubernetes.io/name=ai-navigator-cluster-info-api -n kommander pod \u0026#34;ai-navigator-cluster-info-api-6b78d6449-226cv\u0026#34; deleted pod \u0026#34;ai-navigator-cluster-info-api-6b78d6449-249pv\u0026#34; deleted pod \u0026#34;ai-navigator-cluster-info-api-6b78d6449-2564c\u0026#34; deleted pod \u0026#34;ai-navigator-cluster-info-api-6b78d6449-267s7\u0026#34; deleted ... 最後會卡著要用 Control+C 退出 # 刪除完成 [nkp@ken-rhel9 ~]$ kubectl get pods -n kommander NAME READY STATUS RESTARTS AGE ai-navigator-app-554f86cd46-797tq 1/1 Running 0 54d ai-navigator-cluster-info-agent-76bfb65f49-qdrqx 1/1 Running 0 38d ai-navigator-cluster-info-api-postgresql-0 1/1 Running 0 54d alertmanager-kube-prometheus-stack-alertmanager-0 2/2 Running 0 54d centralized-grafana-7d7b874996-w658s 2/2 Running 0 54d cluster-observer-2360587938-5fc8b86cd6-9kpgn 1/1 Running 0 54d # 刪除完將scale 調整回5 [nkp@ken-rhel9 ~]$ kubectl scale deployment ai-navigator-cluster-info-api -n kommander --replicas=5 [nkp@ken-rhel9 ~]$ kubectl get pods -n kommander NAME READY STATUS RESTARTS AGE ai-navigator-app-554f86cd46-797tq 1/1 Running 0 54d ai-navigator-cluster-info-agent-76bfb65f49-qdrqx 1/1 Running 0 38d ai-navigator-cluster-info-api-557c646c6b-5tkfj 0/1 Init:0/1 0 4s ai-navigator-cluster-info-api-557c646c6b-7sdmk 0/1 Init:0/1 0 4s ai-navigator-cluster-info-api-557c646c6b-htnr7 0/1 Init:0/1 0 4s ai-navigator-cluster-info-api-557c646c6b-wbn4g 0/1 Init:0/1 0 4s ai-navigator-cluster-info-api-557c646c6b-xkb8d 0/1 Init:0/1 0 4s # 還是不行，修正錯誤，describe po 之後發現SizeLimit 超用，並且image有更新版本 [nkp@ken-rhel9 ~]$ kubectl describe pod ai-navigator-cluster-info-api-6c65f767b8-f5cgl -n kommander [nkp@ken-rhel9 ~]$ kubectl edit deployment ai-navigator-cluster-info-api -n kommander deployment.apps/ai-navigator-cluster-info-api edited image 改為 image: mesosphere/ai-navigator-cluster-info-api:v0.1.0-model-revision-fix Volumes SizeLimit改為 8Gi - emptyDir: sizeLimit: 8Gi # 修正ＯＫ [nkp@ken-rhel9 ~]$ kubectl get pods -n kommander NAME READY STATUS RESTARTS AGE ai-navigator-app-554f86cd46-797tq 1/1 Running 0 54d ai-navigator-cluster-info-agent-76bfb65f49-qdrqx 1/1 Running 0 38d ai-navigator-cluster-info-api-6c65f767b8-f5cgl 1/1 Running 0 3m4s ai-navigator-cluster-info-api-6c65f767b8-jpn96 1/1 Running 0 3m4s ai-navigator-cluster-info-api-6c65f767b8-tx69k 1/1 Running 0 3m4s ai-navigator-cluster-info-api-6c65f767b8-vm926 0/1 Running 0 3m4s ai-navigator-cluster-info-api-6c65f767b8-xwsbx [nkp@ken-rhel9 ~]$ kubectl logs ai-navigator-cluster-info-api-6c65f767b8-f5cgl -n kommander . 2024-11-25 02:37:24,745 urllib3.connectionpool DEBUG http://weaviate:80 \u0026#34;GET /v1/meta HTTP/1.1\u0026#34; 200 64 2024-11-25 02:37:24,752 urllib3.connectionpool DEBUG Starting new HTTPS connection (1): pypi.org:443 2024-11-25 02:37:24,946 urllib3.connectionpool DEBUG https://pypi.org:443 \u0026#34;GET /pypi/weaviate-client/json HTTP/1.1\u0026#34; 200 61151 /app/.venv/lib/python3.11/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.25.3. The latest version is 4.9.4. Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details. warnings.warn( INFO: Started server process [1] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit) kubectl rollout restart deployment ai-navigator-cluster-info-agent -n kommander kubectl rollout restart deployment ai-navigator-app -n kommander kubectl rollout restart deployment ai-navigator-cluster-info-api -n kommander AI 服務正常，但有跳出License Error (有使用就會多一筆)\nkubectl logs ai-navigator-app-5945846b57-5c6gl -n kommander |grep licesne\nNKP Ultimate 畫面擷取 Dashboard\nClusters\nWorkspaces\nInfrastructure Providers 可以新增其他的 （原先的會被移除，要重新加入）\nAccess Control 多了一些預設角色，也可以新增其他的角色 Identity Providers 與 NKP Starter 相同\nBanners\n點進去Management Cluster workspaces\n各元件也都有Dashboard可查看\nKubernetes Dashboard 除了看到比較詳細的畫面外，也可以直接從 UI 部署新資源\nGrafana\nTraefik\nPrometheus ，這邊就可以設定告警值等等\nAI 測試\nApplication\nkubecost\nCeph\n所有Pod 資訊 ，相較於 NKP Starter 多了很多東西 AI 也是匯入授權後自動建立\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 [root@ken-rhel9 ~]# kubectl --kubeconfig=nkp-mgmt.conf get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE caaph-system caaph-controller-manager-7557bfbd76-cdx7d 1/1 Running 0 3h15m capa-system capa-controller-manager-549fff5698-9zdlh 1/1 Running 0 3h15m capg-system capg-controller-manager-54df78c867-7zqlw 1/1 Running 0 3h15m capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-589f87d995-gqjh6 1/1 Running 0 3h15m capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-869b46bd8d-tqmr5 1/1 Running 0 3h15m capi-system capi-controller-manager-7bd8c69994-4c5h9 1/1 Running 0 3h15m cappp-system cappp-controller-manager-6cc595974c-dlj27 1/1 Running 0 3h15m capv-system capv-controller-manager-7c6679579f-sj4vc 1/1 Running 0 3h15m capvcd-system capvcd-controller-manager-647bbc5685-74qnv 1/1 Running 0 3h15m capx-system capx-controller-manager-6b4c9976f6-f29bg 1/1 Running 0 3h15m capz-system azureserviceoperator-controller-manager-697d757c4b-bpwmm 2/2 Running 0 3h15m capz-system capz-controller-manager-75c684766c-mp5bb 1/1 Running 0 3h15m caren-system cluster-api-runtime-extensions-nutanix-869f6bc85d-clcgv 1/1 Running 0 3h15m caren-system helm-repository-86c695db8f-t7cgs 1/1 Running 0 3h15m cert-manager cert-manager-c49657b87-kvv2q 1/1 Running 0 3h17m cert-manager cert-manager-cainjector-7b9b545679-w867w 1/1 Running 0 3h17m cert-manager cert-manager-webhook-647c6946df-qwvnk 1/1 Running 0 3h17m default cluster-autoscaler-01924623-02df-76cb-a348-158120974c65-8ff7rp7 1/1 Running 0 3h21m git-operator-system git-operator-controller-manager-585cc87d78-bgl8w 2/2 Running 0 3h10m git-operator-system git-operator-git-0 3/3 Running 0 3h10m kommander-default-workspace karma-traefik-certs-kommander-default-workspace-cert-federj7xjn 1/1 Running 0 3h2m kommander-default-workspace kubecost-traefik-certs-kommander-default-workspace-cert-fe25f2t 1/1 Running 0 3h2m kommander-default-workspace prometheus-traefik-certs-kommander-default-workspace-cert-fxpv6 1/1 Running 0 3h2m kommander-flux helm-controller-dc4455cd5-frdrw 1/1 Running 0 3h11m kommander-flux kustomize-controller-755bbdfc55-kd8rh 1/1 Running 0 3h11m kommander-flux notification-controller-86c44d47f9-nzvf7 1/1 Running 0 3h11m kommander-flux source-controller-68bc9cbf4d-fn7x5 1/1 Running 0 3h11m kommander ai-navigator-app-554f86cd46-797tq 1/1 Running 0 13m kommander ai-navigator-cluster-info-api-6b78d6449-fh756 1/1 Running 0 11m kommander ai-navigator-cluster-info-api-6b78d6449-fn6pn 1/1 Running 0 11m kommander ai-navigator-cluster-info-api-postgresql-0 1/1 Running 0 11m kommander alertmanager-kube-prometheus-stack-alertmanager-0 2/2 Running 0 9m kommander centralized-grafana-7d7b874996-w658s 2/2 Running 0 9m30s kommander cluster-observer-2360587938-5fc8b86cd6-9kpgn 1/1 Running 0 3h1m kommander create-kommander-thanos-query-stores-configmap-mblkt 0/1 Completed 0 11m kommander dex-5ff688ddc5-bs87d 1/1 Running 0 88m kommander dex-dex-controller-84656677b7-t7jtq 2/2 Running 0 3h kommander dex-k8s-authenticator-cd94b9bfb-t6jq5 1/1 Running 0 88m kommander dkp-ceph-prereq-job-f7wqt 0/2 Completed 0 11m kommander etcd-metrics-proxy-4hm42 1/1 Running 0 12m kommander etcd-metrics-proxy-4p8r8 1/1 Running 0 12m kommander etcd-metrics-proxy-mxnlz 1/1 Running 0 12m kommander gatekeeper-audit-57b899b497-msjzc 1/1 Running 0 3h8m kommander gatekeeper-controller-manager-84456b75f-5r8h4 1/1 Running 0 3h8m kommander gatekeeper-controller-manager-84456b75f-nf66v 1/1 Running 0 3h8m kommander grafana-logging-7b655598b9-99gqn 2/2 Running 0 12m kommander grafana-loki-pre-install-x56fg 1/1 Running 0 12m kommander karma-5f8cf7cb76-hllxr 1/1 Running 0 11m kommander karma-traefik-certs-kommander-cert-federation-7b7bbdf54f-jdcfw 1/1 Running 0 3h2m kommander kommander-appmanagement-677d49d6d4-bwgrw 2/2 Running 0 3h6m kommander kommander-appmanagement-webhook-765c8c99f-cknr5 1/1 Running 0 3h6m kommander kommander-authorizedlister-69586f8664-tl5wq 1/1 Running 0 3h3m kommander kommander-bootstrap-kjrcc 0/1 Completed 0 3h21m kommander kommander-capimate-68db846987-d9br9 1/1 Running 0 3h3m kommander kommander-capimate-68db846987-kp44s 1/1 Running 0 3h3m kommander kommander-cm-6cdb956ddc-pbqcn 2/2 Running 0 3h3m kommander kommander-flux-operator-84976cdbd7-l6mkt 2/2 Running 0 3h3m kommander kommander-kommander-ui-594c784fb9-fb5rp 1/1 Running 0 3h2m kommander kommander-licensing-cm-79db86b6f9-hfh9b 2/2 Running 0 3h3m kommander kommander-licensing-webhook-8f48d69f4-n8n2t 1/1 Running 0 3h3m kommander kommander-operator-5ddcf7797f-9mw9k 1/1 Running 0 3h9m kommander kommander-reloader-reloader-68fb77b665-nw854 1/1 Running 0 3h4m kommander kommander-traefik-7bbc46f4f5-6bvxs 1/1 Running 0 3h4m kommander kommander-traefik-7bbc46f4f5-7nxt8 1/1 Running 0 3h4m kommander kommander-webhook-7b758645bd-xkk57 1/1 Running 0 3h3m kommander kube-oidc-proxy-7b8ccdf676-svvqs 1/1 Running 0 3h kommander kube-prometheus-stack-grafana-5c965665bf-zgvft 2/2 Running 0 9m24s kommander kube-prometheus-stack-kube-state-metrics-8c5d9876b-wnf9s 1/1 Running 0 9m24s kommander kube-prometheus-stack-operator-8458bfb4df-jstdm 1/1 Running 0 9m24s kommander kube-prometheus-stack-prometheus-node-exporter-42dg4 1/1 Running 0 9m24s kommander kube-prometheus-stack-prometheus-node-exporter-5k45l 1/1 Running 0 9m24s kommander kube-prometheus-stack-prometheus-node-exporter-87q9v 1/1 Running 0 9m24s kommander kube-prometheus-stack-prometheus-node-exporter-fbpj9 1/1 Running 0 9m24s kommander kube-prometheus-stack-prometheus-node-exporter-hpkzs 1/1 Running 0 9m24s kommander kube-prometheus-stack-prometheus-node-exporter-jdnq4 1/1 Running 0 9m24s kommander kube-prometheus-stack-prometheus-node-exporter-n8cdq 1/1 Running 0 9m24s kommander kubecost-cost-analyzer-5874fc47d5-5587w 2/2 Running 0 12m kommander kubecost-grafana-96d5947cd-kllf4 3/3 Running 0 12m kommander kubecost-kube-state-metrics-6645c45576-d2lmv 1/1 Running 0 12m kommander kubecost-prometheus-alertmanager-55f9cf456b-89w8z 2/2 Running 0 12m kommander kubecost-prometheus-server-776b57c895-8nbmn 3/3 Running 0 12m kommander kubecost-traefik-certs-kommander-cert-federation-c455687b-q8dzq 1/1 Running 0 3h2m kommander kubernetes-dashboard-api-5f5b7994c-kzbgt 1/1 Running 0 9m7s kommander kubernetes-dashboard-auth-8c7546cfc-hcfdn 1/1 Running 0 9m7s kommander kubernetes-dashboard-kong-699fffc989-znmdv 1/1 Running 0 9m7s kommander kubernetes-dashboard-metrics-scraper-5998c4d7d-pshrt 1/1 Running 0 9m7s kommander kubernetes-dashboard-web-6f4466f486-ww949 1/1 Running 0 9m7s kommander kubetunnel-59598865c5-7q97c 1/1 Running 0 12m kommander kubetunnel-webhook-55d9974f85-nlv86 1/1 Running 0 12m kommander logging-operator-5497588957-2x4hs 1/1 Running 0 11m kommander logging-operator-logging-fluentbit-52p52 1/1 Running 0 3m23s kommander logging-operator-logging-fluentbit-88mbm 1/1 Running 0 3m23s kommander logging-operator-logging-fluentbit-8pszh 0/1 ContainerCreating 0 3m23s kommander logging-operator-logging-fluentbit-drgdp 1/1 Running 0 3m23s kommander logging-operator-logging-fluentbit-dsbct 1/1 Running 0 3m23s kommander logging-operator-logging-fluentbit-prhkn 1/1 Running 0 3m23s kommander logging-operator-logging-fluentbit-vnrjl 1/1 Running 0 3m23s kommander logging-operator-logging-fluentd-0 0/3 ContainerCreating 0 3m24s kommander logging-operator-logging-fluentd-configcheck-82fdd7bc 0/1 Completed 0 7m35s kommander nkp-insights-management-mgmt-cm-7d4dddd996-l8gr8 1/1 Running 0 9m56s kommander prometheus-adapter-cd86fdf97-8fppm 1/1 Running 0 6m kommander prometheus-kube-prometheus-stack-prometheus-0 3/3 Running 0 8m59s kommander prometheus-traefik-certs-kommander-cert-federation-677d4b4dlnmz 1/1 Running 0 3h2m kommander rook-ceph-detect-version-v5mk2 0/1 PodInitializing 0 4m2s kommander rook-ceph-operator-7bb9c5bb9f-gvgjs 1/1 Running 0 9m4s kommander rook-ceph-tools-5995cd86bd-6b9xk 0/1 ContainerCreating 0 4m3s kommander runtime-extension-kommander-59d9c676b7-vdk76 1/1 Running 0 3h15m kommander thanos-query-84b7859896-5fmc4 1/1 Running 0 10m kommander traefik-forward-auth-mgmt-7b4848595b-z4kpq 1/1 Running 0 175m kommander velero-pre-install-bqxxz 1/1 Running 0 11m kommander weaviate-0 1/1 Running 0 11m kube-federation-system kubefed-admission-webhook-575c45986d-9c4ff 1/1 Running 0 3h4m kube-federation-system kubefed-controller-manager-c6d6989b8-77bwd 1/1 Running 0 3h3m kube-federation-system kubefed-controller-manager-c6d6989b8-7m48r 1/1 Running 0 3h3m kube-system cilium-2s6jn 1/1 Running 0 3h21m kube-system cilium-5rssh 1/1 Running 0 3h19m kube-system cilium-mswcf 1/1 Running 0 3h20m kube-system cilium-n7h7f 1/1 Running 0 3h19m kube-system cilium-operator-58c5d5d6d-sxxp8 1/1 Running 0 3h21m kube-system cilium-operator-58c5d5d6d-vknvd 1/1 Running 0 3h21m kube-system cilium-p89dz 1/1 Running 0 3h18m kube-system cilium-ph4px 1/1 Running 0 3h20m kube-system cilium-prc67 1/1 Running 0 3h19m kube-system coredns-76f75df574-tnk7v 1/1 Running 0 3h21m kube-system coredns-76f75df574-w84hk 1/1 Running 0 3h21m kube-system etcd-nkp-mgmt-hzxnp-28zsd 1/1 Running 0 3h21m kube-system etcd-nkp-mgmt-hzxnp-cnjn6 1/1 Running 0 3h20m kube-system etcd-nkp-mgmt-hzxnp-gg88v 1/1 Running 0 3h18m kube-system kube-apiserver-nkp-mgmt-hzxnp-28zsd 1/1 Running 0 3h21m kube-system kube-apiserver-nkp-mgmt-hzxnp-cnjn6 1/1 Running 0 3h20m kube-system kube-apiserver-nkp-mgmt-hzxnp-gg88v 1/1 Running 0 3h18m kube-system kube-controller-manager-nkp-mgmt-hzxnp-28zsd 1/1 Running 0 3h21m kube-system kube-controller-manager-nkp-mgmt-hzxnp-cnjn6 1/1 Running 0 3h20m kube-system kube-controller-manager-nkp-mgmt-hzxnp-gg88v 1/1 Running 0 3h18m kube-system kube-proxy-76jcm 1/1 Running 0 3h19m kube-system kube-proxy-96pjt 1/1 Running 0 3h19m kube-system kube-proxy-9qrlm 1/1 Running 0 3h18m kube-system kube-proxy-cxw6r 1/1 Running 0 3h20m kube-system kube-proxy-jkl95 1/1 Running 0 3h20m kube-system kube-proxy-kr5wg 1/1 Running 0 3h21m kube-system kube-proxy-ljlhh 1/1 Running 0 3h19m kube-system kube-scheduler-nkp-mgmt-hzxnp-28zsd 1/1 Running 0 3h21m kube-system kube-scheduler-nkp-mgmt-hzxnp-cnjn6 1/1 Running 0 3h20m kube-system kube-scheduler-nkp-mgmt-hzxnp-gg88v 1/1 Running 0 3h18m kube-system kube-vip-nkp-mgmt-hzxnp-28zsd 1/1 Running 1 (3h21m ago) 3h21m kube-system kube-vip-nkp-mgmt-hzxnp-cnjn6 1/1 Running 0 3h19m kube-system kube-vip-nkp-mgmt-hzxnp-gg88v 1/1 Running 0 3h18m kube-system nutanix-cloud-controller-manager-598b4c7669-4q8j8 1/1 Running 0 3h21m kube-system snapshot-controller-5c7f9fc58-7zjjg 1/1 Running 0 3h21m kubecost copy-kubecost-grafana-datasource-cm-mv7cm 0/1 Completed 0 9m33s kubecost create-kubecost-thanos-query-stores-configmap-j84tx 0/1 Completed 0 12m kubecost kommander-kubecost-cost-analyzer-dbbfd778c-pfs97 2/2 Running 0 10m kubecost kommander-kubecost-thanos-query-656956b46b-xmltk 1/1 Running 0 10m kubecost kommander-kubecost-thanos-query-frontend-8f5774f6f-xsznt 1/1 Running 0 10m metallb-system metallb-controller-94f95d674-48z4s 1/1 Running 0 3h21m metallb-system metallb-speaker-6lclp 4/4 Running 0 3h18m metallb-system metallb-speaker-c78xt 4/4 Running 0 3h18m metallb-system metallb-speaker-cnljr 4/4 Running 0 3h18m metallb-system metallb-speaker-dklnr 4/4 Running 0 3h20m metallb-system metallb-speaker-fmbzv 4/4 Running 0 3h18m metallb-system metallb-speaker-nwq8b 4/4 Running 0 3h20m metallb-system metallb-speaker-nxhz6 4/4 Running 0 3h19m nkp-workload-9x6hx-2k79n cluster-autoscaler-0192467d-9888-7536-8266-ed9e557d46a1-66qkh7z 1/1 Running 1 (91m ago) 102m nkp-workload-9x6hx-2k79n cluster-observer-1774060518-68b9989d54-dzcqt 1/1 Running 0 103m nkp-workload-9x6hx-2k79n karma-traefik-certs-nkp-workload-9x6hx-2k79n-cert-federatish7kf 1/1 Running 0 104m nkp-workload-9x6hx-2k79n kubecost-traefik-certs-nkp-workload-9x6hx-2k79n-cert-federn6pf9 1/1 Running 0 104m nkp-workload-9x6hx-2k79n prometheus-traefik-certs-nkp-workload-9x6hx-2k79n-cert-fedh9vpc 1/1 Running 0 104m node-feature-discovery node-feature-discovery-gc-7f54d58d99-zvgnb 1/1 Running 0 3h21m node-feature-discovery node-feature-discovery-master-ccf75997b-dp7q4 1/1 Running 0 3h21m node-feature-discovery node-feature-discovery-worker-42bxq 1/1 Running 0 3h19m node-feature-discovery node-feature-discovery-worker-478dl 1/1 Running 0 3h20m node-feature-discovery node-feature-discovery-worker-79lmw 1/1 Running 0 3h18m node-feature-discovery node-feature-discovery-worker-7kvw6 1/1 Running 0 3h18m node-feature-discovery node-feature-discovery-worker-7tvvn 1/1 Running 0 3h18m node-feature-discovery node-feature-discovery-worker-dlj8p 1/1 Running 0 3h18m node-feature-discovery node-feature-discovery-worker-hpbmp 1/1 Running 0 3h20m ntnx-system nutanix-csi-controller-754fcf5f85-c9vtt 7/7 Running 3 (3h14m ago) 3h18m ntnx-system nutanix-csi-controller-754fcf5f85-sfrpb 7/7 Running 3 (3h14m ago) 3h18m ntnx-system nutanix-csi-node-5d9gs 3/3 Running 0 3h18m ntnx-system nutanix-csi-node-8qhkx 3/3 Running 1 (3h17m ago) 3h18m ntnx-system nutanix-csi-node-dbxm8 3/3 Running 1 (3h16m ago) 3h18m ntnx-system nutanix-csi-node-qvkv9 3/3 Running 0 3h18m ntnx-system nutanix-csi-node-s2wn7 3/3 Running 1 (3h17m ago) 3h18m ntnx-system nutanix-csi-node-tq5bt 3/3 Running 1 (3h16m ago) 3h18m ntnx-system nutanix-csi-node-vz87g 3/3 Running 0 3h18m Infrastructure Providers Nutanix Add Cluster Attach OCP Enable Dashboard\nEnable Kubecost\nLDAP KB-16668\n連線 AD 設定RBAC\n新增 Identitiy Providers ，選擇 LDAP\n輸入資訊\n1 2 3 4 5 Workspaces: All Workspaces # 也可以針對不同workspaces去設定 Name: dc01.nutanixlab.local # 可辨識名稱即可 Host: 192.168.102.1:389 # IP + Port Bind DN: CN=Administrator,CN=Users,DC=nutanixlab,DC=local # 綁定的管理員帳號 Bind Password: **** 輸入資訊\n1 2 3 4 5 6 Root CA: # 如果有的話才需要 User Search Base DN: DC=nutanixlab,DC=local # 搜尋整個目錄 User Search Username: sAMAccountName # 登入會輸入的帳號格式 User Search Filter: (objectClass=person) User Search Scope: sub # 整個樹系或是一個Level User Search ID Attribute: DN 輸入資訊\n1 2 3 4 5 6 7 User Search E-Mail: mail # 或是UPN User Search Name: name # 登入後右上角顯示的名稱 User Search E-mail Suffix: # 留空 Group Search Base DN: CN=Users,DC=nutanixlab,DC=local # 搜尋Group的地方 Group Search Filter: (objectClass=group) # Group的object class Group Search Scope: sub Group Search Name Attribute: cn # 顯示的group名稱 設定完成 Save\n新增Group\nAccess Control 新增 Role Bindings\n登入測試 dc01.nutanixlab.local\n其他使用者登入驗證OK\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 [nkp@ken-rhel9 ~]$ kubectl get Connector.dex.mesosphere.io -n kommander -o yaml apiVersion: v1 items: - apiVersion: dex.mesosphere.io/v1alpha1 kind: Connector metadata: creationTimestamp: \u0026#34;2024-10-04T06:37:26Z\u0026#34; generateName: ldap-identity-provider- generation: 4 name: ldap-identity-provider-hg4bx namespace: kommander resourceVersion: \u0026#34;8127529\u0026#34; uid: 709bdf88-b4d5-44be-98ba-0f3b5ee70db3 spec: displayName: dc01.nutanixlab.local enabled: true ldap: bindDN: CN=Administrator,CN=Users,DC=nutanixlab,DC=local bindPW: Nutanix/Lab123 bindSecretRef: name: connector-ldap-bindsecret-lf6zq groupSearch: baseDN: CN=Users,DC=nutanixlab,DC=local filter: (objectClass=group) nameAttr: cn scope: \u0026#34;\u0026#34; userMatchers: - groupAttr: member userAttr: DN host: 192.168.102.1:389 insecureNoSSL: true insecureSkipVerify: true startTLS: false userSearch: baseDN: DC=nutanixlab,DC=local emailAttr: mail emailSuffix: \u0026#34;\u0026#34; filter: (objectClass=person) idAttr: DN nameAttr: name scope: sub username: sAMAccountName type: ldap kind: List metadata: resourceVersion: \u0026#34;\u0026#34; Check Logs\n1 2 3 4 5 6 7 8 [nkp@ken-rhel9 ~]$ kubectl logs dex-55978d4bcc-ksgsg -n kommander time=\u0026#34;2024-10-04T07:06:36Z\u0026#34; level=info msg=\u0026#34;Dex Version: v2.37.0-d2iq.5-dirty, Go Version: go1.20.4, Go OS/ARCH: linux amd64\u0026#34; time=\u0026#34;2024-10-04T07:06:36Z\u0026#34; level=info msg=\u0026#34;config using log level: debug\u0026#34; time=\u0026#34;2024-10-04T07:06:36Z\u0026#34; level=info msg=\u0026#34;config issuer: https://172.16.90.209/dex\u0026#34; ... time=\u0026#34;2024-10-17T03:34:21Z\u0026#34; level=info msg=\u0026#34;username \\\u0026#34;ken.wang\\\u0026#34; mapped to entry CN=Ken Wang,OU=NutanixTeam,OU=NetfosTaipei,DC=nutanixlab,DC=local\u0026#34; time=\u0026#34;2024-10-17T03:34:21Z\u0026#34; level=info msg=\u0026#34;performing ldap search CN=Users,DC=nutanixlab,DC=local sub (\u0026amp;(objectClass=group)(member=CN=Ken Wang,OU=NutanixTeam,OU=NetfosTaipei,DC=nutanixlab,DC=local))\u0026#34; time=\u0026#34;2024-10-17T03:34:21Z\u0026#34; level=info msg=\u0026#34;login successful: connector \\\u0026#34;dex-controller_kommander_ldap-identity-provider-hg4bx\\\u0026#34;, username=\\\u0026#34;Ken Wang\\\u0026#34;, preferred_username=\\\u0026#34;\\\u0026#34;, email=\\\u0026#34;ken.wang@nutanixlab.local\\\u0026#34;, groups=[\\\u0026#34;Schema Admins\\\u0026#34; \\\u0026#34;Enterprise Admins\\\u0026#34; \\\u0026#34;Domain Admins\\\u0026#34; \\\u0026#34;Group Policy Creator Owners\\\u0026#34; \\\u0026#34;NKPAdmins\\\u0026#34;]\u0026#34; AI Navigator 啟用 NKP Ultimate License 後，會自動安裝 AI 聊天機器人，主要功能為 直覺的查詢處理、即時問題處理協助、整合NKP\n另外還有NKP AI Navigator Cluster Info Agent，描述的功能與NKP AI Navigator 相同，主要可以偵測整個叢集的版本等資訊\nAI 助理回答如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 Key Features Intuitive Query Processing： Empowered by Natural Language Processing (NLP), the AI Navigator interprets user queries, even if they’re not phrased technically, making it a go-to tool for both novices and experts. Real-time Troubleshooting Assistance： With its vast database of common NKP issues and solutions, the AI Navigator can provide instant solutions, dramatically reducing downtime and enhancing productivity. Seamless NKP Integration： Being native to the NKP platform, the AI Navigator ensures users get the most out of their NKP environments by offering insights, best practices, and performance optimization tips. Privacy First Approach Data privacy and security remain paramount, ensuring user trust and compliance. AI Navigator 所安裝的 pod\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 [nkp@ken-rhel9 ~]$ kubectl get pods -n kommander |grep ai-navigator ai-navigator-app-554f86cd46-797tq 1/1 Running 0 15d ai-navigator-cluster-info-api-6b78d6449-fh756 1/1 Running 0 15d ai-navigator-cluster-info-api-6b78d6449-fn6pn 1/1 Running 0 15d ai-navigator-cluster-info-api-postgresql-0 1/1 Running 0 15d [nkp@ken-rhel9 ~]$ kubectl -n kommander describe pod ai-navigator-app-554f86cd46-797tq Name: ai-navigator-app-554f86cd46-797tq Namespace: kommander Priority: 100001000 Priority Class Name: dkp-high-priority Service Account: ai-navigator-app Node: nkp-mgmt-md-0-l965p-fczv8-n6tlc/172.16.90.171 Start Time: Tue, 01 Oct 2024 14:42:44 +0800 Labels: app=ai-navigator-app pod-template-hash=554f86cd46 Annotations: \u0026lt;none\u0026gt; Status: Running IP: 192.168.1.226 IPs: IP: 192.168.1.226 Controlled By: ReplicaSet/ai-navigator-app-554f86cd46 Containers: ai-navigator-app: Container ID: containerd://32691c6af7bcb6942c7d11de6a0c331c6aa75faf27d8cd9c98bc8e692b8a055c Image: mesosphere/ai-navigator-app:v0.1.1 Image ID: docker.io/mesosphere/ai-navigator-app@sha256:2026ed7aabb265eead86917ad95de623aef7c13a775c7901a4629909460f5bce Port: 8080/TCP Host Port: 0/TCP SeccompProfile: RuntimeDefault State: Running Started: Tue, 01 Oct 2024 14:42:56 +0800 Ready: True 啟用NKP AI Navigator Cluster Info Agent 前的回答\n啟用Agent\n大約等3分鐘後 ， Pod 新增一個 agent\n1 2 3 4 5 6 [nkp@ken-rhel9 ~]$ kubectl get pods -n kommander |grep ai-navigator ai-navigator-app-554f86cd46-797tq 1/1 Running 0 15d ai-navigator-cluster-info-agent-76bfb65f49-qdrqx 1/1 Running 0 77s ai-navigator-cluster-info-api-6b78d6449-fh756 1/1 Running 0 15d ai-navigator-cluster-info-api-6b78d6449-fn6pn 1/1 Running 0 15d ai-navigator-cluster-info-api-postgresql-0 1/1 Running 0 15d 同樣問題已可回答\n驗證版本稍微有點不一樣，而且多問幾次的回答也不同，應該還要多點時間學習\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [nkp@ken-rhel9 ~]$ kubectl version Client Version: v1.28.0 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 Server Version: v1.29.6 [nkp@ken-rhel9 ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION nkp-mgmt-hzxnp-28zsd Ready control-plane 16d v1.29.6 nkp-mgmt-hzxnp-cnjn6 Ready control-plane 16d v1.29.6 nkp-mgmt-hzxnp-gg88v Ready control-plane 16d v1.29.6 nkp-mgmt-md-0-l965p-fczv8-mspnw Ready \u0026lt;none\u0026gt; 16d v1.29.6 nkp-mgmt-md-0-l965p-fczv8-n6tlc Ready \u0026lt;none\u0026gt; 16d v1.29.6 nkp-mgmt-md-0-l965p-fczv8-t5xmd Ready \u0026lt;none\u0026gt; 16d v1.29.6 nkp-mgmt-md-0-l965p-fczv8-w7sxk Ready \u0026lt;none\u0026gt; 16d v1.29.6 [nkp@ken-rhel9 ~]$ kubectl get pods -n kommander |grep ai-navigator ai-navigator-app-554f86cd46-797tq 1/1 Running 0 15d ai-navigator-cluster-info-agent-76bfb65f49-qdrqx 1/1 Running 0 9m25s ai-navigator-cluster-info-api-6b78d6449-24v4r 1/1 Running 0 7m51s ai-navigator-cluster-info-api-6b78d6449-bmxk2 1/1 Running 0 7m51s ai-navigator-cluster-info-api-6b78d6449-c5z4s 1/1 Running 0 7m51s ai-navigator-cluster-info-api-6b78d6449-fh756 1/1 Running 0 15d ai-navigator-cluster-info-api-6b78d6449-fn6pn 1/1 Running 0 15d ai-navigator-cluster-info-api-postgresql-0 1/1 Running 0 15d NDB Operator https://github.com/nutanix-cloud-native/ndb-operator?tab=readme-ov-file\nPre-install 安裝 Operator-SDK ( Helm Install 可略過 )\n設定OS 版本\n1 2 3 [nkp@ken-rhel9 ~]$ export ARCH=$(case $(uname -m) in x86_64) echo -n amd64 ;; aarch64) echo -n arm64 ;; *) echo -n $(uname -m) ;; esac) [nkp@ken-rhel9 ~]$ export OS=$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;) 下載Binary\n1 2 3 4 5 6 7 [nkp@ken-rhel9 ~]$ export OPERATOR_SDK_DL_URL=https://github.com/operator-framework/operator-sdk/releases/download/v1.37.0 [nkp@ken-rhel9 ~]$ curl -LO ${OPERATOR_SDK_DL_URL}/operator-sdk_${OS}_${ARCH} % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 91.6M 100 91.6M 0 0 1994k 0 0:00:47 0:00:47 --:--:-- 2059k 確認Binary\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [nkp@ken-rhel9 ~]$ gpg --keyserver keyserver.ubuntu.com --recv-keys 052996E2A20B5C7E gpg: directory \u0026#39;/home/nkp/.gnupg\u0026#39; created gpg: keybox \u0026#39;/home/nkp/.gnupg/pubring.kbx\u0026#39; created gpg: /home/nkp/.gnupg/trustdb.gpg: trustdb created gpg: key 052996E2A20B5C7E: public key \u0026#34;Operator SDK (release) \u0026lt;cncf-operator-sdk@cncf.io\u0026gt;\u0026#34; imported gpg: Total number processed: 1 gpg: imported: 1 [nkp@ken-rhel9 ~]$ curl -LO ${OPERATOR_SDK_DL_URL}/checksums.txt [nkp@ken-rhel9 ~]$ curl -LO ${OPERATOR_SDK_DL_URL}/checksums.txt.asc [nkp@ken-rhel9 ~]$ gpg -u \u0026#34;Operator SDK (release) \u0026lt;cncf-operator-sdk@cncf.io\u0026gt;\u0026#34; --verify checksums.txt.asc [nkp@ken-rhel9 ~]$ grep operator-sdk_${OS}_${ARCH} checksums.txt | sha256sum -c - operator-sdk_linux_amd64: OK 安裝並複製到/usr/loca/bin\n1 2 [nkp@ken-rhel9 ~]$ chmod +x operator-sdk_${OS}_${ARCH} \u0026amp;\u0026amp; sudo mv operator-sdk_${OS}_${ARCH} /usr/local/bin/operator-sdk [sudo] password for nkp: 下載 git 跟 go\nhttps://git-scm.com/downloads\nhttps://go.dev/dl/\n1 2 3 4 5 [nkp@ken-rhel9 ~]$ git --version git version 2.31.1 [nkp@ken-rhel9 ~]$ go version go version go1.22.2 linux/amd64 確認 cert-manager 安裝 1 2 3 4 5 [nkp@ken-rhel9 ~]$ kubectl get pods -n cert-manager --kubeconfig=nkp-cluster01-kubeconfig.yaml NAME READY STATUS RESTARTS AGE cert-manager-556c9f47d-c4d6q 1/1 Running 0 11d cert-manager-cainjector-57d8d7ff64-cbzfg 1/1 Running 0 11d cert-manager-webhook-844cfb8858-g8ftz 1/1 Running 0 11d 確認 NDB 連線 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 NDB: 172.16.90.196 [nkp@ken-rhel9 ~]$ kubectl get nodes -o wide --kubeconfig=nkp-cluster01-kubeconfig.yaml NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME nkp-cluster01-9swsx-zwgx9 Ready control-plane 11d v1.29.6 172.16.90.126 \u0026lt;none\u0026gt; Rocky Linux 9.4 (Blue Onyx) 5.14.0-427.31.1.el9_4.x86_64 containerd://1.6.33-d2iq.1 nkp-cluster01-md-0-ztljf-l4nbq-5xnsc Ready \u0026lt;none\u0026gt; 11d v1.29.6 172.16.90.127 \u0026lt;none\u0026gt; Rocky Linux 9.4 (Blue Onyx) 5.14.0-427.31.1.el9_4.x86_64 containerd://1.6.33-d2iq.1 nkp-cluster01-md-0-ztljf-l4nbq-vhfvq Ready \u0026lt;none\u0026gt; 11d v1.29.6 172.16.90.128 \u0026lt;none\u0026gt; Rocky Linux 9.4 (Blue Onyx) 5.14.0-427.31.1.el9_4.x86_64 containerd://1.6.33-d2iq.1 [nkp@ken-rhel9 ~]$ ssh nkp@172.16.90.126 [nkp@nkp-cluster01-9swsx-zwgx9 ~]$ ping 172.16.90.196 PING 172.16.90.196 (172.16.90.196) 56(84) bytes of data. 64 bytes from 172.16.90.196: icmp_seq=1 ttl=64 time=2.92 ms ^C --- 172.16.90.196 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 2.921/2.921/2.921/0.000 ms Install Github Repo git clone\n1 2 3 4 5 6 7 8 9 10 11 [nkp@ken-rhel9 ~]$ git clone https://github.com/nutanix-cloud-native/ndb-operator.git Cloning into \u0026#39;ndb-operator\u0026#39;... remote: Enumerating objects: 3345, done. remote: Counting objects: 100% (1264/1264), done. remote: Compressing objects: 100% (520/520), done. remote: Total 3345 (delta 974), reused 793 (delta 743), pack-reused 2081 (from 1) Receiving objects: 100% (3345/3345), 772.31 KiB | 1.58 MiB/s, done. Resolving deltas: 100% (2073/2073), done. [nkp@ken-rhel9 ~]$ ls -l |grep ndb drwxr-xr-x. 13 nkp nkp 4096 Nov 11 09:55 ndb-operator cd ndb-operator 後安裝於 kubernetes 中\n1 2 [nkp@ken-rhel9 ~]$ cd ndb-operator/ [nkp@ken-rhel9 ndb-operator]$ make deploy Helm Chart Add Nutanix Repository\n1 2 3 4 5 6 [nkp@ken-rhel9 ~]$ helm repo add nutanix https://nutanix.github.io/helm/ --kubeconfig=nkp-cluster01-kubeconfig.yaml \u0026#34;nutanix\u0026#34; has been added to your repositories [nkp@ken-rhel9 ~]$ helm repo list --kubeconfig=nkp-cluster01-kubeconfig.yaml NAME URL nutanix\thttps://nutanix.github.io/helm/ Install Chart\n1 2 3 4 5 6 7 8 9 10 11 [nkp@ken-rhel9 ~]$ kubectl create ns ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml namespace/ndb-operator created [nkp@ken-rhel9 ~]$ helm install netfos-ndb-operator -n ndb-operator nutanix/ndb-operator --version 0.5.3 --kubeconfig=nkp-cluster01-kubeconfig.yaml NAME: netfos-ndb-operator LAST DEPLOYED: Mon Nov 11 10:20:27 2024 NAMESPACE: ndb-operator STATUS: deployed REVISION: 1 TEST SUITE: None Create Secret and Connet 建立 ndb-secret 連線用的secret\n1 2 3 4 5 6 7 8 9 10 11 [nkp@ken-rhel9 ndb]$ vim netfos-ndb-secret.yaml apiVersion: v1 kind: Secret metadata: name: netfos-ndb-secret type: Opaque stringData: username: admin password: Nutanix/Lab123 [nkp@ken-rhel9 ndb]$ kubectl apply -f netfos-ndb-secret.yaml -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml 連線 ndb-server\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 [nkp@ken-rhel9 ndb]$ vim ndb-server.yaml apiVersion: ndb.nutanix.com/v1alpha1 kind: NDBServer metadata: labels: app.kubernetes.io/name: ndbserver app.kubernetes.io/instance: ndbserver app.kubernetes.io/part-of: ndb-operator app.kubernetes.io/managed-by: kustomize app.kubernetes.io/created-by: ndb-operator name: ndb spec: # Name of the secret that holds the credentials for NDB: username, password and ca_certificate created earlier credentialSecret: netfos-ndb-secret # NDB Server\u0026#39;s API URL server: https://172.16.90.196/era/v0.9 # Set to true to skip SSL certificate validation, should be false if ca_certificate is provided in the credential secret. skipCertificateVerification: true [nkp@ken-rhel9 ndb]$ kubectl apply -f ndb-server.yaml -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig. [nkp@ken-rhel9 ndb]$ kubectl get pods -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml NAME READY STATUS RESTARTS AGE netfos-ndb-operator-controller-manager-5c77798ddf-7xz49 2/2 Running 0 13m [nkp@ken-rhel9 ndb]$ kubectl get pods -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml NAME READY STATUS RESTARTS AGE netfos-ndb-operator-controller-manager-5c77798ddf-7xz49 2/2 Running 0 13m [nkp@ken-rhel9 ndb]$ kubectl describe pod netfos-ndb-operator-controller-manager-5c77798ddf-7xz49 -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yamlName: netfos-ndb-operator-controller-manager-5c77798ddf-7xz49 Namespace: ndb-operator Priority: 0 Service Account: netfos-ndb-operator-service-account Node: nkp-cluster01-md-0-ztljf-l4nbq-vhfvq/172.16.90.128 Start Time: Mon, 11 Nov 2024 10:21:00 +0800 Labels: control-plane=controller-manager pod-template-hash=5c77798ddf Annotations: kubectl.kubernetes.io/default-container: manager Status: Running IP: 192.168.2.57 IPs: IP: 192.168.2.57 Controlled By: ReplicaSet/netfos-ndb-operator-controller-manager-5c77798ddf Containers: manager: Container ID: containerd://3a8754ee1381f2bb87fdffe202ae91770f19642d690f552086e1b451d3869fcf Image: ghcr.io/nutanix-cloud-native/ndb-operator/controller:v0.5.1 Image ID: ghcr.io/nutanix-cloud-native/ndb-operator/controller@sha256:5579d0349f5dc8b7f45d0eaddfe6ad087aa92ad925c799d4fe83eda7beba4ac5 Port: 9443/TCP Host Port: 0/TCP Args: --health-probe-bind-address=:8081 --metrics-bind-address=127.0.0.1:8080 --leader-elect State: Running Started: Mon, 11 Nov 2024 10:21:17 +0800 Ready: True Restart Count: 0 Limits: cpu: 500m memory: 128Mi Requests: cpu: 10m memory: 64Mi Liveness: http-get http://:8081/healthz delay=15s timeout=1s period=20s #success=1 #failure=3 Readiness: http-get http://:8081/readyz delay=5s timeout=1s period=10s #success=1 #failure=3 Environment: \u0026lt;none\u0026gt; Mounts: /tmp/k8s-webhook-server/serving-certs from cert (ro) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hmp6h (ro) kube-rbac-proxy: Container ID: containerd://20c40b65d22a766217ce8ead5db6ed7d6d3316a676c733f916e859cdf6f87557 Image: gcr.io/kubebuilder/kube-rbac-proxy:v0.16.0 Image ID: gcr.io/kubebuilder/kube-rbac-proxy@sha256:771a9a173e033a3ad8b46f5c00a7036eaa88c8d8d1fbd89217325168998113ea Port: 8443/TCP Host Port: 0/TCP Args: --secure-listen-address=0.0.0.0:8443 --upstream=http://127.0.0.1:8080/ --logtostderr=true --v=0 State: Running Started: Mon, 11 Nov 2024 10:21:22 +0800 Ready: True Restart Count: 0 Limits: cpu: 500m memory: 128Mi Requests: cpu: 5m memory: 64Mi Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hmp6h (ro) Conditions: Type Status PodReadyToStartContainers True Initialized True Ready True ContainersReady True PodScheduled True [nkp@ken-rhel9 ndb]$ kubectl get all -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml NAME READY STATUS RESTARTS AGE pod/netfos-ndb-operator-controller-manager-5c77798ddf-7xz49 2/2 Running 0 53m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/netfos-ndb-operator-controller-manager-metrics-service ClusterIP 10.107.170.244 \u0026lt;none\u0026gt; 8443/TCP 53m service/netfos-ndb-operator-webhook-service ClusterIP 10.109.133.195 \u0026lt;none\u0026gt; 443/TCP 53m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/netfos-ndb-operator-controller-manager 1/1 1 1 53m NAME DESIRED CURRENT READY AGE replicaset.apps/netfos-ndb-operator-controller-manager-5c77798ddf 1 1 1 53m Post-install Create postgres Create db-instance-secret\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [nkp@ken-rhel9 ndb]$ cat db-postgres-secret.yaml apiVersion: v1 kind: Secret metadata: name: ndb-postgres-secret type: Opaque stringData: password: P@55w.rd ssh_public_key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCVN+q1hhDCFrbW4Gw+wPryGl2BQbLi9NNOv+hXLgI+NXJqhTT0XEL6Uun5WKd7ceNznfiCHee2AkQXm9nrvVyPpFU5zQ91j9zQbMsFylSDv7RALFyEaXO45u6bzKRvVnL4lsKygBcMwxSW8yIrv0qTeDl3rChfkxjhpWR+C7gbn6uIVINRoC/5L1xNGcThbF0CiHgOt5P03ZV4tsm6C7Z65Wj41vczaH460qqq//kaf2uNveOZhi4juIQMc0FcE4kvGwtqRJ5HIs8CRq1N/wX+uIIQEKsqtmQqSujcHOwosYPSNvVXvVgqvy9t0jfEU4Y3bcBkoF5zfq98OCG0978ewjb8yR909bQM2s2QRoQML8Gb6Be+EV7+xtUcqZOpAkyZOmWXN8yoL5EMcelgDZoyK3yH9FgKuz1sbB0f+4b1B8LM/SULTq8Lv7ww6ps3EIvKDwbH5rMQIxf3IEIuk7qvjyQlAZiE2nxiU88fcPB54WUoQBNd3uUjr0YVt1i5GM0= nkp@ken-rhel9.nutanixlab.local [nkp@ken-rhel9 ndb]$ kubectl apply -f db-postgres-secret.yaml -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml [nkp@ken-rhel9 ndb]$ kubectl get secret -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml NAME TYPE DATA AGE db-postgres-secret Opaque 2 5s netfos-ndb-secret Opaque 2 38m sh.helm.release.v1.netfos-ndb-operator.v1 helm.sh/release.v1 1 50m webhook-server-cert kubernetes.io/tls 3 50m Cluster ID ( Nutanix PE Cluster 有註冊在NDB上的 )\nCreate Postgres DB\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 [nkp@ken-rhel9 ndb]$ cat db-postgres-deploy.yaml apiVersion: ndb.nutanix.com/v1alpha1 kind: Database metadata: name: nkp-postgres spec: ndbRef: ndb isClone: false databaseInstance: clusterId: \u0026#34;b9ab1352-1673-4ce1-a9d8-174771f3dcbb\u0026#34; name: \u0026#34;nkp-postgres\u0026#34; description: This for testing NDB Operator in NKP Cluster databaseNames: - nkp_database credentialSecret: db-postgres-secret size: 100 timezone: \u0026#34;UTC\u0026#34; type: postgres # You can specify any (or none) of these types of profiles: compute, software, network, dbParam # If not specified, the corresponding Out-of-Box (OOB) profile will be used wherever applicable # Name is case-sensitive. ID is the UUID of the profile. Profile should be in the \u0026#34;READY\u0026#34; state # \u0026#34;id\u0026#34; \u0026amp; \u0026#34;name\u0026#34; are optional. If none provided, OOB may be resolved to any profile of that type profiles: compute: id: \u0026#34;8adb0c25-4136-4aa9-bf48-d0801d01e3e4\u0026#34; name: \u0026#34;DEFAULT_OOB_SMALL_COMPUTE\u0026#34; # A Software profile is a mandatory input for closed-source engines: SQL Server \u0026amp; Oracle software: name: \u0026#34;POSTGRES_15.5_Rocky8.10\u0026#34; id: \u0026#34;71466901-fba3-4391-a9b3-b77d8933de46\u0026#34; network: id: \u0026#34;731cf689-e55a-4b05-a8ac-98ff8a18af13\u0026#34; name: \u0026#34;DEFAULT_OOB_POSTGRESQL_NETWORK\u0026#34; dbParam: name: \u0026#34;DEFAULT_POSTGRES_PARAMS\u0026#34; id: \u0026#34;c6c8517a-3697-4090-b5b7-08632c77bdce\u0026#34; [nkp@ken-rhel9 ndb]$ kubectl apply -f db-postgres-deploy.yaml -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml 看logs [nkp@ken-rhel9 ndb]$ kubectl logs -f netfos-ndb-operator-controller-manager-5c77798ddf-7xz49 -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml 從NDB查看進度\n查看Logs\n部署完成即可看到此Service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 [nkp@ken-rhel9 ndb]$ kubectl --kubeconfig=nkp-cluster01-kubeconfig.yaml get all -n ndb-operator NAME READY STATUS RESTARTS AGE pod/netfos-ndb-operator-controller-manager-5c77798ddf-7xz49 2/2 Running 1 (6d22h ago) 11d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/netfos-ndb-operator-controller-manager-metrics-service ClusterIP 10.107.170.244 \u0026lt;none\u0026gt; 8443/TCP 11d service/netfos-ndb-operator-webhook-service ClusterIP 10.109.133.195 \u0026lt;none\u0026gt; 443/TCP 11d service/netfos-nkp-mssql-svc ClusterIP 10.108.221.85 \u0026lt;none\u0026gt; 80/TCP 10d service/nkp-postgres-svc ClusterIP 10.96.183.27 \u0026lt;none\u0026gt; 80/TCP 5m40s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/netfos-ndb-operator-controller-manager 1/1 1 1 11d NAME DESIRED CURRENT READY AGE replicaset.apps/netfos-ndb-operator-controller-manager-5c77798ddf 1 1 1 11d [nkp@ken-rhel9 ndb]$ kubectl --kubeconfig=nkp-cluster01-kubeconfig.yaml describe service/nkp-postgres-svc -n ndb-operator Name: nkp-postgres-svc Namespace: ndb-operator Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: \u0026lt;none\u0026gt; Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.96.183.27 IPs: 10.96.183.27 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 5432/TCP Endpoints: 172.16.90.164:5432 Session Affinity: None Events: \u0026lt;none\u0026gt; # 列出 database [nkp@ken-rhel9 ndb]$ kubectl --kubeconfig=nkp-cluster01-kubeconfig.yaml get database -n ndb-operator NAME IP ADDRESS STATUS TYPE netfos-nkp-mssql 172.16.90.163 READY mssql nkp-postgres 172.16.90.164 READY postgres # 描述 database (包含Cloen會用到的database ID) [nkp@ken-rhel9 ndb]$ kubectl --kubeconfig=nkp-cluster01-kubeconfig.yaml describe database nkp-postgres -n ndb-operator Name: nkp-postgres Namespace: ndb-operator Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: ndb.nutanix.com/v1alpha1 Kind: Database Metadata: Creation Timestamp: 2024-11-22T06:58:19Z Finalizers: ndb.nutanix.com/finalizerinstance ndb.nutanix.com/finalizerserver Generation: 1 Resource Version: 18657950 UID: c9ae56f5-a86e-44ba-a6be-b96e3ac89767 Spec: Clone: Additional Arguments: Cluster Id: Credential Secret: Description: Name: Profiles: Compute: Id: Name: Db Param: Id: Name: Db Param Instance: Id: Name: Network: Id: Name: Software: Id: Name: Snapshot Id: Source Database Id: Timezone: Type: Database Instance: Additional Arguments: Cluster Id: b9ab1352-1673-4ce1-a9d8-174771f3dcbb Credential Secret: db-postgres-secret Database Names: nkp_database Description: This for testing NDB Operator in NKP Cluster Name: nkp-postgres Profiles: Compute: Id: 8adb0c25-4136-4aa9-bf48-d0801d01e3e4 Name: DEFAULT_OOB_SMALL_COMPUTE Db Param: Id: c6c8517a-3697-4090-b5b7-08632c77bdce Name: DEFAULT_POSTGRES_PARAMS Db Param Instance: Id: Name: Network: Id: 731cf689-e55a-4b05-a8ac-98ff8a18af13 Name: DEFAULT_OOB_POSTGRESQL_NETWORK Software: Id: 71466901-fba3-4391-a9b3-b77d8933de46 Name: POSTGRES_15.5_Rocky8.10 Size: 100 Time Machine: Daily Snapshot Time: 04:00:00 Description: Log Catch Up Frequency: 30 Monthly Snapshot Day: 15 Name: Quarterly Snapshot Month: Jan Sla: NONE Snapshots Per Day: 1 Weekly Snapshot Day: FRIDAY Timezone: UTC Type: postgres Is Clone: false Ndb Ref: ndb Status: Creation Operation Id: c60d8a4a-7c20-4a22-8006-f48fa28b9ff7 Db Server Id: 4ef4c7d0-c891-460d-9c9a-4b71032a3d5a Deregistration Operation Id: Id: 7e9f5189-adbf-4228-abb1-b6f69cb13d3e ## Clone 會用到此Database Id Ip Address: 172.16.90.164 Status: READY Type: postgres Events: \u0026lt;none\u0026gt; 確認Endpoints\n測試連線 用kubernetes service 方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # 啟動測試容器 [nkp@ken-rhel9 ndb]$ kubectl --kubeconfig=nkp-cluster01-kubeconfig.yaml -n ndb-operator run -i --tty postgres --image=postgres --restart=Never -- sh # 連線 kubernetes service （預設帶80 port） # psql -U postgres -p 80 -h nkp-postgres-svc -W Password: psql (17.1 (Debian 17.1-1.pgdg120+1), server 15.5) Type \u0026#34;help\u0026#34; for help. # 列出 database postgres=# \\l List of databases Name | Owner | Encoding | Locale Provider | Collate | Ctype | Locale | ICU Rules | Access privileges --------------+----------+----------+-----------------+-------------+-------------+--------+-----------+----------------------- nkp_database | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | postgres | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | template0 | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | =c/postgres + | | | | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | =c/postgres + | | | | | | | | postgres=CTc/postgres (4 rows) # 新增 test database postgres=# create database test; CREATE DATABASE # 列出 database postgres=# \\l List of databases Name | Owner | Encoding | Locale Provider | Collate | Ctype | Locale | ICU Rules | Access privileges --------------+----------+----------+-----------------+-------------+-------------+--------+-----------+----------------------- nkp_database | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | postgres | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | template0 | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | =c/postgres + | | | | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | =c/postgres + | | | | | | | | postgres=CTc/postgres test | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | (5 rows) # 移除 test database postgres=# drop database test; DROP DATABASE # 退出 \\q 測試連線 直連 VM IP 方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 啟動測試容器 [nkp@ken-rhel9 ndb]$ kubectl --kubeconfig=nkp-cluster01-kubeconfig.yaml -n ndb-operator run -i --tty postgres --image=postgres --restart=Never -- sh # 連線 VM 的 IP 以及 p 5432 # psql -U postgres -p 5432 -h 172.16.90.164 -W Password: psql (17.1 (Debian 17.1-1.pgdg120+1), server 15.5) Type \u0026#34;help\u0026#34; for help. # 列出database postgres=# \\l List of databases Name | Owner | Encoding | Locale Provider | Collate | Ctype | Locale | ICU Rules | Access privileges --------------+----------+----------+-----------------+-------------+-------------+--------+-----------+----------------------- nkp_database | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | postgres | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | template0 | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | =c/postgres + | | | | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | libc | en_US.UTF-8 | en_US.UTF-8 | | | =c/postgres + | | | | | | | | postgres=CTc/postgres (4 rows) Clone Database 建立Clone manifest Snapshot Id 從 NDB API Exporter上查看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: ndb.nutanix.com/v1alpha1 kind: Database metadata: name: nkp-postgres-clone spec: ndbRef: ndb isClone: true clone: clusterId: \u0026#34;b9ab1352-1673-4ce1-a9d8-174771f3dcbb\u0026#34; name: \u0026#34;nkp-postgres-clone\u0026#34; description: This for testing NDB Operator Cloning in NKP Cluster credentialSecret: db-postgres-secret timezone: \u0026#34;UTC\u0026#34; type: postgres # You can specify any (or none) of these types of profiles: compute, software, network, dbParam # If not specified, the corresponding Out-of-Box (OOB) profile will be used wherever applicable # Name is case-sensitive. ID is the UUID of the profile. Profile should be in the \u0026#34;READY\u0026#34; state # \u0026#34;id\u0026#34; \u0026amp; \u0026#34;name\u0026#34; are optional. If none provided, OOB may be resolved to any profile of that type profiles: compute: id: \u0026#34;8adb0c25-4136-4aa9-bf48-d0801d01e3e4\u0026#34; name: \u0026#34;DEFAULT_OOB_SMALL_COMPUTE\u0026#34; # A Software profile is a mandatory input for closed-source engines: SQL Server \u0026amp; Oracle software: name: \u0026#34;POSTGRES_15.5_Rocky8.10\u0026#34; id: \u0026#34;71466901-fba3-4391-a9b3-b77d8933de46\u0026#34; network: id: \u0026#34;731cf689-e55a-4b05-a8ac-98ff8a18af13\u0026#34; name: \u0026#34;DEFAULT_OOB_POSTGRESQL_NETWORK\u0026#34; dbParam: name: \u0026#34;DEFAULT_POSTGRES_PARAMS\u0026#34; id: \u0026#34;c6c8517a-3697-4090-b5b7-08632c77bdce\u0026#34; sourceDatabaseId: \u0026#34;7e9f5189-adbf-4228-abb1-b6f69cb13d3e\u0026#34; snapshotId: \u0026#34;942b39b7-ca20-461e-8313-baf17148a8ac\u0026#34; Clone Database\n1 2 [nkp@ken-rhel9 ndb]$ kubectl --kubeconfig=nkp-cluster01-kubeconfig.yaml -n ndb-operator apply -f db-postgres-clone.yaml database.ndb.nutanix.com/nkp-postgres-clone created NDB 查看狀態\nLog 狀態\nClone 完成\nNKP 可認得此Service\nCreate MSSQL Create mssql secret\n1 2 3 4 5 6 7 8 9 10 11 12 13 [nkp@ken-rhel9 ndb]$ vim db-mssql-secret.yaml apiVersion: v1 kind: Secret metadata: name: db-mssql-secret type: Opaque stringData: password: P@ssw0rd123 ssh_public_key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCVN+q1hhDCFrbW4Gw+wPryGl2BQbLi9NNOv+hXLgI+NXJqhTT0XEL6Uun5WKd7ceNznfiCHee2AkQXm9nrvVyPpFU5zQ91j9zQbMsFylSDv7RALFyEaXO45u6bzKRvVnL4lsKygBcMwxSW8yIrv0qTeDl3rChfkxjhpWR+C7gbn6uIVINRoC/5L1xNGcThbF0CiHgOt5P03ZV4tsm6C7Z65Wj41vczaH460qqq//kaf2uNveOZhi4juIQMc0FcE4kvGwtqRJ5HIs8CRq1N/wX+uIIQEKsqtmQqSujcHOwosYPSNvVXvVgqvy9t0jfEU4Y3bcBkoF5zfq98OCG0978ewjb8yR909bQM2s2QRoQML8Gb6Be+EV7+xtUcqZOpAkyZOmWXN8yoL5EMcelgDZoyK3yH9FgKuz1sbB0f+4b1B8LM/SULTq8Lv7ww6ps3EIvKDwbH5rMQIxf3IEIuk7qvjyQlAZiE2nxiU88fcPB54WUoQBNd3uUjr0YVt1i5GM0= nkp@ken-rhel9.nutanixlab.local [nkp@ken-rhel9 ndb]$ kubectl apply -f db-mssql-secret.yaml -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml secret/db-mssql-secret created Create mssql manifests\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 [nkp@ken-rhel9 ndb]$ vim db-mssql-deploy.yaml apiVersion: ndb.nutanix.com/v1alpha1 kind: Database metadata: name: netfos-nkp-mssql spec: ndbRef: ndb isClone: false databaseInstance: clusterId: \u0026#34;b9ab1352-1673-4ce1-a9d8-174771f3dcbb\u0026#34; name: \u0026#34;netfos-nkp-mssql\u0026#34; description: This for testing NDB Operator in NKP Cluster databaseNames: - netfos_database credentialSecret: db-mssql-secret size: 100 timezone: \u0026#34;UTC\u0026#34; type: mssql # You can specify any (or none) of these types of profiles: compute, software, network, dbParam # If not specified, the corresponding Out-of-Box (OOB) profile will be used wherever applicable # Name is case-sensitive. ID is the UUID of the profile. Profile should be in the \u0026#34;READY\u0026#34; state # \u0026#34;id\u0026#34; \u0026amp; \u0026#34;name\u0026#34; are optional. If none provided, OOB may be resolved to any profile of that type profiles: compute: id: \u0026#34;79b6fd91-4d3d-4a30-9e7b-180bc4265922\u0026#34; name: \u0026#34;SQL_Default_Compute\u0026#34; # A Software profile is a mandatory input for closed-source engines: SQL Server \u0026amp; Oracle software: name: \u0026#34;Win2022_SQL2019_NKP\u0026#34; id: \u0026#34;3e2cb21d-073d-4b06-9815-07ebc85746f9\u0026#34; network: id: \u0026#34;7248aa73-46d7-438f-bb42-0afb9fb4f996\u0026#34; name: \u0026#34;DEFAULT_OOB_SQLSERVER_NETWORK\u0026#34; dbParam: name: \u0026#34;DEFAULT_SQLSERVER_DATABASE_PARAMS\u0026#34; id: \u0026#34;530dc232-b664-4d01-b542-81b4c2befb7c\u0026#34; # Only applicable for MSSQL databases dbParamInstance: name: \u0026#34;DEFAULT_SQLSERVER_INSTANCE_PARAMS\u0026#34; id: \u0026#34;90aa7e49-0ade-4122-b3ea-ba7c6fa4854c\u0026#34; timeMachine: # Optional block, if removed the SLA defaults to NONE sla : \u0026#34;DEFAULT_OOB_BRASS_SLA\u0026#34; dailySnapshotTime: \u0026#34;12:00:00\u0026#34; # Time for daily snapshot in hh:mm:ss format snapshotsPerDay: 1 # Number of snapshots per day logCatchUpFrequency: 30 # Frequency (in minutes) weeklySnapshotDay: \u0026#34;MONDAY\u0026#34; # Day of the week for weekly snapshot monthlySnapshotDay: 11 # Day of the month for monthly snapshot additionalArguments: sql_user_name: \u0026#34;nkpuser\u0026#34; authentication_mode: \u0026#34;mixed\u0026#34; sql_user_password: \u0026#34;P@ssw0rd123\u0026#34; Create MSSQL\n1 2 3 4 5 6 7 8 9 10 [nkp@ken-rhel9 ndb]$ kubectl apply -f db-mssql-deploy.yaml -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml database.ndb.nutanix.com/netfos-nkp-mssql created 看logs [nkp@ken-rhel9 ndb]$ kubectl logs -f netfos-ndb-operator-controller-manager-5c77798ddf-7xz49 -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml 2024-11-12T00:28:18Z\tINFO\tDatabase CR Status: {\u0026#34;ipAddress\u0026#34;:\u0026#34;172.16.90.163\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;2582b1e6-f6c2-4eca-85f0-da5837845bff\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;READY\u0026#34;,\u0026#34;dbServerId\u0026#34;:\u0026#34;649e5d21-0c2b-4 139-9a14-a617473ebfd3\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;mssql\u0026#34;,\u0026#34;creationOperationId\u0026#34;:\u0026#34;73c5a0d6-d46d-47b2-9e5f-ec972158efd3\u0026#34;,\u0026#34;deregistrationOperationId\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;controller\u0026#34;: \u0026#34;database\u0026#34;, \u0026#34;control lerGroup\u0026#34;: \u0026#34;ndb.nutanix.com\u0026#34;, \u0026#34;controllerKind\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;Database\u0026#34;: {\u0026#34;name\u0026#34;:\u0026#34;netfos-nkp-mssql\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;ndb-operator\u0026#34;}, \u0026#34;namespace\u0026#34;: \u0026#34;ndb-operator\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;netfo s-nkp-mssql\u0026#34;, \u0026#34;reconcileID\u0026#34;: \u0026#34;89bae2ea-bdad-40c8-b9cb-137f56a8f99e\u0026#34;} NDB 查看進度\n部署完成\n從 NKP Cluster 可以看到此 Database 的 Service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [nkp@ken-rhel9 ndb]$ kubectl get svc -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE netfos-ndb-operator-controller-manager-metrics-service ClusterIP 10.107.170.244 \u0026lt;none\u0026gt; 8443/TCP 5h2m netfos-ndb-operator-webhook-service ClusterIP 10.109.133.195 \u0026lt;none\u0026gt; 443/TCP 5h2m netfos-nkp-mssql-svc ClusterIP 10.100.14.241 \u0026lt;none\u0026gt; 80/TCP 47m [nkp@ken-rhel9 ndb]$ kubectl describe svc netfos-nkp-mssql-svc -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml Name: netfos-nkp-mssql-svc Namespace: ndb-operator Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: \u0026lt;none\u0026gt; Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.100.14.241 IPs: 10.100.14.241 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 1433/TCP Endpoints: 172.16.90.163:1433 Session Affinity: None Events: \u0026lt;none\u0026gt; 測試連線\n1 2 3 4 [nkp@ken-rhel9 ndb]$ nc -zv 172.16.90.163 1433 Ncat: Version 7.92 ( https://nmap.org/ncat ) Ncat: Connected to 172.16.90.163:1433. Ncat: 0 bytes sent, 0 bytes received in 0.25 seconds. 安裝sqlcmd 來確認連線\n1 2 3 4 5 6 7 8 9 10 11 12 13 [nkp@ken-rhel9 ndb]$ curl https://packages.microsoft.com/config/rhel/9/prod.repo | sudo tee /etc/yum.repos.d/mssql-release.repo [nkp@ken-rhel9 ndb]$ sudo yum install -y mssql-tools18 unixODBC-devel Installed products updated. Installed: libtool-ltdl-2.4.6-45.el9.x86_64 msodbcsql18-18.4.1.1-1.x86_64 mssql-tools18-18.4.1.1-1.x86_64 unixODBC-2.3.11-1.rh.x86_64 unixODBC-devel-2.3.11-1.rh.x86_64 Complete! [nkp@ken-rhel9 ndb]$ echo \u0026#39;export PATH=\u0026#34;$PATH:/opt/mssql-tools18/bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile [nkp@ken-rhel9 ndb]$ source ~/.bash_profile sqlcmd連線測試\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [nkp@ken-rhel9 ~]$ sqlcmd -S 172.16.90.163 -U nkpuser -p -Q \u0026#34;SELECT @@VERSION\u0026#34; -C Password: ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ Microsoft SQL Server 2022 (RTM) - 16.0.1000.6 (X64) Oct 8 2022 05:58:25 Copyright (C) 2022 Microsoft Corporation Enterprise Edition (64-bit) on Windows Server 2019 Standard 10.0 \u0026lt;X64\u0026gt; (Build 17763: ) (Hypervisor) (1 rows affected) Network packet size (bytes): 4096 1 xact[s]: Clock Time (ms.): total 1 avg 1.0 (1000.0 xacts per sec.) [nkp@ken-rhel9 ~]$ sqlcmd -S 172.16.90.163 -U nkpuser -p -Q \u0026#34;SELECT name from sys.databases\u0026#34; -C Password: name -------------------------------------------------------------------------------------------------------------------------------- master tempdb model msdb netfos_database (5 rows affected) Network packet size (bytes): 4096 1 xact[s]: Clock Time (ms.): total 37 avg 37.0 (27.0 xacts per sec.) Create Database\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [nkp@ken-rhel9 ~]$ sqlcmd -S 172.16.90.163 -U nkpuser -p -C -Q \u0026#34;CREATE DATABASE nkpdev\u0026#34; Password: Network packet size (bytes): 4096 1 xact[s]: Clock Time (ms.): total 379 avg 379.0 (2.6 xacts per sec.) [nkp@ken-rhel9 ~]$ sqlcmd -S 172.16.90.163 -U nkpuser -p -Q \u0026#34;SELECT name from sys.databases\u0026#34; -C Password: name -------------------------------------------------------------------------------------------------------------------------------- master tempdb model msdb netfos_database nkpdev (6 rows affected) Network packet size (bytes): 4096 1 xact[s]: Clock Time (ms.): total 3 avg 3.0 (333.3 xacts per sec.) Delete Database VM\n1 2 [nkp@ken-rhel9 ndb]$ kubectl delete -f db-mssql-deploy.yaml -n ndb-operator --kubeconfig=nkp-cluster01-kubeconfig.yaml database.ndb.nutanix.com \u0026#34;netfos-nkp-mssql\u0026#34; deleted Workspace \u0026amp; Project 建立Project 設定權限\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [nkp@ken-rhel9 ~]$ kubectl get workspace --kubeconfig=/home/nkp/.kube/adminconfig NAME DISPLAY NAME WORKSPACE NAMESPACE AGE default-workspace Default Workspace kommander-default-workspace 69d kommander-workspace Management Cluster Workspace kommander 69d nkp-workload-lab nkp-workload-lab nkp-workload-lab 40d export WORKSPACE_NAME=\u0026lt;name_target_workspace\u0026gt; export WORKSPACE_NAME=nkp-workload-lab echo https://$(kubectl get kommandercluster -n kommander host-cluster -o jsonpath=\u0026#39;{ .status.ingress.address }\u0026#39;)/token/landing/${WORKSPACE_NAME} [nkp@ken-rhel9 ~]$ export WORKSPACE_NAME=nkp-workload-lab [nkp@ken-rhel9 ~]$ echo https://$(kubectl --kubeconfig=/home/nkp/.kube/adminconfig get kommandercluster -n kommander host-cluster -o jsonpath=\u0026#39;{ .status.ingress.address }\u0026#39;)/token/landing/${WORKSPACE_NAME} https://172.16.90.209/token/landing/nkp-workload-lab 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 [nkpuser@ken-rhel9 ~]$ cat ~/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority: certs/vpnuser12-172.16.90.211/k8s-ca.crt server: https://172.16.90.211/dkp/api-server name: vpnuser12-172.16.90.211 contexts: - context: cluster: vpnuser12-172.16.90.211 user: vpnuser12-172.16.90.211 name: vpnuser12-172.16.90.211 current-context: vpnuser12-172.16.90.211 kind: Config preferences: {} users: - name: vpnuser12-172.16.90.211 user: token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjRjOWY5NGZjM2I0ZThmYzJmN2YyN2I5NmUwMzc2YzIyNGViMzVhNjMifQ.eyJhdF9oYXNoIjoidERPS2Z2cG5fRXY0T2xfVFpLSDBNUSIsImF1ZCI6WyJkZXgtY29udHJvbGxlci1kZXh0ZmEtY2xpZW50LW5rcC1jbHVzdGVyMDEtMjZrYmgiLCJkZXgtY29udHJvbGxlci1kZXgtZGthLWNsaWVudC1rdWJlLWFwaXNlcnZlciJdLCJhenAiOiJkZXgtY29udHJvbGxlci1kZXgtZGthLWNsaWVudC1rdWJlLWFwaXNlcnZlciIsImNfaGFzaCI6Ik5Rb0haSUFrekNJblpZcEJCdXRUdlEiLCJlbWFpbCI6InZwbnVzZXIxMkBudXRhbml4bGFiLmxvY2FsIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTczMzgxOTUwMCwiZmVkZXJhdGVkX2NsYWltcyI6bnVsbCwiZ3JvdXBzIjpbIk5LUFVzZXJzIl0sImlhdCI6MTczMzczMzEwMCwiaXNzIjoiaHR0cHM6Ly8xNzIuMTYuOTAuMjA5L2RleCIsIm5hbWUiOiJ2cG51c2VyMTIiLCJub25jZSI6IiIsInByZWZlcnJlZF91c2VybmFtZSI6IiIsInN1YiI6IkNrQkRUajEyY0c1MWMyVnlNVElzVDFVOVEzVnpkRzl0WlhKekxFOVZQVTVsZEdadmMxUmhhWEJsYVN4RVF6MXVkWFJoYm1sNGJHRmlMRVJEUFd4dlkyRnNFalZrWlhndFkyOXVkSEp2Ykd4bGNsOXJiMjF0WVc1a1pYSmZiR1JoY0MxcFpHVnVkR2wwZVMxd2NtOTJhV1JsY2kxb1p6UmllQSJ9.a7NGtbrz2cHHxXYSIvXjQwUXQubOPQWCHVjPpFrREK0uuW8B7__t0Nc-v10uhRsHNCPIAmHwo-bnGkyjk8fGwcGDRFg9iDTTKy35Zsx_nc-OuHblKOG8tU42aUiGKG5fftd0vfHgzGaBolQ-dKbk45hn_h7y9l60eYYU5yUCIdI7kX5OgNV2eWksm3qhHathkF1vzC022Yo6Q2QmZceMSFQp9ckn9K033EHAIsSK3CEuyZcTqejA23XItE6Sfh3UOdwjzqRi7H810XSaQVnEMrctgIDnpbSUZMpLE2UCt0jAFXB841Ce7oNQvfzteCcHDvd9wiMfUY1gHIR5mbZL7g [nkpuser@ken-rhel9 ~]$ kubectl get nodes Error from server (Forbidden): nodes is forbidden: User \u0026#34;vpnuser12@nutanixlab.local\u0026#34; cannot list resource \u0026#34;nodes\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope [nkpuser@ken-rhel9 ~]$ kubectl get pods -n dev-team No resources found in dev-team namespace. [nkpuser@ken-rhel9 ~]$ kubectl get ns NAME STATUS AGE cert-manager Active 40d default Active 40d dev-team-npbwm Active 128m kommander-flux Active 40d kube-federation-system Active 40d kube-node-lease Active 40d kube-public Active 40d kube-system Active 40d metallb-system Active 40d ndb-operator Active 28d nkp-workload-lab Active 40d node-feature-discovery Active 40d ntnx-system Active 40d # 在別的namespace無法新增pod [nkpuser@ken-rhel9 ~]$ kubectl run nginx --image=nginx Error from server (Forbidden): pods is forbidden: User \u0026#34;vpnuser12@nutanixlab.local\u0026#34; cannot create resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;default\u0026#34; # 可在指定的namespace新增pod [nkpuser@ken-rhel9 ~]$ kubectl run nginx --image=nginx -n dev-team-npbwm pod/nginx created [nkpuser@ken-rhel9 ~]$ kubectl expose po nginx --type=NodePort --name=nginx-svc --port=80 -n dev-team-npbwm service/nginx-svc exposed # 確認pod與服務並連線 [nkpuser@ken-rhel9 ~]$ kubectl get all -n dev-team-npbwm NAME READY STATUS RESTARTS AGE pod/nginx 1/1 Running 0 44s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nginx-svc NodePort 10.98.244.223 \u0026lt;none\u0026gt; 80:31403/TCP 20s [nkpuser@ken-rhel9 ~]$ kubectl describe service/nginx-svc -n dev-team-npbwm Name: nginx-svc Namespace: dev-team-npbwm Labels: run=nginx Annotations: \u0026lt;none\u0026gt; Selector: run=nginx Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.98.244.223 IPs: 10.98.244.223 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 31403/TCP Endpoints: 192.168.1.163:80 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; Metallb-system 新增 loadbalancer ip 到 ip pool 裡面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [nkp@ken-rhel9 ~]$ kubectl get ipaddresspools -n metallb-system NAME AUTO ASSIGN AVOID BUGGY IPS ADDRESSES metallb true false [\u0026#34;172.16.90.209-172.16.90.209\u0026#34;] [nkp@ken-rhel9 ~]$ kubectl edit ipaddresspools -n metallb-system ipaddresspool.metallb.io/metallb edited [nkp@ken-rhel9 ~]$ kubectl describe ipaddresspools -n metallb-system Name: metallb Namespace: metallb-system Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: metallb.io/v1beta1 Kind: IPAddressPool Metadata: Creation Timestamp: 2024-10-01T03:37:31Z Generation: 2 Resource Version: 318510970 UID: 3cba6b6f-06c6-4002-a039-12774eedda72 Spec: Addresses: 172.16.90.209-172.16.90.211 Auto Assign: true Avoid Buggy I Ps: false Events: \u0026lt;none\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [nkp@ken-rhel9 ~]$ kubectl get svc -n ken-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE carts ClusterIP 10.99.139.74 \u0026lt;none\u0026gt; 80/TCP 18h carts-db ClusterIP 10.99.20.222 \u0026lt;none\u0026gt; 27017/TCP 18h catalogue ClusterIP 10.106.141.207 \u0026lt;none\u0026gt; 80/TCP 18h catalogue-db ClusterIP 10.102.80.220 \u0026lt;none\u0026gt; 3306/TCP 18h front-end LoadBalancer 10.103.0.112 172.16.90.211 80:30239/TCP 18h orders ClusterIP 10.106.57.160 \u0026lt;none\u0026gt; 80/TCP 18h orders-db ClusterIP 10.110.189.178 \u0026lt;none\u0026gt; 27017/TCP 18h payment ClusterIP 10.102.165.251 \u0026lt;none\u0026gt; 80/TCP 18h queue-master ClusterIP 10.96.248.113 \u0026lt;none\u0026gt; 80/TCP 18h rabbitmq ClusterIP 10.109.56.231 \u0026lt;none\u0026gt; 5672/TCP,9090/TCP 18h session-db ClusterIP 10.98.29.122 \u0026lt;none\u0026gt; 6379/TCP 18h shipping ClusterIP 10.97.155.46 \u0026lt;none\u0026gt; 80/TCP 18h user ClusterIP 10.99.13.238 \u0026lt;none\u0026gt; 80/TCP 18h user-db ClusterIP 10.105.150.252 \u0026lt;none\u0026gt; 27017/TCP 18h ","date":"2025-02-24T07:52:30+08:00","permalink":"https://wangken0129.github.io/p/nutanix_nkp-v2.12_ahv/","title":"Nutanix_NKP-v2.12_AHV"},{"content":"NDK v1.0.0 on OCP v4.13 NDK ( Nutanix Data Service for Kubernetes ) ：針對Kubernetes 的容器備份還原\n目前備援功能只支援在同一個供應商提供的 Kubernetes 叢集，且目前只有Volume 的PVC才可快照\n排程備份只支援Async ，此測試僅有針對 Namespace 內全部的項目做手動快照還原\nOCP IPI 安裝此篇不贅述，請參考其他篇\n2024.05.23\nReference https://portal.nutanix.com/page/documents/solutions/details?targetId=TN-2030-Red-Hat-OpenShift-on-Nutanix:TN-2030-Red-Hat-OpenShift-on-Nutanix\nhttps://portal.nutanix.com/page/documents/details?targetId=Nutanix-Data-Services-for-Kubernetes-v1_0:Nutanix-Data-Services-for-Kubernetes-v1_0\nhttps://portal.nutanix.com/page/documents/details?targetId=Release-Notes-Nutanix-Data-Services-for-Kubernetes-v1_0:Release-Notes-Nutanix-Data-Services-for-Kubernetes-v1_0\nhttps://github.com/nutanix/helm/tree/nutanix-csi-snapshot-6.3.2/charts/nutanix-csi-snapshot#install\nhttps://github.com/nutanix/helm/tree/csi-v3.0.0-beta-beta1/charts/nutanix-csi-storage\nhttps://access.redhat.com/documentation/zh-tw/openshift_container_platform/4.13/html/building_applications/working-with-helm-charts\nhttps://blog.csdn.net/u010918487/article/details/115509935\nhttps://portal.nutanix.com/page/documents/kbs/details?targetId=kA0VO00000008Qb0AI\nhttps://quay.io/repository/karbon/k8s-agent?tab=tags\nhttps://github.com/nutanix/helm-releases/releases\nhttps://github.com/orgs/nutanix-cloud-native/repositories?type=all\nRequirements NKE v2.9 以上\nPrism Central 2023.4 以上\nKubernetes version v1.26 以上\nNutanix CSI Driver v3.0 (beta) 以上\nLab Enviroments Prism Central : 2024.1\nAOS: 6.7.1\nNKE: 2.9\nOpenShift : 4.13\nOpenshift IPI on Nutanix AHV\nNDK install Lab install helm chart on bastion https://access.redhat.com/documentation/zh-tw/openshift_container_platform/4.13/html/building_applications/working-with-helm-charts\n1 2 3 4 5 6 7 8 $ sudo curl -L https://mirror.openshift.com/pub/openshift-v4/clients/helm/latest/helm-linux-amd64 -o /usr/local/bin/helm $ sudo chmod +x /usr/local/bin/helm $ helm version WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/nutanix/.kube/config version.BuildInfo{Version:\u0026#34;v3.13.2+35.el9\u0026#34;, GitCommit:\u0026#34;fa6e939d7984e1be0d6fbc2dc920b6bbcf395932\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.20.12\u0026#34;} $ helm repo add nutanix https://nutanix.github.io/helm/ Nutanix CSI 3.0 (beta) Create OCP Secret 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Create Secret pc and pe $ vi ntnx-secret.yaml apiVersion: v1 kind: Secret metadata: name: ntnx-secret namespace: ntnx-system stringData: # prism-element-ip:prism-port:admin:password key: pe-ip:9440:admin:password $ vi ntnx-pc-secret.yaml apiVersion: v1 kind: Secret metadata: name: ntnx-pc-secret namespace: ntnx-system stringData: # prism-pc-ip:prism-port:admin:password key: pc-ip:9440:admin:password --- $ oc apply -f ntnx-pc-secret.yaml $ oc apply -f ntnx-secret.yaml Install CSI Driver Download CSI Driver and Edit value.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ wget https://github.com/nutanix/helm-releases/releases/download/nutanix-csi-storage/nutanix-csi-storage-3.0.0-beta.1912.tgz $ tar -xvf nutanix-csi-storage-3.0.0-beta.1912.tgz nutanix-csi-storage/Chart.yaml nutanix-csi-storage/values.yaml nutanix-csi-storage/templates/NOTES.txt nutanix-csi-storage/templates/_helpers.tpl nutanix-csi-storage/templates/csi-driver.yaml nutanix-csi-storage/templates/machine-config.yaml nutanix-csi-storage/templates/ntnx-csi-controller-deployment.yaml nutanix-csi-storage/templates/ntnx-csi-init-configmap.yaml nutanix-csi-storage/templates/ntnx-csi-node-ds.yaml nutanix-csi-storage/templates/ntnx-csi-rbac.yaml nutanix-csi-storage/templates/ntnx-csi-scc.yaml nutanix-csi-storage/templates/ntnx-sc.yaml nutanix-csi-storage/templates/ntnx-secret.yaml nutanix-csi-storage/templates/service-prometheus-csi.yaml nutanix-csi-storage/.helmignore nutanix-csi-storage/Nutanix core_k8s-csi_beta1 Notice.txt nutanix-csi-storage/README.md nutanix-csi-storage/questions.yml Edit values.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ cd nutanix-csi-storage $ vim values.yaml --- # all namespace namespace: ntnx-system createPrismCentralSecret: false # prismCentralEndPoint: 00.00.00.00 # pcUsername: username # pcPassword: password pcSecretName: ntnx-pc-secret createSecret: false # prismEndPoint: 00.00.000.000 # username: username # password: password peSecretName: ntnx-secret Install CSI Driver using helm\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ helm install -n ntnx-system -f nutanix-csi-storage/values.yaml nutanix-csi ./nutanix-csi-storage WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/nutanix/.kube/config NAME: nutanix-csi LAST DEPLOYED: Mon May 20 16:38:01 2024 NAMESPACE: ntnx-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Driver name: csi.nutanix.com Nutanix CSI provider was deployed in namespace ntnx-system. Check it\u0026#39;s status by running: kubectl -n ntnx-system get pods | grep \u0026#39;nutanix-csi\u0026#39; $ oc describe daemonset.apps/nutanix-csi-node ... ---- ------- Warning FailedCreate 12m (x19 over 23m) daemonset-controller Error creating: pods \u0026#34;nutanix-csi-node-\u0026#34; is forbidden: unable to validate against any security context constraint: [provider \u0026#34;anyuid\u0026#34;: Forbidden: not usable by user or serviceaccount Apply privilege to openshift-cluster-csi-drivers namespace\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ oc project ntnx-system Now using project \u0026#34;ntnx-system\u0026#34; on server \u0026#34;https://api.ocplab.nutanixlab.local:6443\u0026#34;. $ oc adm policy add-scc-to-user privileged -z nutanix-csi-controller clusterrole.rbac.authorization.k8s.io/system:openshift:scc:privileged added: \u0026#34;nutanix-csi-controller\u0026#34; $ oc adm policy add-scc-to-user privileged -z nutanix-csi-node clusterrole.rbac.authorization.k8s.io/system:openshift:scc:privileged added: \u0026#34;nutanix-csi-node\u0026#34; $ oc adm policy add-scc-to-user privileged -z node-exporter $ oc adm policy add-scc-to-user anyuid -z nutanix-csi-controller clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \u0026#34;nutanix-csi-controller\u0026#34; $ oc adm policy add-scc-to-user anyuid -z nutanix-csi-node clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \u0026#34;nutanix-csi-node\u0026#34; Check CSI installed\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ oc get all NAME READY STATUS RESTARTS AGE pod/nutanix-csi-controller-844589fc9f-4jr5m 7/7 Running 3 (16h ago) 16h pod/nutanix-csi-controller-844589fc9f-wwq7m 7/7 Running 4 (16h ago) 16h pod/nutanix-csi-node-fkmdd 3/3 Running 0 16h pod/nutanix-csi-node-hjnwg 3/3 Running 0 16h pod/nutanix-csi-node-pwqwl 3/3 Running 0 16h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nutanix-csi-metrics ClusterIP 172.30.168.77 \u0026lt;none\u0026gt; 9809/TCP,9810/TCP,9811/TCP,9812/TCP 17h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/nutanix-csi-node 3 3 3 3 3 kubernetes.io/os=linux 17h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nutanix-csi-controller 2/2 2 2 17h NAME DESIRED CURRENT READY AGE replicaset.apps/nutanix-csi-controller-844589fc9f 2 2 2 17h $ oc describe daemonset.apps/nutanix-csi-node Normal SuccessfulCreate 40s daemonset-controller Created pod: nutanix-csi-node-cgdvs Normal SuccessfulCreate 40s daemonset-controller Created pod: nutanix-csi-node-c49f5 Normal SuccessfulCreate 40s daemonset-controller Created pod: nutanix-csi-node-4j69r OCP Console\nCreate StorageClass create nutanix-volume.yaml\noc apply -f nutanix-volume.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ vim nutanix-volume.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; name: nutanixvolume provisioner: csi.nutanix.com parameters: csi.storage.k8s.io/provisioner-secret-name: ntnx-secret csi.storage.k8s.io/provisioner-secret-namespace: ntnx-system csi.storage.k8s.io/node-publish-secret-name: ntnx-secret csi.storage.k8s.io/node-publish-secret-namespace: ntnx-system csi.storage.k8s.io/controller-expand-secret-name: ntnx-secret csi.storage.k8s.io/controller-expand-secret-namespace: ntnx-system csi.storage.k8s.io/fstype: ext4 dataServiceEndPoint: IP:3260 # Prism data service ip storageContainer: NX_1365_AHV_cr # Prism 上建立的Storage Container storageType: NutanixVolumes #whitelistIPMode: ENABLED #chapAuth: ENABLED allowVolumeExpansion: true reclaimPolicy: Delete $ oc apply -f nutanix-volume.yaml Check StorageClass\n1 2 3 $ oc get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE nutanixvolume csi.nutanix.com Delete Immediate true 121m Create test-pvc\nCert-manager Install cert-manager tools\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 路徑 https://github.com/cert-manager/cert-manager/releases/tag/v1.14.5 https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.yaml $ oc new-project cert-manager $ oc apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.yaml $ oc get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5658d944df-9tqqp 1/1 Running 0 34s cert-manager-cainjector-cb99ff845-24r6d 1/1 Running 0 34s cert-manager-webhook-7fd74b8dc7-ts6pk 1/1 Running 0 34s Onboard OCP to PC 建立Project 把 ocp相關的VM放進去 (ocp-lab)\n用Support Portal 上下載的k8s-agent.tar 需要自行上傳到Dokcer Hub或是其他私倉，並且修改value.yaml 內的image路徑\nget cluster uid and name\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ kubectl get ns kube-system -o json |grep uid \u0026#34;openshift.io/sa.scc.uid-range\u0026#34;: \u0026#34;1000020000/10000\u0026#34; \u0026#34;uid\u0026#34;: \u0026#34;25c8543a-3df0-442a-83eb-5af1ab1b7be3\u0026#34; $ kubectl get infrastructure cluster -o yaml | grep infrastructureName infrastructureName: ocplab-qx6t5 $ oc get -o jsonpath=\u0026#39;{.status.infrastructureName}{\u0026#34;\\n\u0026#34;}\u0026#39; infrastructure cluster ocplab-qx6t5 $ export CLUSTER_UUID=25c8543a-3df0-442a-83eb-5af1ab1b7be3 $ export CLUSTER_INFRA_NAME=ocplab-qx6t5 PC上的VG會多k8s-的prefix k8s-25c8543a-3df0-442a-83eb-5af1ab1b7be3 Download k8s-agent.tar and ndk-1.0.0.tar\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 下載連結要重新產生 $ curl -o k8s-agent-1.0.0.tar -O \u0026#34;https://download.nutanix.com/downloads/ndk/1.0.0/k8s-agent-1.0.0.tar?Expires=1715275315306\u0026amp;Key-Pair-Id=APKAJTTNCWPEI42QKMSA\u0026amp;Signature=exuDWlLvpFIhLfN6YpDd9SyAz1a8o0CudJabSf8j~FEAVpwd6CjFe2WgCae2Jp5gZjNwwIEvHq4lXbvRMqDWm~LEVxO9E84ribrxu2uDNbkbAuG7I0OeWEDROsdpM1-qN~g9fp16-X~Y8lG-sh1cOLz-N8hjgq83vDMeusfTUrRMQOjlrGV7h44QL0-LlIs8GkkoIJbh3rxhZO~vavCvs4EJ9uec9qH6BrQLMfstVZr7j6uLu4juf6hnEsWJzalJXZ04YaA8onpgm-jhNBj2m5fxjXlJ-iCsu6u6BSLmm2ffpzSkUUlFpoSJZX~QODiHHiQi9V9xe9R8wdkQPfo-1g__\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 59.2M 100 59.2M 0 0 10.4M 0 0:00:05 0:00:05 --:--:-- 13.2M $ curl -o ndk-1.0.0.tar -O \u0026#34;https://download.nutanix.com/downloads/ndk/1.0.0/ndk-1.0.0.tar?Expires=1715276310943\u0026amp;Key-Pair-Id=APKAJTTNCWPEI42QKMSA\u0026amp;Signature=A8y8JoJupVvWedB~jSHTysEZK18eyITvdT6DFw9C9JpVOhKc9jnZraaDir2DZDeRVpnPaOraMF2lR5lFqlSaikGj274ETgetuz33R0jcJ-fB44~xfXL~eK5cpqNhuuG3Yufs0biq1b8CC-frrVeupzKzOdmJyBEWXPe4GSCLWwqPubPjhUAHMKyoOgcxSd-A6kS9f~c0eEikartUrgtCjyZP-iZwPxn6gg3jGoZuHK3f2Nf~HjmyrJqwBmLQA0ibVdRTN0YkYVfplpkr5ucp8MLMPFbhdmAJZIAot8vc2-qMPoidczgU1Msd1w8NsXWMUFQBnO-Y73VZ31d4yxI31A__\u0026#34; $ mkdir ndk-k8s-agent $ cd ndk-k8s-agent $ ls k8s-agent-1.0.0.tar k8s-agent-1.0.0.tar $ mv k8s-agent-1.0.0.tar ndk-k8s-agent $ tar xvf k8s-agent-1.0.0.tar ./k8s-agent-1.0.0/ ./k8s-agent-1.0.0/Chart.yaml ./k8s-agent-1.0.0/.helmignore ./k8s-agent-1.0.0/k8s-agent-1.0.0.tar ./k8s-agent-1.0.0/templates/ ./k8s-agent-1.0.0/templates/hook-predelete.yaml ./k8s-agent-1.0.0/templates/hook-preinstall.yaml ./k8s-agent-1.0.0/templates/hook-postdelete.yaml ./k8s-agent-1.0.0/templates/hook-postupgrade.yaml ./k8s-agent-1.0.0/templates/hook-postinstall.yaml ./k8s-agent-1.0.0/templates/nutanix_agent_deployment.yaml ./k8s-agent-1.0.0/values.schema.json ./k8s-agent-1.0.0/README.md ./k8s-agent-1.0.0/values.yaml ./k8s-agent-1.0.0/Nutanix-core_k8s-agent-Notice.txt push image to docker hub\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ echo -n \u0026#39;kenxxxxxxxx:xuxxxxxxxx\u0026#39; |base64 dockerconfigVubnxxxxxxxxxxxxxxxx2 { \u0026#34;auths\u0026#34;: { \u0026#34;https://index.docker.io/v1/\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;a2Vua2Vubnxxxxxxxxxxxxxxxx2\u0026#34; } } } $ vim dockerconfig.json $ cat dockerconfig.json |base64 dockerconfigVubnxxxxxxxxxxxxxxxx2 $ podman load -i k8s-agent-1.0.0.tar $ podman tag localhost/k8s-agent:1.0.0 kenkennyinfo/nke-k8s-agent:1.0.0 $ podman push kenkennyinfo/nke-k8s-agent:1.0.0 $ podman tag localhost/k8s-agent:1.0.0 kenkennyinfo/nke-k8s-agent:668 $ podman push kenkennyinfo/nke-k8s-agent:668 Install k8s-agent\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 安裝失敗時可用刪除指令 $ oc delete clusterroles.rbac.authorization.k8s.io \u0026#34;nke-k8s-agent-preinstall\u0026#34; $ oc delete ClusterRoleBinding nke-k8s-agent-preinstall $ oc delete project ntnx-system $ oc new-project ntnx-system !用最新版本的需要自行上傳! ## Docker Hub (版本較新 v668) --- $ vim values.yaml agent: namespaceOverride: ntnx-system name: nutanix-agent port: 8080 image: repository: docker.io/kenkennyinfo name: nke-k8s-agent pullPolicy: IfNotPresent tag: 668 privateRegistry: true imageCredentials: dockerconfig: \u0026#34;dockerconfigVubnxxxxxxxxxxxxxxxx2\u0026#34; updateConfigInMin: 10 updateMetricsInMin: 360 pc: port: 9440 insecure: false #set this to true if PC does not have https enabled endpoint: \u0026#34;\u0026#34; # eg: ip or fqdn username: \u0026#34;\u0026#34; # eg: admin or any other user with Kubernetes Infrastructure provision role password: \u0026#34;\u0026#34; k8sClusterName: \u0026#34;\u0026#34; k8sClusterUUID: \u0026#34;\u0026#34; k8sDistribution: \u0026#34;OCP\u0026#34; # eg: CAPX or NKE or OCP categoryMappings: \u0026#34;\u0026#34; # \u0026#34;one or more comma separated key=value\u0026#34; eg: \u0026#34;key1=value1\u0026#34; or \u0026#34;key1=value1,key2=value2\u0026#34; --- ## 官方quay.io (版本較舊 v594) $ vim values.yaml agent: namespaceOverride: ntnx-system name: nutanix-agent port: 8080 image: repository: quay.io/karbon name: k8s-agent pullPolicy: IfNotPresent tag: 594 privateRegistry: false updateConfigInMin: 10 updateMetricsInMin: 360 pc: port: 9440 insecure: false #set this to true if PC does not have https enabled endpoint: \u0026#34;\u0026#34; # eg: ip or fqdn username: \u0026#34;\u0026#34; # eg: admin or any other user with Kubernetes Infrastructure provision role password: \u0026#34;\u0026#34; k8sClusterName: \u0026#34;\u0026#34; k8sClusterUUID: \u0026#34;\u0026#34; k8sDistribution: \u0026#34;OCP\u0026#34; # eg: CAPX or NKE or OCP categoryMappings: \u0026#34;\u0026#34; # \u0026#34;one or more comma separated key=value\u0026#34; eg: \u0026#34;key1=value1\u0026#34; or \u0026#34;key1=value1,key2=value2\u0026#34; --- Helm 安裝 $ helm install k8s-agent-1.0.0 /home/nutanix/k8s-agent-1.0.0 --set pc.endpoint=172.16.90.75,pc.username=ocpadmin,pc.password=password,pc.insecure=true,k8sClusterName=ocplab-qx6t5,k8sClusterUUID=25c8543a-3df0-442a-83eb-5af1ab1b7be3,k8sDistribution=OCP -n ntnx-system --create-namespace WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/nutanix/.kube/config NAME: k8s-agent-594 LAST DEPLOYED: Wed May 15 14:09:01 2024 NAMESPACE: ntnx-system STATUS: deployed REVISION: 1 TEST SUITE: None ## 安裝新版本或是更改密碼可以使用helm upgrade (修改values.yaml) $ helm upgrade k8s-agent-594 /home/nutanix/k8s-agent-1.0.0 --set pc.endpoint=172.16.90.75,pc.username=ocpadmin,pc.password=password,pc.insecure=true,k8sClusterName=ocplab-qx6t5,k8sClusterUUID=25c8543a-3df0-442a-83eb-5af1ab1b7be3 -n ntnx-system WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/nutanix/.kube/config Release \u0026#34;k8s-agent-594\u0026#34; has been upgraded. Happy Helming! NAME: k8s-agent-594 LAST DEPLOYED: Wed May 15 14:13:56 2024 NAMESPACE: ntnx-system STATUS: deployed REVISION: 2 TEST SUITE: None $ helm upgrade k8s-agent-594 /home/nutanix/k8s-agent-1.0.0 --set pc.endpoint=172.16.90.75,pc.username=ocpadmin,pc.password=password,pc.insecure=true,k8sClusterName=ocplab-qx6t5,k8sClusterUUID=25c8543a-3df0-442a-83eb-5af1ab1b7be3 -n ntnx-system WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/nutanix/.kube/config Release \u0026#34;k8s-agent-594\u0026#34; has been upgraded. Happy Helming! NAME: k8s-agent-594 LAST DEPLOYED: Wed May 15 15:18:35 2024 NAMESPACE: ntnx-system STATUS: deployed REVISION: 3 TEST SUITE: None ## Check pods and deployments $ oc get all -n ntnx-system NAME READY STATUS RESTARTS AGE pod/nutanix-agent-75df9d879b-98smz 1/1 Running 0 80s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nutanix-agent 1/1 1 1 70m NAME DESIRED CURRENT READY AGE replicaset.apps/nutanix-agent-75df9d879b 1 1 1 80s replicaset.apps/nutanix-agent-7c4b644996 0 0 0 70m $ oc describe deployment.apps/nutanix-agent -n ntnx-system Name: nutanix-agent Namespace: ntnx-system CreationTimestamp: Wed, 15 May 2024 14:09:31 +0800 Labels: app.kubernetes.io/managed-by=Helm Annotations: deployment.kubernetes.io/revision: 2 meta.helm.sh/release-name: k8s-agent-594 meta.helm.sh/release-namespace: ntnx-system Selector: app=nutanix-agent Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nutanix-agent Service Account: nutanix-agent Containers: nutanix-agent: Image: docker.io/kenkennyinfo/nke-k8s-agent:668 Port: 8080/TCP Host Port: 0/TCP Command: /k8s-agent/k8s-agent Args: -pc-endpoint=172.16.90.75 -pc-port=9440 -insecure=true -k8s-cluster-name=ocplab-qx6t5 -k8s-cluster-uuid=25c8543a-3df0-442a-83eb-5af1ab1b7be3 -k8s-distribution=OCP -k8s-cluster-category= -update-config-in-min=10 -update-metrics-in-min=360 Limits: cpu: 500m memory: 128Mi Requests: cpu: 250m memory: 64Mi Liveness: http-get http://:8080/health delay=5s timeout=5s period=15s #success=1 #failure=3 Readiness: http-get http://:8080/readiness delay=5s timeout=1s period=10s #success=1 #failure=3 Environment: \u0026lt;none\u0026gt; Mounts: /certs from pc-creds-volume (ro) Volumes: pc-creds-volume: Type: Secret (a volume populated by a Secret) SecretName: nutanix-agent Optional: false Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nutanix-agent-75df9d879b (1/1 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 70m deployment-controller Scaled up replica set nutanix-agent-7c4b644996 to 1 Normal ScalingReplicaSet 84s deployment-controller Scaled up replica set nutanix-agent-75df9d879b to 1 Normal ScalingReplicaSet 73s deployment-controller Scaled down replica set nutanix-agent-7c4b644996 to 0 from 1 安裝後進入Prism Central 確認已 Onboard 完成\nNDK Install without TLS 下載 ndk-1.0.0.tar 並解壓縮\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 $ wget https://download.nutanix.com/downloads/ndk/1.0.0/ndk-1.0.0.tar?Expires=1715793783203\u0026amp;Key-Pair-Id=APKAJTTNCWPEI42QKMSA\u0026amp;Signature=bT~JG3wJjEQ3dvNO85khPiJq1yVxAH6YPV7PGtJvKaf4iJVRDfQIockkqSyH3ZubXpSvF2loZAXqvueSef41SZa9vSzEC1zrc~IzwcCjAcXnAbXC1LGy566yIBvwNaFm1qSr0jONb3zwzckNHidS8QmTRRyscaiHdF8xQ~gD0MQfKBlrl4hej3Pnae0ckMZoT5yCPzAv8vQU5XhvL0YECmy2T6LhBH13mxHpijM70aACCjdE5KdlCHFrSZhsDYYy5FWRYT02q70mM4SAUNYYcBnW4FNmmKuPWPyq0r2lcJRktIOgfQOHvUv-qw0vyrEdRj7QmgWpiCAAoSfqWl-lKA__ $ tar -xvf ndk-1.0.0.tar $ tree ndk-1.0.0 ndk-1.0.0 ├── chart │ ├── Chart.yaml │ ├── crds │ │ ├── dataservices.nutanix.com_applicationsnapshotcontents.yaml │ │ ├── dataservices.nutanix.com_applicationsnapshotreplications.yaml │ │ ├── dataservices.nutanix.com_applicationsnapshotrestores.yaml │ │ ├── dataservices.nutanix.com_applicationsnapshots.yaml │ │ ├── dataservices.nutanix.com_applications.yaml │ │ ├── dataservices.nutanix.com_appprotectionplans.yaml │ │ ├── dataservices.nutanix.com_protectionplans.yaml │ │ ├── dataservices.nutanix.com_remotes.yaml │ │ ├── dataservices.nutanix.com_replicationtargets.yaml │ │ ├── dataservices.nutanix.com_storageclusters.yaml │ │ └── scheduler.nutanix.com_jobschedulers.yaml │ ├── Nutanix-core-k8s-job-scheduler-Disclosure.txt │ ├── Nutanix-core-k8s-juno-aos-pc-client-Disclosure.txt │ ├── Nutanix-core-k8s-juno-Disclosure.txt │ ├── Nutanix-kube-rbac-proxy-Disclosure.txt │ ├── templates │ │ ├── cert-manager.yaml │ │ ├── controller-manager-metrics-monitor.yaml │ │ ├── csi-precheck.yaml │ │ ├── deployment.yaml │ │ ├── _helpers.tpl │ │ ├── intercom-service.yaml │ │ ├── leader-election-rbac.yaml │ │ ├── manager-config.yaml │ │ ├── manager-rbac.yaml │ │ ├── metrics-reader-rbac.yaml │ │ ├── metrics-service.yaml │ │ ├── ndk-logs-pvc.yaml │ │ ├── ntnx-log-sc.yaml │ │ ├── proxy-rbac.yaml │ │ ├── registry-secret.yaml │ │ ├── scheduler-webhook-service.yaml │ │ ├── webhook-certificate.yaml │ │ └── webhook-service.yaml │ └── values.yaml └── ndk-1.0.0.tar 3 directories, 36 files 儲存Container image 並上傳至Docker hub or 私倉\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # Load Images $ podman load -i ndk-1.0.0/ndk-1.0.0.tar Writing manifest to image destination Storing signatures Loaded image: localhost/ndk/infra-manager:1.0.0 Loaded image: localhost/ndk/job-scheduler:1.0.0 Loaded image: localhost/ndk/kube-rbac-proxy:v0.8.0 Loaded image: localhost/ndk/manager:1.0.0 # Tag Images podman tag \u0026lt;ndk-image-name\u0026gt;:tag \u0026lt;image-registry-url\u0026gt;/ndk/\u0026lt;conatiner-name\u0026gt;:tag $ podman tag ndk/manager:1.0.0 kenkennyinfo/ndk-manager:1.0.0 $ podman tag ndk/job-scheduler:1.0.0 kenkennyinfo/ndk-job-scheduler:1.0.0 $ podman tag ndk/kube-rbac-proxy:v0.8.0 kenkennyinfo/ndk-kube-rbac-proxy:v0.8.0 $ podman tag ndk/infra-manager:1.0.0 kenkennyinfo/ndk-infra-manager:1.0.0 # batch tag images $ for img in ndk/manager:1.0.0 ndk/infra-manager:1.0.0 ndk/job-scheduler:1.0.0 ndk/kube-rbac-proxy:v0.8.0; do docker tag $img kenkennyinfo/${img}; done Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. Error: ndk/infra-manager-client:1.0.0: image not known Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. $ podman images [nutanix@ken-rhel9 ndk-1.0.0]$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE localhost/kenkennyinfo/ndk-manager 1.0.0 ebffd7549ea5 2 months ago 69.2 MB localhost/ndk/manager 1.0.0 ebffd7549ea5 2 months ago 69.2 MB localhost/kenkennyinfo/ndk-infra-manager 1.0.0 2982ed521b0e 2 months ago 60 MB localhost/ndk/infra-manager 1.0.0 2982ed521b0e 2 months ago 60 MB localhost/kenkennyinfo/nke-k8s-agent 1.0.0 9bda464636b6 2 months ago 61.7 MB localhost/kenkennyinfo/nke-k8s-agent 668 9bda464636b6 2 months ago 61.7 MB localhost/k8s-agent 1.0.0 9bda464636b6 2 months ago 61.7 MB docker.io/kenkennyinfo/nke-k8s-agent 1.0.0 9bda464636b6 2 months ago 61.7 MB quay.io/karbon/k8s-agent 594 171ea3376596 3 months ago 58.4 MB localhost/ndk/job-scheduler 1.0.0 d594a3e567b7 3 months ago 59.1 MB localhost/kenkennyinfo/ndk-job-scheduler 1.0.0 d594a3e567b7 3 months ago 59.1 MB localhost/kenkennyinfo/ndk-kube-rbac-proxy v0.8.0 ad393d6a4d1b 3 years ago 50.2 MB localhost/ndk/kube-rbac-proxy v0.8.0 ad393d6a4d1b 3 years ago 50.2 MB # Push Images $ podman push kenkennyinfo/ndk-kube-rbac-proxy:v0.8.0 $ podman push kenkennyinfo/ndk-job-scheduler:1.0.0 $ podman push kenkennyinfo/ndk-infra-manager:1.0.0 $ podman push kenkennyinfo/ndk-manager:1.0.0 建立Secret，修改 chart/values.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 # 建立Docker hub secret $ vim nutanix-k8s-agent-pull-secret.yaml apiVersion: v1 kind: Secret metadata: name: nutanix-k8s-agent-pull-secret type: kubernetes.io/dockerconfigjson data: .dockerconfigjson: \u0026#34;dockerconfigVubnxxxxxxxxxxxxxxxx2\u0026#34; --- $ vim ntnx-pc-secret2.yaml apiVersion: v1 kind: Secret metadata: name: ntnx-pc-secret namespace: ntnx-system stringData: # prism-pc-ip:prism-port:admin:password key: 172.16.90.75:9440:ocpadmin:password --- $ oc apply -f nutanix-k8s-agent-pull-secret.yaml -n ntnx-system $ oc apply -f ntnx-pc-secret2.yaml -n ntnx-system $ oc get secret -n ntnx-system NAME TYPE DATA AGE nutanix-k8s-agent-pull-secret kubernetes.io/dockerconfigjson 1 12s --- $ vim /home/nutanix/ndk-1.0.0/chart/values.yaml manager: # -- Image Repository repository: kenkennyinfo/ndk-manager # -- Image tag # @default -- .Chart.AppVersion tag: 1.0.0 # -- Image pull policy pullPolicy: Always infraManager: # -- Image Repository repository: kenkennyinfo/ndk-infra-manager # -- Image tag # @default -- .Chart.AppVersion tag: 1.0.0 # -- Image pull policy pullPolicy: Always kubeRbacProxy: # -- Image Repository repository: kenkennyinfo/ndk-kube-rbac-proxy # -- Image tag # @default -- .Chart.AppVersion tag: v0.8.0 jobScheduler: # -- Image Repository repository: kenkennyinfo/ndk-job-scheduler # -- Image tag # @default -- .Chart.AppVersion tag: 1.0.0 # -- Image pull policy pullPolicy: Always imageCredentials: # Name of the secret containing the credentials to pull image from the container registry. imagePullSecretName: nutanix-k8s-agent-pull-secret clusterName: ocplab.nutanixlab.local intercomService: ports: - port: 2021 protocol: TCP targetPort: 2021 type: NodePort ---end 安裝NDK\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 helm install ndk -n ntnx-system \u0026lt;chart-location\u0026gt; --set tls.server.enable=false $ helm install ndk -n ntnx-system /home/nutanix/ndk-1.0.0/chart/ --set tls.server.enable=false WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/nutanix/.kube/config NAME: ndk LAST DEPLOYED: Wed May 15 17:14:37 2024 NAMESPACE: ntnx-system STATUS: deployed REVISION: 1 TEST SUITE: None $ oc get pods -n ntnx-system NAME READY STATUS RESTARTS AGE ndk-controller-manager-55bf75f64b-blnfc 4/4 Running 0 3m57s nutanix-agent-75df9d879b-98smz 1/1 Running 0 120m $ oc adm policy add-scc-to-user anyuid -z ndk-controller-manager clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \u0026#34;ndk-controller-manager\u0026#34; $ helm upgrade ndk -n ntnx-system /home/nutanix/ndk-1.0.0/chart/ --set tls.server.enable=false WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/nutanix/.kube/config Release \u0026#34;ndk\u0026#34; has been upgraded. Happy Helming! NAME: ndk LAST DEPLOYED: Mon May 20 17:57:43 2024 NAMESPACE: ntnx-system STATUS: deployed REVISION: 3 TEST SUITE: None 安裝完成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ oc get all -n ntnx-system NAME READY STATUS RESTARTS AGE pod/ndk-controller-manager-55bf75f64b-blnfc 4/4 Running 0 17h pod/nutanix-agent-75df9d879b-98smz 1/1 Running 0 19h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ndk-controller-manager-metrics-service ClusterIP 172.30.186.204 \u0026lt;none\u0026gt; 8443/TCP 17h service/ndk-intercom-service NodePort 172.30.237.126 \u0026lt;none\u0026gt; 2021:30998/TCP 17h service/ndk-scheduler-webhook-service ClusterIP 172.30.64.110 \u0026lt;none\u0026gt; 9444/TCP 17h service/ndk-webhook-service ClusterIP 172.30.66.56 \u0026lt;none\u0026gt; 443/TCP 17h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ndk-controller-manager 1/1 1 1 17h deployment.apps/nutanix-agent 1/1 1 1 20h NAME DESIRED CURRENT READY AGE replicaset.apps/ndk-controller-manager-55bf75f64b 1 1 1 17h replicaset.apps/nutanix-agent-75df9d879b 1 1 1 19h replicaset.apps/nutanix-agent-7c4b644996 0 0 0 20h 測試NDK 連線 grpcurl\n1 2 3 4 5 6 $ podman pull fullstorydev/grpcurl:latest $ podman run fullstorydev/grpcurl -plaintext 172.16.90.180:30998 list grpc.reflection.v1.ServerReflection grpc.reflection.v1alpha.ServerReflection juno_interface.Juno Storage Cluster 取得PC、PE的Cluster UUID\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 on Prism Central CVM $ ncli cluster info Cluster Id : 8c52e6be-697b-42a4-bacd-8e6ada7994f2::4237199414008583410 Cluster Uuid : 8c52e6be-697b-42a4-bacd-8e6ada7994f2 Cluster Name : NTNXSELABPC Cluster Version : pc.2023.4 Cluster Full Version : el7.3-release-fraser-2023.4-stable-e10760423ef518a91beb02b784c7ff2e0cf3a45c Cluster FQDN : NTNXSELABPC.nutanixlab.local Is LTS : false External Data Services... : Support Verbosity Level : BASIC_COREDUMP Lock Down Status : Disabled Password Remote Login ... : Enabled Timezone : America/Los_Angeles NCC Version : ncc-4.6.6.1 Degraded Node Monitoring : Enabled on Prism Element CVM $ ncli cluster info Cluster Id : 00060f36-1379-a8ba-0000-000000028e95::167573 Cluster Uuid : 00060f36-1379-a8ba-0000-000000028e95 Cluster Name : NX1365G6PE Cluster Version : 6.7.1 Cluster Full Version : el7.3-release-fraser-6.7.1-stable-104e6662e8b936a9225379d2d68efd93b5448289 External IP address : 172.16.90.74 Node Count : 3 Block Count : 1 Shadow Clones Status : Enabled Has Self Encrypting Disk : no Cluster Masquerading I... : Cluster Masquerading PORT : Is registered to PC : true Rf1 Container Support : Enabled Rebuild Reservation : Disabled Encryption In Transit : Disabled Is LTS : false External Data Services... : 172.16.90.76 Support Verbosity Level : BASIC_COREDUMP Lock Down Status : Disabled Password Remote Login ... : Enabled Timezone : Asia/Taipei NCC Version : ncc-4.6.6.1 Degraded Node Monitoring : Enabled 建立StorageCluster.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ vim storagecluster-1365g6.yaml apiVersion: dataservices.nutanix.com/v1alpha1 kind: StorageCluster metadata: name: nx1365g6pe spec: storageServerUuid: 00060f36-1379-a8ba-0000-000000028e95 managementServerUuid: 8c52e6be-697b-42a4-bacd-8e6ada7994f2 $ oc apply -f storagecluster-1365g6.yaml $ oc get storagecluster NAME AVAILABLE storagecluster-1365g6 true $ oc get storagecluster NAME AVAILABLE nx1365g6pe false Create Sample App MySQL Mysql 測試，建立mysql.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 $ vim mysql-deploy.yaml apiVersion: v1 kind: Secret metadata: name: mysql-password type: opaque stringData: MYSQL_ROOT_PASSWORD: password --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-set spec: selector: matchLabels: app: mysql serviceName: \u0026#34;mysql\u0026#34; replicas: 3 template: metadata: labels: app: mysql spec: terminationGracePeriodSeconds: 10 containers: - name: mysql image: mysql:8.0 ports: - containerPort: 3306 volumeMounts: - name: mysql-store mountPath: /var/lib/mysql - name: mysql-data-1 mountPath: /usr/data1 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-password key: MYSQL_ROOT_PASSWORD volumeClaimTemplates: - metadata: name: mysql-store spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: nutanixvolume resources: requests: storage: 5Gi - metadata: name: mysql-data-1 spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: nutanixvolume resources: requests: storage: 3Gi 建立Mysql app\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 $ oc new-project ndk-mysql $ oc apply -f mysql-deploy.yaml -n ndk-mysql secret/mysql-password created statefulset.apps/mysql-set created $ oc get pods NAME READY STATUS RESTARTS AGE mysql-set-0 0/1 ContainerCreating 0 42s $ oc get pods NAME READY STATUS RESTARTS AGE mysql-set-0 1/1 Running 0 3m18s mysql-set-1 1/1 Running 0 2m22s mysql-set-2 1/1 Running 0 72s $ oc get pvc,pv NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/mysql-data-1-mysql-set-0 Bound pvc-276ea905-a815-4635-98c0-3a4fade1bc44 3Gi RWO nutanixvolume 6m24s persistentvolumeclaim/mysql-data-1-mysql-set-1 Bound pvc-5638dc79-8271-4a49-94b9-d22c6212e232 3Gi RWO nutanixvolume 5m28s persistentvolumeclaim/mysql-data-1-mysql-set-2 Bound pvc-a4e832f4-e99b-4bdc-bc5b-253731447ce4 3Gi RWO nutanixvolume 4m18s persistentvolumeclaim/mysql-store-mysql-set-0 Bound pvc-262a52dc-1861-4992-b2aa-e41f448a6db5 5Gi RWO nutanixvolume 6m24s persistentvolumeclaim/mysql-store-mysql-set-1 Bound pvc-1e809e1d-2190-4220-80ba-35712900d3a6 5Gi RWO nutanixvolume 5m28s persistentvolumeclaim/mysql-store-mysql-set-2 Bound pvc-fafd94af-11be-4b02-aef1-4d959607c0cf 5Gi RWO nutanixvolume 4m18s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-127de2ed-452d-4fc2-a08e-f320cc4c85e0 1Gi RWO Delete Bound default/my-pvc nutanixvolume 6d23h persistentvolume/pvc-1e809e1d-2190-4220-80ba-35712900d3a6 5Gi RWO Delete Bound ndk-mysql/mysql-store-mysql-set-1 nutanixvolume 5m22s persistentvolume/pvc-262a52dc-1861-4992-b2aa-e41f448a6db5 5Gi RWO Delete Bound ndk-mysql/mysql-store-mysql-set-0 nutanixvolume 6m15s persistentvolume/pvc-276ea905-a815-4635-98c0-3a4fade1bc44 3Gi RWO Delete Bound ndk-mysql/mysql-data-1-mysql-set-0 nutanixvolume 6m14s persistentvolume/pvc-5638dc79-8271-4a49-94b9-d22c6212e232 3Gi RWO Delete Bound ndk-mysql/mysql-data-1-mysql-set-1 nutanixvolume 5m21s persistentvolume/pvc-8468392e-a564-4d05-81f1-175820eceba2 1Gi RWO Delete Bound default/test-pvc nutanixvolume 20h persistentvolume/pvc-a4e832f4-e99b-4bdc-bc5b-253731447ce4 3Gi RWO Delete Bound ndk-mysql/mysql-data-1-mysql-set-2 nutanixvolume 4m9s persistentvolume/pvc-fafd94af-11be-4b02-aef1-4d959607c0cf 5Gi RWO Delete Bound ndk-mysql/mysql-store-mysql-set-2 nutanixvolume 4m9s 建立Application CR\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ vim mysql-cr.yaml apiVersion: dataservices.nutanix.com/v1alpha1 kind: Application metadata: name: mysql-cr namespace: ndk-mysql spec: applicationSelector: $ oc apply -f mysql-cr.yaml $ oc get application NAME AGE LAST-STATUS-UPDATE mysql-cr 31s 31s NDK Snapshot Manual Local Snapshot Create Application Snapshot CR\n1 2 3 4 5 6 7 8 9 10 11 $ vim mysql-snap.yaml apiVersion: dataservices.nutanix.com/v1alpha1 kind: ApplicationSnapshot metadata: name: mysql-snap-1 namespace: ndk-mysql spec: source: applicationRef: name: mysql-cr Apply Snapshot\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 $ oc apply -f mysql-snap.yaml applicationsnapshot.dataservices.nutanix.com/mysql-snap-1 created $ oc get applicationsnapshot -n ndk-mysql NAME AGE READY-TO-USE BOUND-SNAPSHOTCONTENT SNAPSHOT-AGE mysql-snap-1 8s false asc-8889156a-43f2-4d95-9075-11ccd52c9457 $ oc get applicationsnapshot -n ndk-mysql NAME AGE READY-TO-USE BOUND-SNAPSHOTCONTENT SNAPSHOT-AGE mysql-snap-1 58s true asc-8889156a-43f2-4d95-9075-11ccd52c9457 30s $ oc describe applicationsnapshot -n ndk-mysql Name: mysql-snap-1 Namespace: ndk-mysql Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: dataservices.nutanix.com/v1alpha1 Kind: ApplicationSnapshot Metadata: Creation Timestamp: 2024-05-22T10:10:24Z Finalizers: dataservices.nutanix.com/app-snap Generation: 1 Managed Fields: API Version: dataservices.nutanix.com/v1alpha1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:spec: .: f:source: .: f:applicationRef: .: f:name: Manager: kubectl-client-side-apply Operation: Update Time: 2024-05-22T10:10:24Z API Version: dataservices.nutanix.com/v1alpha1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:finalizers: .: v:\u0026#34;dataservices.nutanix.com/app-snap\u0026#34;: Manager: manager Operation: Update Time: 2024-05-22T10:10:24Z API Version: dataservices.nutanix.com/v1alpha1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:boundApplicationSnapshotContentName: f:creationTime: f:readyToUse: f:summary: .: f:snapshotArtifacts: .: f:apps/v1/StatefulSet: f:authorization.openshift.io/v1/RoleBinding: f:rbac.authorization.k8s.io/v1/RoleBinding: f:v1/ConfigMap: f:v1/PersistentVolumeClaim: f:v1/Secret: f:v1/ServiceAccount: Manager: manager Operation: Update Subresource: status Time: 2024-05-22T10:10:52Z Resource Version: 48052327 UID: 8889156a-43f2-4d95-9075-11ccd52c9457 Spec: Source: Application Ref: Name: mysql-cr Status: Bound Application Snapshot Content Name: asc-8889156a-43f2-4d95-9075-11ccd52c9457 Creation Time: 2024-05-22T10:10:52Z Ready To Use: true Summary: Snapshot Artifacts: apps/v1/StatefulSet: Name: mysql-set authorization.openshift.io/v1/RoleBinding: Name: admin Name: system:image-builders Name: system:deployers Name: system:image-pullers rbac.authorization.k8s.io/v1/RoleBinding: Name: system:image-builders Name: admin Name: system:image-pullers Name: system:deployers v1/ConfigMap: Name: openshift-service-ca.crt Name: kube-root-ca.crt v1/PersistentVolumeClaim: Name: mysql-store-mysql-set-1 Name: mysql-store-mysql-set-2 Name: mysql-data-1-mysql-set-0 Name: mysql-data-1-mysql-set-1 Name: mysql-data-1-mysql-set-2 Name: mysql-store-mysql-set-0 v1/Secret: Name: default-token-rpqq4 Name: mysql-password Name: deployer-token-cc92m Name: builder-token-xk5g4 v1/ServiceAccount: Name: deployer Name: default Name: builder Events: \u0026lt;none\u0026gt; OCP 上的 Volume Snapshot\nOCP Volume SnapshotContents\nNutanix Prism Central - VG Recovery Point\nNDK Restore Restore Local Snapshot Delete mysql.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 $ ll total 12 -rw-r--r--. 1 nutanix nutanix 393 May 16 13:45 mysql-cr.yaml -rw-r--r--. 1 nutanix nutanix 1213 May 16 11:18 mysql-deploy.yaml -rw-r--r--. 1 nutanix nutanix 183 May 20 15:45 mysql-snap.yaml $ oc get pvc -n ndk-mysql NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-data-1-mysql-set-0 Bound pvc-276ea905-a815-4635-98c0-3a4fade1bc44 3Gi RWO nutanixvolume 4d4h mysql-data-1-mysql-set-1 Bound pvc-5638dc79-8271-4a49-94b9-d22c6212e232 3Gi RWO nutanixvolume 4d4h mysql-data-1-mysql-set-2 Bound pvc-a4e832f4-e99b-4bdc-bc5b-253731447ce4 3Gi RWO nutanixvolume 4d4h mysql-store-mysql-set-0 Bound pvc-262a52dc-1861-4992-b2aa-e41f448a6db5 5Gi RWO nutanixvolume 4d4h mysql-store-mysql-set-1 Bound pvc-1e809e1d-2190-4220-80ba-35712900d3a6 5Gi RWO nutanixvolume 4d4h mysql-store-mysql-set-2 Bound pvc-fafd94af-11be-4b02-aef1-4d959607c0cf 5Gi RWO nutanixvolume 4d4h $ oc get all NAME READY STATUS RESTARTS AGE pod/mysql-set-0 1/1 Running 0 4d4h pod/mysql-set-1 1/1 Running 0 4d4h pod/mysql-set-2 1/1 Running 0 4d4h NAME READY AGE statefulset.apps/mysql-set 3/3 4d4h $ oc delete app mysql-cr -n ndk-mysql application.dataservices.nutanix.com \u0026#34;mysql-cr\u0026#34; deleted $ oc delete -f mysql-deploy.yaml -n ndk-mysql secret \u0026#34;mysql-password\u0026#34; deleted statefulset.apps \u0026#34;mysql-set\u0026#34; deleted $ oc delete pvc --all -n ndk-mysql persistentvolumeclaim \u0026#34;mysql-data-1-mysql-set-0\u0026#34; deleted persistentvolumeclaim \u0026#34;mysql-data-1-mysql-set-1\u0026#34; deleted persistentvolumeclaim \u0026#34;mysql-data-1-mysql-set-2\u0026#34; deleted persistentvolumeclaim \u0026#34;mysql-store-mysql-set-0\u0026#34; deleted persistentvolumeclaim \u0026#34;mysql-store-mysql-set-1\u0026#34; deleted persistentvolumeclaim \u0026#34;mysql-store-mysql-set-2\u0026#34; deleted $ oc get pvc -n ndk-mysql No resources found in ndk-mysql namespace. $ oc get all -n ndk-mysql No resources found in ndk-mysql namespace. Create Application Restore CR\n1 2 3 4 5 6 7 8 9 $ vim mysql-restore.yaml apiVersion: dataservices.nutanix.com/v1alpha1 kind: ApplicationSnapshotRestore metadata: name: restore-mysql-snap-1 namespace: ndk-mysql spec: applicationSnapshotName: mysql-snap-1 oc apply restore CR\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 $ oc apply -f mysql-restore.yaml applicationsnapshotrestore.dataservices.nutanix.com/restore-mysql-snap-1 created $ oc get applicationsnapshotrestore -n ndk-mysql -o yaml -o wide NAME SNAPSHOT-NAME COMPLETED restore-mysql-snap-1 mysql-snap-1 true $ oc describe applicationsnapshotrestore restore-mysql-snap-1 -n ndk-mysql Name: restore-mysql-snap-1 Namespace: ndk-mysql Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: dataservices.nutanix.com/v1alpha1 Kind: ApplicationSnapshotRestore Metadata: Creation Timestamp: 2024-05-22T10:14:26Z Finalizers: dataservices.nutanix.com/application-restore Generation: 1 Managed Fields: API Version: dataservices.nutanix.com/v1alpha1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:spec: .: f:applicationSnapshotName: Manager: kubectl-client-side-apply Operation: Update Time: 2024-05-22T10:14:26Z API Version: dataservices.nutanix.com/v1alpha1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:finalizers: .: v:\u0026#34;dataservices.nutanix.com/application-restore\u0026#34;: Manager: manager Operation: Update Time: 2024-05-22T10:14:26Z API Version: dataservices.nutanix.com/v1alpha1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:completed: f:conditions: Manager: manager Operation: Update Subresource: status Time: 2024-05-22T10:14:43Z Resource Version: 48054533 UID: 7a10fa58-3aaa-4592-8bdf-f6db2e9c4a0d Spec: Application Snapshot Name: mysql-snap-1 Status: Completed: true Conditions: Last Transition Time: 2024-05-22T10:14:26Z Message: Observed Generation: 1 Reason: RequestCompleted Status: False Type: Progressing Last Transition Time: 2024-05-22T10:14:27Z Message: All prechecks passed and finalizers on dependent resources set. Skipped restoring resources since they already exist: [\u0026#34;/v1, Kind=ConfigMap, ndk-mysql/openshift-service-ca.crt\u0026#34;,\u0026#34;/v1, Kind=ServiceAccount, ndk-mysql/default\u0026#34;,\u0026#34;/v1, Kind=Secret, ndk-mysql/builder-token-xk5g4\u0026#34;,\u0026#34;/v1, Kind=ServiceAccount, ndk-mysql/deployer\u0026#34;,\u0026#34;rbac.authorization.k8s.io/v1, Kind=RoleBinding, ndk-mysql/admin\u0026#34;,\u0026#34;rbac.authorization.k8s.io/v1, Kind=RoleBinding, ndk-mysql/admin\u0026#34;,\u0026#34;/v1, Kind=Secret, ndk-mysql/default-token-rpqq4\u0026#34;,\u0026#34;rbac.authorization.k8s.io/v1, Kind=RoleBinding, ndk-mysql/system:image-pullers\u0026#34;,\u0026#34;rbac.authorization.k8s.io/v1, Kind=RoleBinding, ndk-mysql/system:image-pullers\u0026#34;,\u0026#34;/v1, Kind=Secret, ndk-mysql/deployer-token-cc92m\u0026#34;,\u0026#34;/v1, Kind=ServiceAccount, ndk-mysql/builder\u0026#34;,\u0026#34;rbac.authorization.k8s.io/v1, Kind=RoleBinding, ndk-mysql/system:image-builders\u0026#34;,\u0026#34;rbac.authorization.k8s.io/v1, Kind=RoleBinding, ndk-mysql/system:image-builders\u0026#34;,\u0026#34;/v1, Kind=ConfigMap, ndk-mysql/kube-root-ca.crt\u0026#34;,\u0026#34;rbac.authorization.k8s.io/v1, Kind=RoleBinding, ndk-mysql/system:deployers\u0026#34;,\u0026#34;rbac.authorization.k8s.io/v1, Kind=RoleBinding, ndk-mysql/system:deployers\u0026#34;] Observed Generation: 1 Reason: PrechecksPassed Status: True Type: PrechecksPassed Last Transition Time: 2024-05-22T10:14:27Z Message: All eligible volumes restored Observed Generation: 1 Reason: VolumesRestored Status: True Type: VolumesRestored Last Transition Time: 2024-05-22T10:14:28Z Message: All eligible application configs restored Observed Generation: 1 Reason: ApplicationConfigRestored Status: True Type: ApplicationConfigRestored Last Transition Time: 2024-05-22T10:14:43Z Message: Application restore successfully finalised Observed Generation: 1 Reason: ApplicationRestoreFinalised Status: True Type: ApplicationRestoreFinalised Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal WaitingForVolumeBound 55s (x2 over 55s) NDK Waiting for PVCs to get Bound. Unbound PVCs: [mysql-store-mysql-set-2 mysql-data-1-mysql-set-0 mysql-store-mysql-set-1 mysql-data-1-mysql-set-2 mysql-data-1-mysql-set-1 mysql-store-mysql-set-0] 確認Application 、PVC、PV 復原完成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ oc get all -n ndk-mysql NAME READY STATUS RESTARTS AGE pod/mysql-set-0 1/1 Running 0 2m9s pod/mysql-set-1 1/1 Running 0 106s pod/mysql-set-2 1/1 Running 0 92s NAME READY AGE statefulset.apps/mysql-set 3/3 2m10s $ oc get pv,pvc -n ndk-mysql NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-06e3f4c4-c4f1-415f-8194-2e4d356570d0 5Gi RWO Delete Bound ndk-mysql/mysql-store-mysql-set-1 nutanixvolume 3m35s persistentvolume/pvc-094bda7b-6a4d-44f1-a275-288d73b5545a 3Gi RWO Delete Bound ndk-mysql/mysql-data-1-mysql-set-2 nutanixvolume 3m35s persistentvolume/pvc-5cc840da-2fc9-431f-8c99-9948fe8b70f1 5Gi RWO Delete Bound ndk-mysql/mysql-store-mysql-set-2 nutanixvolume 3m39s persistentvolume/pvc-928e91ad-87db-41b6-96bd-e12d86608cbf 5Gi RWO Delete Bound ndk-mysql/mysql-store-mysql-set-0 nutanixvolume 3m35s persistentvolume/pvc-9b9aba57-c449-419f-9a09-5e4960604321 3Gi RWO Delete Bound ndk-mysql/mysql-data-1-mysql-set-1 nutanixvolume 3m40s persistentvolume/pvc-dccfcf6d-c9fd-45c0-9a93-53d94e96fdf6 3Gi RWO Delete Bound ndk-mysql/mysql-data-1-mysql-set-0 nutanixvolume 3m40s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/mysql-data-1-mysql-set-0 Bound pvc-dccfcf6d-c9fd-45c0-9a93-53d94e96fdf6 3Gi RWO nutanixvolume 3m48s persistentvolumeclaim/mysql-data-1-mysql-set-1 Bound pvc-9b9aba57-c449-419f-9a09-5e4960604321 3Gi RWO nutanixvolume 3m48s persistentvolumeclaim/mysql-data-1-mysql-set-2 Bound pvc-094bda7b-6a4d-44f1-a275-288d73b5545a 3Gi RWO nutanixvolume 3m48s persistentvolumeclaim/mysql-store-mysql-set-0 Bound pvc-928e91ad-87db-41b6-96bd-e12d86608cbf 5Gi RWO nutanixvolume 3m48s persistentvolumeclaim/mysql-store-mysql-set-1 Bound pvc-06e3f4c4-c4f1-415f-8194-2e4d356570d0 5Gi RWO nutanixvolume 3m48s persistentvolumeclaim/mysql-store-mysql-set-2 Bound pvc-5cc840da-2fc9-431f-8c99-9948fe8b70f1 5Gi RWO nutanixvolume 3m48s Prism Central - Kubernetes Clusters\n","date":"2024-05-23T08:00:46+08:00","permalink":"https://wangken0129.github.io/p/nutanix_ndk-v1.0.0_on_ocp-4.13/","title":"Nutanix_NDK-v1.0.0_On_OCP-4.13"},{"content":"OCP4.12.45 on Nutanix - IPI Using IPI method install ocp 4.12.45 ( Stable )\n大致步驟\n建立Bastion 建立Nutanix Prism Central CA 建立Install-config.yaml 建立OCP Cluster Nutanix CSI Operator Worker node autoscale Reference Disk Resize:\nhttps://www.geekersdigest.com/how-to-extend-grow-linux-file-systems-without-downtime/#using-disk-partitions-vm\nCA:\nhttps://www.nutanix.dev/2023/09/26/generating-a-self-signed-san-certificate-to-use-red-hat-openshift-in-nutanix-marketplace/\nInstallation:\nhttps://www.youtube.com/watch?v=G8fFB6EUiOA\nhttps://docs.openshift.com/container-platform/4.12/installing/installing_nutanix/installing-restricted-networks-nutanix-installer-provisioned.html\nhttps://www.nutanix.dev/2022/08/16/red-hat-openshift-ipi-on-nutanix-cloud-platform/\nhttps://docs.openshift.com/container-platform/4.12/installing/disconnected_install/installing-mirroring-disconnected.html\nhttps://blog.csdn.net/weixin_43902588/article/details/107727804\nhttps://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.12.45/\nCluster Information Prism Central: IP + DNS FQDN\nPrism Element: IP + DNS FQDN\niSCSI Data Service: IP\nDNS Server: IP\nNTP: IP\nNetwork Subnet: Name VM Network IPAM: IP Range\nbastion: IP\nOCP Cluster Name: poc\nOCP Cluster Base Domain: Domain Name\nOCP API VIP: IP + DNS FQDN\nOCP Ingress VIP: IP + DNS FQDN\nCreate Bastion VM rhel9.3 從Prism建立rhel 9.3 VM，配置vCPU, Memory, Disk, Network，掛rhel 9.3 的 iso檔案並開機\n開機後點選Console, 設定基本的帳號密碼、網路、硬碟等\n安裝完成ssh 進入bastion\n下載ccoctl、openshift-install、oc client\n1 2 3 4 5 $ wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.12.45/ccoctl-linux-4.12.45.tar.gz $ wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.12.45/openshift-client-linux-4.12.45.tar.gz $ wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.12.45/openshift-install-linux-4.12.45.tar.gz 解壓縮下載的bin檔，並放到/usr/local/bin、建立ocp-install資料夾\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ tar xvf ccoctl-linux-4.12.45.tar.gz $ tar xvf openshift-client-linux-4.12.45.tar.gz $ tar xvf openshift-install-linux-4.12.45.tar.gz $ chmod 775 ccoctl $ sudo cp ccoctl oc kubectl openshift-install /usr/local/bin $ mkdir ocp-install $ cp openshift-install ocp-install/ $ oc version Client Version: 4.12.45 Kustomize Version: v4.5.7 下載pull-secret from registry.redhat.io\n1 {\u0026#34;auths\u0026#34;:{\u0026#34;cloud.openshift.com\u0026#34;:{\u0026#34;auth\u0026#34;:.....}}} Create SSH Key\n1 ssh-keygen Prism Central CA 產生CA 1 2 3 4 5 6 7 $ export PC_IP= PC_IP $ export PC_FQDN= PC_FQDN $ openssl req -x509 -nodes -days 3650 \\ -newkey rsa:2048 -keyout ${PC_IP}.key -out ${PC_IP}.crt \\ -subj \u0026#34;/C=US/ST=CA/L=San Jose/O=Nutanix Inc./OU=Manageability/CN=*.nutanix.local\u0026#34; \\ -addext \u0026#34;subjectAltName=IP:${PC_IP},DNS:${PC_FQDN}\u0026#34; 置換Prism Central CA\n加入CA至Bastion\n1 2 3 $ sudo cp xxx.crt /etc/pki/ca-trust/source/anchors $ sudo cp xxx.key /etc/pki/ca-trust/source/anchors $ sudo update-ca-trust extract Install OCP Requirements Storage Space: 800GB\n1 Cluster VIP for load balancer on control plane\n1 Ingress VIP for load balancer on worker node\nDNS record : vip.(cluster name).(base domain) *.apps.(cluster name).(base domain)\nIPAM with 7 ip addresses.\nVirtual machines:\n1 temporary bootstrap node 3 control plane nodes 3 compute machines 1 installation bastion machines SSH Key download oc , ccoctl, openshift-install, CoreOS image on web server or Nutanix Object Prism Central CA Create Install-config.yaml 利用openshift-install建立install-config.yaml檔案，依序輸入ssh key、Nutanix資訊、OCP Cluster資訊\n1 $ ./openshift-install create install-config --dir \u0026lt;installation_directory\u0026gt; 修改 install-config.yaml additionalTrustBundle: 加入 Prism Central 的 root CA\nadditionalTrustBundlePolicy: 改為 Always\nmachineNetwork: 改為與VM相同網段 xxx.xxx.xxx.xxx/24\nplatform 自定義cpu、memory、disk\n完成後如下，建議備份此檔案以利部署到其他地方或是重新部署\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 apiVersion: v1 baseDomain: XXX.XXX additionalTrustBundle: | -----BEGIN CERTIFICATE----- MIID8zCCAtugAwIBAgIUPE0LteDWc8JLjEm5LlqJF0s17n0wDQYJKoZIhvcNAQEL XXX -----END CERTIFICATE----- additionalTrustBundlePolicy: Always compute: - architecture: amd64 hyperthreading: Enabled name: worker platform: nutanix: cpus: 4 coresPerSocket: 2 memoryMiB: 16384 osDisk: diskSizeGiB: 120 replicas: 3 controlPlane: architecture: amd64 hyperthreading: Enabled name: master platform: nutanix: cpus: 4 coresPerSocket: 2 memoryMiB: 16384 osDisk: diskSizeGiB: 120 replicas: 3 credentialsMode: Manual metadata: creationTimestamp: null name: poc networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: xxx.xxx.xxx.xxx/24 networkType: OVNKubernetes serviceNetwork: - 172.30.0.0/16 platform: nutanix: apiVIPs: - xxx.xxx.xxx.xxx ingressVIPs: - xxx.xxx.xxx.xxx prismCentral: endpoint: address: xxx.xxx.xxx.xxx port: 9440 password: password username: admin prismElements: - endpoint: address: xxx.xxx.xxx.xxx port: 9440 uuid: xxxxxxx-cd36-0cc8-0f4a-xxxxxxxxxx subnetUUIDs: - xxxxxxxx-27ab-46ab-a0b1-xxxxxxxxxx3 publish: External pullSecret: \u0026#39;{\u0026#34;auths\u0026#34;:.....}}}\u0026#39; sshKey: | ssh-rsa AAAAxxxxxxxxxxxxxxxxxxxxxxx nutanix@ocp-bastion 設定CCO 安裝Cluster需要Cloud Credential Operator (CCO)，讓OCP能與Prism溝通\n大致作法為\n建立帳號密碼檔案\n建立要求帳密的目錄及檔案\n利用上述檔案建立OCP Shared Secret\n在 ocp-install 資料夾內建立 credentials.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 $ vim credentials.yaml credentials: - type: basic_auth data: prismCentral: username: admin password: P@ssw0rd123 prismElements: - name: AHV-Cluster username: admin password: P@ssw0rd123 設定Image環境變數\n1 $ RELEASE_IMAGE=$(./openshift-install version | awk \u0026#39;/release image/ {print $3}\u0026#39;) 建立credrequests資料夾\n1 $ mkdir credrequests 建立0000_30_machine-api-operator_00_credentials-request.yaml\n1 $ oc adm release extract --credentials-requests --cloud=nutanix --to=./credrequests $RELEASE_IMAGE 建立 CCO Secret\n1 $ ccoctl nutanix create-shared-secrets --credentials-requests-dir=/home/nutanix/ocp_install/credrequests --output-dir=/home/nutanix/ocp-install --credentials-source-filepath=/home/nutanix/ocp_install/credentials.yaml Create Cluster Create Manifests 建立 manifests資料夾，確認openshift-machine-api-nutanix-credentials-credentials.yaml有在裡面\n1 $ ./openshift-install create manifests --dir /home/nutanix/ocp-install/ Create Cluster ocp-install\n1 $ ./openshift-install create cluster --dir /home/nutanix/ocp-install/ --log-level=debug 安裝過程，會先建立OS image\ninstall completed\n安裝完後在安裝目錄底下會有auth的資料夾，裡面含有kubeadmin的帳密、kubeconfig檔案\n建議將kubeconfig檔案複製到~/.kube/config ，這樣就可以直接使用 oc 指令連線到該叢集\n1 2 3 4 in /home/nutanix/ocp-install $ mkdir ~/.kube $ cp auth/kubeconfig ~/.kube/config oc get nodes\nget console url\nlogin page，密碼在/home/nutanix/ocp-install/auth/\nlogin\n安裝完有遇到預設ntp無法連通，需改為內部的ntp server 修改MachineConfig方法為撰寫bu設定檔，再透過bu去轉成yaml檔案，最後部署上去OCP\nNutanix CSI 在OCP Console上點選OperatorHub，搜尋Nutanix\n點進去點選Install，描述內容比較重要的像支援Nutanix Volumes (rwo)、Nutanix FIles (rwx)\n以及下方有部署yaml範例，可以先複製下來\n選安裝版本、Namespace、更新方式\n安裝完成\n點選Nutanix CSI Storage，點Create NutanixCsiStorage\n建立NutanixCsiStorage，點選Create即可\n建立一個 Secret 讓OCP可以透過該Secret與Prism做溝通\n1 2 3 4 5 6 7 8 9 10 $ vi ntnx-secret apiVersion: v1 kind: Secret metadata: name: ntnx-secret namespace: openshift-cluster-csi-drivers stringData: # prism-element-ip:prism-port:admin:password key: PE-IP:9440:admin:P@ssw0rd123 建立Volume的Storage Class ，需修改dataServiceEndPoint、storageContainer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ vi nutanix-volume.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: nutanix-volume provisioner: csi.nutanix.com parameters: csi.storage.k8s.io/provisioner-secret-name: ntnx-secret csi.storage.k8s.io/provisioner-secret-namespace: openshift-cluster-csi-drivers csi.storage.k8s.io/node-publish-secret-name: ntnx-secret csi.storage.k8s.io/node-publish-secret-namespace: openshift-cluster-csi-drivers csi.storage.k8s.io/controller-expand-secret-name: ntnx-secret csi.storage.k8s.io/controller-expand-secret-namespace: openshift-cluster-csi-drivers csi.storage.k8s.io/fstype: ext4 dataServiceEndPoint: IP:3260 # Prism data service ip storageContainer: default-container # Prism 上建立的Storage Container storageType: NutanixVolumes #whitelistIPMode: ENABLED #chapAuth: ENABLED allowVolumeExpansion: true reclaimPolicy: Delete 建立Nutanix Files的Storage Class\n有分為兩種：NFS Servers、Dynamic NFS Shares (Valid Design)\nNFS Share Server：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ vi ntnx-files.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: nutanix-files annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;false\u0026#34; provisioner: csi.nutanix.com parameters: nfsServer: files.poc.xxx.xxx.xxx nfsPath: /ocp storageType: NutanixFiles squashType: \u0026#34;root-squash\u0026#34; # 若沒有資安疑慮可以用none，比較簡單 reclaimPolicy: Delete or Retain Dynamic NFS Shares\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ vi ntnx-dynamic-files.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: nutanix-dynfiles provisioner: csi.nutanix.com parameters: dynamicProv: ENABLED csi.storage.k8s.io/provisioner-secret-name: ntnx-secret csi.storage.k8s.io/provisioner-secret-namespace: openshift-cluster-csi-drivers csi.storage.k8s.io/node-publish-secret-name: ntnx-secret csi.storage.k8s.io/node-publish-secret-namespace: openshift-cluster-csi-drivers csi.storage.k8s.io/controller-expand-secret-name: ntnx-secret csi.storage.k8s.io/controller-expand-secret-namespace: openshift-cluster-csi-drivers storageType: NutanixFiles squashType: \u0026#34;root-squash\u0026#34; # 若沒有資安疑慮可以用none，比較簡單 nfsServerName: files # 對應Nutanix Files Server的名稱 allowVolumeExpansion: true 兩者建PVC在Nutanix Files Console上的差異如下：\n選擇第一種會建立在ocp這個share內，選擇第二種會建立各個PVC對應的share\n建立PVC去確認Status為Bound即可，Volume測試\n在Nutanix Prism上的Volume Group\n擴充PVC\n建立Files PVC測試\n檢視PV確認都為Bound\nAutoscaler Nutanix 結合Openshift的Autoscaler做到Worker Node的自動擴展\n名詞解釋：\nMachine：為透過Openshift部署的機器\nNodes：為部署完加入OCP Cluster內的節點\nscale up：對於Openshift 來說是整體的Resource增加，以實際看到的意思就是worker node新增，因為master node不能被使用\nscale down：相對scale up 就是整體的Resource減少，意思就是減少worker node的數量\nClusterautoscaler：針對整個Cluster做Resource管控的設定，例如整體CPU、Memory、整體最大節點等\nMachinesets：machine的設定值，包含有多種不同的環境例如 AWS、GCP、Nutanix，就會有多個Machinesets\nMachineautoscaler：針對Machinesets去做scale的設定，例如最大12個節點、最小2個節點等\nConfig 確認現有的Machinesets、Machine數量\n1 2 3 在Bastion上 $ oc get machinesets -n openshift-machine-api $ oc get machine -n openshift-machine-api 新增ClusterAutoscaler設定檔，這邊還可自定義Cluster的Resource資源總共多少、Woker Node使用量到多少時需要擴展 預設是單一Work Node 使用量達50%時會自動擴展，詳情見Redhat 官方連結 https://docs.openshift.com/container-platform/4.12/machine_management/applying-autoscaling.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ vi clusterautoscaler.yaml apiVersion: \u0026#34;autoscaling.openshift.io/v1\u0026#34; kind: \u0026#34;ClusterAutoscaler\u0026#34; metadata: name: \u0026#34;default\u0026#34; spec: resourceLimits: maxNodesTotal: 10 # 最大的worker node數量 scaleDown: # scaleDown 參數 enabled: true delayAfterAdd: 10s delayAfterDelete: 10s delayAfterFailure: 10s 新增MachineAutoscaler設定檔\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ vi machineautoscaler.yaml apiVersion: \u0026#34;autoscaling.openshift.io/v1beta1\u0026#34; kind: \u0026#34;MachineAutoscaler\u0026#34; metadata: name: \u0026#34;poc-gqwvb-worker\u0026#34; # 這裡請對應machineset的名稱 namespace: \u0026#34;openshift-machine-api\u0026#34; spec: minReplicas: 1 maxReplicas: 12 scaleTargetRef: apiVersion: machine.openshift.io/v1beta1 kind: MachineSet name: poc-gqwvb-worker # 這裡請對應machineset的名稱 部署兩個設定檔案到ocp cluster內，即可看到MachineAutoscalers內有該設定檔\n1 2 3 4 5 $ oc create -f clusterautoscaler.yaml clusterautoscaler.autoscaling.openshift.io/default created $ oc create -f machineautoscaler.yaml machineautoscaler.autoscaling.openshift.io/ocp-demo-t82dn-worker created Test 現有Machine及Nodes\nMachine\nNodes\n建立測試用的Project (namespace)\n1 $ oc adm new-project autoscale-example \u0026amp;\u0026amp; oc project autoscale-example 建立測試的Pod，50個pod、480秒後自動關閉\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ vi demo-job.yaml apiVersion: batch/v1 kind: Job metadata: generateName: demo-job- spec: template: spec: containers: - name: work image: busybox command: [\u0026#34;sleep\u0026#34;, \u0026#34;480\u0026#34;] resources: requests: memory: 1000Mi cpu: 1000m restartPolicy: Never backoffLimit: 4 completions: 50 parallelism: 50 部署到OCP Cluster內\n1 $ oc apply -f demo-job.yaml 當OCP的autoscaler偵測到時，即會自動開始新增worker node (scale up 約10分鐘) 可在Machines的頁面查看\nPrism Central上也可以看到VM在建立中\nMachinesets 會看到應該有的machines要有4個現在只有3個\nMachine Provision完成後，會自動加入OCP Cluster，自動安裝叢集所需的Pod、CSI Driver等\nNode加入完成\n當Pod的時間到了自動關閉之後，即會開始做縮減的動作 (scale down 約3分鐘完成 )\nOCP會自動選擇其中一個Worker Node做刪除，會排除該節點上有local PV、使用量低於50%的Worker Node\n最後回歸為原本的狀態\n以上為使用Nutanix 透過 IPI Online的方式安裝Red Hat Openshift 叢集的測試過程。\n","date":"2024-03-04T08:15:21+08:00","permalink":"https://wangken0129.github.io/p/ocp-4.12.45_on_nutanixipi/","title":"OCP-4.12.45_on_Nutanix(IPI)"},{"content":"Nutanix NKE (Karbon) 實作 前言 此測試主要目的為安裝NKE、測試CSI Driver、驗證服務及升級，\n並且能夠執行擴充、刪減、升級等操作，皆使用非離線的方式完成。\n版本、資訊 PC2023.3 預設NKE版本為v2.2.3，已先升級到v2.8.0，以測試更高的Kubernetes版本。\n參考連結：https://portal.nutanix.com/page/documents/details?targetId=Nutanix-Kubernetes-Engine-v2_8:Nutanix-Kubernetes-Engine-v2_8\nPort需求：\nhttps://portal.nutanix.com/page/documents/list?type=software\u0026filterKey=software\u0026filterVal=Ports%20and%20Protocols\u0026productType=Nutanix%20Kubernetes%20Engine\nCalico vs Flannel (Calico不支援Node在不同Vlan)：\nhttps://www.modb.pro/db/152417\nShell：https://blog.csdn.net/low5252/article/details/103646475\n版本：\nAOS: 6.5.3.6\nPrism Central: 2023.3\nNKE: 2.8.0\nKubernetes: 1.24.10\n架構：\n共會有八個VM：Control Plane *2 、etcd *3 、Worker Node *3\nNKE 運作架構\n安裝步驟 Check NKE Enabled Check NKE Version NKE: v2.7.0\nv2.7.0 UI出不來，故升級到v2.8.0\nDownload OS Image 下載的是ntnx-1.5\nCreate Cluster 配置IPAM\n到Kubernetes Management \u0026ndash;\u0026gt; Create Cluster \u0026ndash;\u0026gt; Production Cluster \u0026ndash;\u0026gt; Next\n輸入Cluster Name、版本等資訊，先安裝1.24.10\n選擇Node的子網，Additional可不選，Woker Node數量\nControl Plane可選擇External Loadbalance or Active-Passive，\n前者需建立好Loadbalance，可擴充性較高，\n後者只能使用兩個Control Plane，作為主要跟備援的Node，且要提供一個VIP在IPAM的管理範圍外面。\netcd的數量\n選擇Kubenetes網路 Calico or Flannel ，兩者比較在上面的連結，這裡選擇Flannel 其他預設\n選擇預設的Storage Class ，預設的會使用iSCSI Data Service的IP去連結Nutanix Cluster的Volume空間\n後續有需要NFS可以再加入，其他預設即可點選Create\n之後會開始自動安裝NKE Cluster\n順序大概是 Create etcd VM \u0026ndash;\u0026gt; Create etcd Cluster \u0026ndash;\u0026gt; Create Master VM \u0026ndash;\u0026gt; Create Worker VM \u0026ndash;\u0026gt; Create NKE Cluster\n會自動建立一些PVC給Cluster內部使用\n建立的VM 以及VIP已掛在Master Node上面\n約15分鐘Cluster就建立完成了\n回到Kubenetes Management 可看到NKE Cluster建立完成，Status: Healthy\n驗證 NKE 叢集 介面 Node Pools：Worker可以再加，etcd跟 Control Plane是不能再新增\nWoker Node\nControl Plane Node\netcd Node\nStorage Class為預設的 Nutanix Volume\nEnter Cluster SSH Access： 此方法可直接進入被鎖定的節點上面，下載.sh檔案並執行，裡面會內含Private Key\nDownload Kubeconfig： 下載Kubeconfig並設定環境變數，需要安裝kubectl\n此檔案可能會自動rotate certicifate，記得再重新下載即可\n安裝Kubectl ( Mac )\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 wangken@wangken-MAC ken-nke1 % curl -LO \u0026#34;https://dl.k8s.io/release/v1.24.10/bin/darwin/amd64/kubectl\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 138 100 138 0 0 518 0 --:--:-- --:--:-- --:--:-- 532 100 50.2M 100 50.2M 0 0 717k 0 0:01:11 0:01:11 --:--:-- 984k wangken@wangken-MAC ken-nke1 % ll total 104600 drwxr-xr-x 5 wangken staff 160 10 3 14:52 . drwxr-xr--+ 73 wangken staff 2336 10 3 14:30 .. -rw-r--r--@ 1 wangken staff 3497 10 3 14:28 ken-nke1-kubectl.cfg -rwxr-xr-x@ 1 wangken staff 4538 10 3 14:26 ken-nke1-ssh-access.sh -rw-r--r-- 1 wangken staff 52739872 10 3 14:45 kubectl wangken@wangken-MAC ken-nke1 % chmod +x kubectl wangken@wangken-MAC ken-nke1 % ./kubectl version WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;24\u0026#34;, GitVersion:\u0026#34;v1.24.10\u0026#34;, GitCommit:\u0026#34;5c1d2d4295f9b4eb12bfbf6429fdf989f2ca8a02\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2023-01-18T19:15:31Z\u0026#34;, GoVersion:\u0026#34;go1.19.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/amd64\u0026#34;} Kustomize Version: v4.5.4 The connection to the server localhost:8080 was refused - did you specify the right host or port? 可直接執行或是把kubectl 丟到 /usr/local/bin\n直接執行需要帶入 \u0026ndash;kubeconfig + 下載的config檔案\n1 2 3 4 5 6 7 wangken@wangken-MAC ken-nke1 % ./kubectl get nodes --kubeconfig ken-nke1-kubectl.cfg NAME STATUS ROLES AGE VERSION ken-nke1-906a07-master-0 Ready control-plane,master 55m v1.24.10 ken-nke1-906a07-master-1 Ready control-plane,master 54m v1.24.10 ken-nke1-906a07-worker-0 Ready node 51m v1.24.10 ken-nke1-906a07-worker-1 Ready node 51m v1.24.10 ken-nke1-906a07-worker-2 Ready node 51m v1.24.10 另一種就是把kubectl 放到預設指令執行的地方，可用echo $PATH去找\n再輸入export KUBECONFIG=下載的kubeconfig 存放的路徑即可直接執行kubectl get nodes等指令\n1 2 wangken@wangken-MAC ken-nke1 % echo $PATH /Library/Frameworks/Python.framework/Versions/3.11/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/Library/Apple/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin 因為我不想把kubectl放到系統裡面(日後可能還要更新)，所以我直接做一個執行檔案來連線此NKE叢集\n日後就透過nke-ctl這個執行檔來連線\n1 2 3 4 wangken@wangken-MAC ken-nke1 % cat nke-ctl #!/bin/bash ./kubectl --kubeconfig=./ken-nke1-kubectl.cfg $* 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 範例 wangken@wangken-MAC ken-nke1 % ./nke-ctl get nodes NAME STATUS ROLES AGE VERSION ken-nke1-906a07-master-0 Ready control-plane,master 79m v1.24.10 ken-nke1-906a07-master-1 Ready control-plane,master 78m v1.24.10 ken-nke1-906a07-worker-0 Ready node 75m v1.24.10 ken-nke1-906a07-worker-1 Ready node 75m v1.24.10 ken-nke1-906a07-worker-2 Ready node 75m v1.24.10 wangken@wangken-MAC ken-nke1 % ./nke-ctl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE default-storageclass (default) csi.nutanix.com Delete Immediate true 74m wangken@wangken-MAC ken-nke1 % ./nke-ctl get ns NAME STATUS AGE default Active 84m kube-node-lease Active 84m kube-public Active 84m kube-system Active 84m ntnx-system Active 79m wangken@wangken-MAC ken-nke1 % ./nke-ctl get pod -n ntnx-system NAME READY STATUS RESTARTS AGE alertmanager-main-0 2/2 Running 0 76m alertmanager-main-1 2/2 Running 0 76m alertmanager-main-2 2/2 Running 0 76m csi-snapshot-controller-5fc6f8f5c4-7bq96 1/1 Running 0 79m csi-snapshot-webhook-78957b8ddd-2c9n8 1/1 Running 0 79m fluent-bit-67vdl 1/1 Running 0 79m fluent-bit-6kxjz 1/1 Running 0 79m fluent-bit-fgnrx 1/1 Running 0 79m fluent-bit-gffxw 1/1 Running 0 79m fluent-bit-rs5n8 1/1 Running 0 79m kube-state-metrics-7bcb7f8cb6-tkjwf 3/3 Running 0 76m kubernetes-events-printer-8d4c49454-m9ss6 1/1 Running 0 79m node-exporter-9gtjl 2/2 Running 0 76m node-exporter-dl7k8 2/2 Running 0 76m node-exporter-g7jw7 2/2 Running 0 76m node-exporter-mv6xt 2/2 Running 0 76m node-exporter-nvk6w 2/2 Running 0 76m nutanix-csi-controller-864cc4767b-gl2sx 5/5 Running 0 79m nutanix-csi-node-8c2cc 3/3 Running 0 79m nutanix-csi-node-sf6wd 3/3 Running 0 79m nutanix-csi-node-shkkl 3/3 Running 0 79m prometheus-k8s-0 2/2 Running 0 76m prometheus-k8s-1 2/2 Running 0 76m prometheus-operator-59d64b7bc7-t8slv 2/2 Running 0 76m kube-system kube-system coredns-7b95cd4468-nv7jk 1/1 Running 0 2d1h kube-system coredns-7b95cd4468-xxwgf 1/1 Running 0 2d1h kube-system kube-apiserver-ken-nke1-906a07-master-0 3/3 Running 0 2d1h kube-system kube-apiserver-ken-nke1-906a07-master-1 3/3 Running 0 2d1h kube-system kube-flannel-ds-4r6j2 1/1 Running 0 2d1h kube-system kube-flannel-ds-dp5jv 1/1 Running 0 2d1h kube-system kube-flannel-ds-gqz5j 1/1 Running 0 2d1h kube-system kube-flannel-ds-slmqt 1/1 Running 0 2d1h kube-system kube-flannel-ds-z7hnh 1/1 Running 0 2d1h kube-system kube-proxy-ds-6x8c2 1/1 Running 0 2d1h kube-system kube-proxy-ds-892ch 1/1 Running 0 2d1h kube-system kube-proxy-ds-frrjt 1/1 Running 0 2d1h kube-system kube-proxy-ds-nkfrm 1/1 Running 0 2d1h kube-system kube-proxy-ds-pf9xm 1/1 Running 0 2d1h Nutanix Files 串接 Files 建立一個File Server，DNS設定 192.168.101.198 \u0026gt; ken-files.nutanixlab.local\n選擇一個Files VM就好\n設定Client Network\n設定gateway、IP等\n設定Storage Network\n選擇SMB、NFS Protocol，然後Create\n建立完成\n建立一個NFS Share : ken-nke\n選擇Authentication: System、Root Squash (後續因redmine需要修改資料夾權限為777，故改為none)\n建立完後複製Mount Path、Share Path\nCreate Storage Class 點回Kubenetes Management \u0026ndash;\u0026gt; Storage \u0026ndash;\u0026gt; Storage Class\n輸入 Endpoint、Export Path，\n建立完成\n確認Storageclass\n1 2 3 4 wangken@wangken-MAC ken-nke1 % ./nke-ctl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE default-storageclass (default) csi.nutanix.com Delete Immediate true 154m ntnx-files csi.nutanix.com Delete Immediate false 3m35s 建立測試pvc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 wangken@wangken-MAC ken-nke1 % cat test.pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-pv-claim labels: app: test spec: storageClassName: ntnx-files accessModes: - ReadWriteOnce resources: requests: storage: 20Gi wangken@wangken-MAC ken-nke1 % ./nke-ctl create -f test.pvc.yaml persistentvolumeclaim/test-pv-claim created 確認為bond狀態代表OK\n1 2 3 4 5 6 wangken@wangken-MAC ken-nke1 % ./nke-ctl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pv-claim Bound pvc-3f91422b-03d6-469e-940a-da87691be83a 20Gi RWO ntnx-files 7s wangken@wangken-MAC ken-nke1 % ./nke-ctl delete -f test.pvc.yaml persistentvolumeclaim \u0026#34;test-pv-claim\u0026#34; deleted 部署服務 之後有其他服務部署上去會再更新。\n部署redmine redmine是開源的專案管理軟體，部署時會需要使用PostgreSQL作為Database\n部署在redmine的namespace，共會有以下幾個檔案\n1 2 3 4 5 6 7 8 9 wangken@wangken-MAC ken-nke1 % ll -rw-r--r-- 1 wangken staff 244 10 3 17:30 pg-config.yaml -rw-r--r-- 1 wangken staff 1106 10 3 17:30 pg-deploy.yaml -rw-r--r-- 1 wangken staff 255 10 3 17:30 pg-pvc.yaml -rw-r--r-- 1 wangken staff 222 10 3 15:47 pg-svc.yaml -rw-r--r-- 1 wangken staff 1070 10 3 16:24 redmine-config.yaml -rw-r--r-- 1 wangken staff 2461 10 3 17:29 redmine-deploy.yaml -rw-r--r-- 1 wangken staff 264 10 5 10:05 redmine-svc.yaml -rw-r--r-- 1 wangken staff 218 10 3 16:40 test.pvc.yaml 建立Namespace\n1 2 wangken@wangken-MAC ken-nke1 % ./nke-ctl create ns redmine namespace/redmine created 建立PostgreSQL\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 wangken@wangken-MAC ken-nke1 % ./nke-ctl create -f pg-pvc.yaml -n redmine persistentvolumeclaim/redmine-postgres-pv-claim created wangken@wangken-MAC ken-nke1 % ./nke-ctl create -f pg-config.yaml -n redmine configmap/redmine-postgres-config created wangken@wangken-MAC ken-nke1 % ./nke-ctl create -f pg-deploy.yaml -n redmine deployment.apps/redmine-postgres-deployment created wangken@wangken-MAC ken-nke1 % ./nke-ctl create -f pg-svc.yaml -n redmine service/redmine-postgres-service created wangken@wangken-MAC ken-nke1 % ./nke-ctl get all -n redmine NAME READY STATUS RESTARTS AGE pod/redmine-postgres-deployment-7bd9859c6d-8c4b5 1/1 Running 0 67s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/redmine-postgres-service NodePort 172.19.78.32 \u0026lt;none\u0026gt; 5432:31432/TCP 60s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/redmine-postgres-deployment 1/1 1 1 67s NAME DESIRED CURRENT READY AGE replicaset.apps/redmine-postgres-deployment-7bd9859c6d 1 1 1 67s 驗證PostgreSQL，可以下載 pgAdmin連線確認\nhttps://www.pgadmin.org/download/pgadmin-4-macos/\n確認有redmine database存在\n建立redmine服務，service選用cluster vip，Port 8000\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 wangken@wangken-MAC ken-nke1 % ./nke-ctl apply -f redmine-pvc.yaml -n redmine persistentvolumeclaim/redmine-files created wangken@wangken-MAC ken-nke1 % ./nke-ctl apply -f redmine-config.yaml -n redmine configmap/redmine-config created wangken@wangken-MAC ken-nke1 % ./nke-ctl apply -f redmine-deploy.yaml -n redmine deployment.apps/redmine created wangken@wangken-MAC ken-nke1 % ./nke-ctl apply -f redmine-svc.yaml -n redmine service/redmine-service created wangken@wangken-MAC ken-nke1 % ./nke-ctl get pvc -n redmine NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE redmine-files Bound pvc-fa7253bf-da76-4048-a622-e6b159a94124 1Gi RWO ntnx-files 12s redmine-postgres-pv-claim Bound pvc-532dc4e2-876e-4ebf-b3c5-481226c0eb4f 20Gi RWO default-storageclass 24m wangken@wangken-MAC ken-nke1 % ./nke-ctl get all -n redmine NAME READY STATUS RESTARTS AGE pod/redmine-7fd78679f9-cwxg4 1/1 Running 0 6m31s pod/redmine-postgres-deployment-7bd9859c6d-fv5pb 1/1 Running 0 53m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/redmine-postgres-service NodePort 172.19.193.158 \u0026lt;none\u0026gt; 5432:31432/TCP 53m service/redmine-service ClusterIP 172.19.139.23 192.168.101.199 8000/TCP 2s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/redmine 1/1 1 1 6m31s deployment.apps/redmine-postgres-deployment 1/1 1 1 53m NAME DESIRED CURRENT READY AGE replicaset.apps/redmine-7fd78679f9 1 1 1 6m31s replicaset.apps/redmine-postgres-deployment-7bd9859c6d 1 1 1 53m 確認config file可以存在ntnx-files這個storageclass\n網頁連線redmine : http://192.168.101.199:8000 ，預設 admin/admin 再進去修改\n確認OK\n後續就可以透由redmine去做專案管理，甚至是結合CI/CD\nOptions \u0026amp; Add-Ons Infra Logging 說明：如果沒有Logging stack的話，可以啟用Infra logging (Elasticsearch and Kibana)來使用。\n1 2 3 4 Note: You can deploy the ELK-based NKE logging stack to provide a simplified experience for users with environments that do not have the logging stack implementation. In environments where the logging implementation exists or where there are no explicit requirements for logging, disable the logging stack to reduce resource consumption. The minimum uplink speed recommended is 30 Mb/s. 從Prism Central VM 登入Karbon\n1 2 3 4 5 6 7 nutanix@NTNX-172-16-90-75-A-PCVM:~$ karbon/karbonctl login --pc-username ken.wang@nutanixlab.local Please enter the password for the PC user: ken.wang@nutanixlab.local Login successful nutanix@NTNX-172-16-90-75-A-PCVM:~$ karbon/karbonctl cluster list Name UUID Control Plane IPs (VIP) Version OS Version Status Worker IPs ken-nke1 906a075d-e27b-47bd-59c3-b71701147650 192.168.101.213, 192.168.101.215 (192.168.101.199) 1.24.10-0 ntnx-1.5 kSuccess 192.168.101.208, 192.168.101.201, 192.168.101.207 enable infra logging\n1 2 nutanix@NTNX-172-16-90-75-A-PCVM:~$ karbon/karbonctl cluster infra-logging enable --cluster-name=ken-nke1 Successfully enabled infra logging: [POST /karbon/v1-alpha.1/k8s/clusters/{name}/enable-infra-logging][200] postEnableInfraLoggingO 回到NKE Cluster確認 pod status\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 wangken@wangken-MAC ken-nke1 % ./nke-ctl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-7b95cd4468-nv7jk 1/1 Running 0 2d2h kube-system coredns-7b95cd4468-xxwgf 1/1 Running 0 2d2h kube-system kube-apiserver-ken-nke1-906a07-master-0 3/3 Running 0 2d2h kube-system kube-apiserver-ken-nke1-906a07-master-1 3/3 Running 0 2d2h kube-system kube-flannel-ds-4r6j2 1/1 Running 0 2d2h kube-system kube-flannel-ds-dp5jv 1/1 Running 0 2d2h kube-system kube-flannel-ds-gqz5j 1/1 Running 0 2d2h kube-system kube-flannel-ds-slmqt 1/1 Running 0 2d2h kube-system kube-flannel-ds-z7hnh 1/1 Running 0 2d2h kube-system kube-proxy-ds-6x8c2 1/1 Running 0 2d2h kube-system kube-proxy-ds-892ch 1/1 Running 0 2d2h kube-system kube-proxy-ds-frrjt 1/1 Running 0 2d2h kube-system kube-proxy-ds-nkfrm 1/1 Running 0 2d2h kube-system kube-proxy-ds-pf9xm 1/1 Running 0 2d2h ntnx-system alertmanager-main-0 2/2 Running 0 2d2h ntnx-system alertmanager-main-1 2/2 Running 0 2d2h ntnx-system alertmanager-main-2 2/2 Running 0 2d2h ntnx-system csi-snapshot-controller-5fc6f8f5c4-7bq96 1/1 Running 0 2d2h ntnx-system csi-snapshot-webhook-78957b8ddd-2c9n8 1/1 Running 0 2d2h ntnx-system elasticsearch-logging-0 1/1 Running 0 2m45s ntnx-system fluent-bit-6cjst 1/1 Running 0 2m43s ntnx-system fluent-bit-74fw9 1/1 Running 0 2m42s ntnx-system fluent-bit-dn9qc 1/1 Running 0 2m43s ntnx-system fluent-bit-gwrsq 1/1 Running 0 2m43s ntnx-system fluent-bit-xdq2n 1/1 Running 0 2m43s ntnx-system kibana-logging-6ccd475856-cb4dd 1/2 Running 0 2m45s ntnx-system kube-state-metrics-7bcb7f8cb6-tkjwf 3/3 Running 0 2d2h ntnx-system kubernetes-events-printer-8d4c49454-m9ss6 1/1 Running 0 2d2h ntnx-system node-exporter-9gtjl 2/2 Running 0 2d2h ntnx-system node-exporter-dl7k8 2/2 Running 0 2d2h ntnx-system node-exporter-g7jw7 2/2 Running 0 2d2h ntnx-system node-exporter-mv6xt 2/2 Running 0 2d2h ntnx-system node-exporter-nvk6w 2/2 Running 0 2d2h ntnx-system nutanix-csi-controller-864cc4767b-gl2sx 5/5 Running 0 2d2h ntnx-system nutanix-csi-node-8c2cc 3/3 Running 0 2d2h ntnx-system nutanix-csi-node-sf6wd 3/3 Running 0 2d2h ntnx-system nutanix-csi-node-shkkl 3/3 Running 0 2d2h ntnx-system prometheus-k8s-0 2/2 Running 0 2d2h ntnx-system prometheus-k8s-1 2/2 Running 0 2d2h ntnx-system prometheus-operator-59d64b7bc7-t8slv 2/2 Running 0 2d2h redmine redmine-7fd78679f9-cwxg4 1/1 Running 0 75m redmine redmine-postgres-deployment-7bd9859c6d-fv5pb 1/1 Running 0 123m 多了以下幾個pod出現\n1 2 3 4 5 6 7 ntnx-system elasticsearch-logging-0 1/1 Running 0 2m45s ntnx-system fluent-bit-6cjst 1/1 Running 0 2m43s ntnx-system fluent-bit-74fw9 1/1 Running 0 2m42s ntnx-system fluent-bit-dn9qc 1/1 Running 0 2m43s ntnx-system fluent-bit-gwrsq 1/1 Running 0 2m43s ntnx-system fluent-bit-xdq2n 1/1 Running 0 2m43s ntnx-system kibana-logging-6ccd475856-cb4dd 1/2 Running 0 2m45s 從Prism 介面上去看多了Logging的選項\n點進去Logging介面，預設使用worker node的30950 port，點選Explore on my own\n點選左邊的選單\u0026ndash;\u0026gt; Analytics \u0026ndash;\u0026gt; Discover\n建立新的index pattern，名稱輸入*\n點回Discover確認有抓到資訊\n擴充節點 兩個方式，第一個是新增Node Pool，第二種是在現有的Node Pool 調整 Node的數量\nAdd Node Pool 原本的Node數量\n1 2 3 4 5 6 7 wangken@wangken-MAC ken-nke1 % ./nke-ctl get nodes NAME STATUS ROLES AGE VERSION ken-nke1-906a07-master-0 Ready control-plane,master 2d3h v1.24.10 ken-nke1-906a07-master-1 Ready control-plane,master 2d3h v1.24.10 ken-nke1-906a07-worker-0 Ready node 2d3h v1.24.10 ken-nke1-906a07-worker-1 Ready node 2d3h v1.24.10 ken-nke1-906a07-worker-2 Ready node 2d3h v1.24.10 新增後約3~5分鐘就完成建立，並帶有dev的前綴詞\n1 2 3 4 5 6 7 8 wangken@wangken-MAC ken-nke1 % ./nke-ctl get nodes NAME STATUS ROLES AGE VERSION ken-nke1-906a07-dev-worker-0 Ready node 54s v1.24.10 ken-nke1-906a07-master-0 Ready control-plane,master 2d3h v1.24.10 ken-nke1-906a07-master-1 Ready control-plane,master 2d3h v1.24.10 ken-nke1-906a07-worker-0 Ready node 2d3h v1.24.10 ken-nke1-906a07-worker-1 Ready node 2d3h v1.24.10 ken-nke1-906a07-worker-2 Ready node 2d3h v1.24.10 要刪除Node Pool 要先刪除那個Node\n約3~5分鐘後就刪除完成，再刪除Node Pool\n1 2 3 4 5 6 7 wangken@wangken-MAC ken-nke1 % ./nke-ctl get nodes NAME STATUS ROLES AGE VERSION ken-nke1-906a07-master-0 Ready control-plane,master 2d3h v1.24.10 ken-nke1-906a07-master-1 Ready control-plane,master 2d3h v1.24.10 ken-nke1-906a07-worker-0 Ready node 2d3h v1.24.10 ken-nke1-906a07-worker-1 Ready node 2d3h v1.24.10 ken-nke1-906a07-worker-2 Ready node 2d3h v1.24.10 Resize Node Pool 點選Node Pool \u0026ndash;\u0026gt; Actions \u0026ndash;\u0026gt; Resize ，從 3 調整到4\n約3~5分鐘就出現新的worker node 3\n1 2 3 4 5 6 7 8 wangken@wangken-MAC ken-nke1 % ./nke-ctl get nodes NAME STATUS ROLES AGE VERSION ken-nke1-906a07-master-0 Ready control-plane,master 2d3h v1.24.10 ken-nke1-906a07-master-1 Ready control-plane,master 2d3h v1.24.10 ken-nke1-906a07-worker-0 Ready node 2d3h v1.24.10 ken-nke1-906a07-worker-1 Ready node 2d3h v1.24.10 ken-nke1-906a07-worker-2 Ready node 2d3h v1.24.10 ken-nke1-906a07-worker-3 Ready node 54s v1.24.10 除了用Delete的方式移除worker-3之外，也可以用resize調整worker node數量\n驗證OK\n升級Kubernetes 理論上順序應為 LCM 升級NKE版本 \u0026ndash;\u0026gt; OS image 升級 \u0026ndash;\u0026gt; Kubernetes version 升級\n這裡NKE、OS image已經為最新版本，故只做Kubernetes version升級\n步驟 點進NKE Cluster \u0026ndash;\u0026gt; More \u0026ndash;\u0026gt; Upgrade Kubernetes\n點選要升級的版本 1.25.6-0 \u0026ndash;\u0026gt; Upgrade\n開始跑 Health ckeck\n現行版本\n會先進行master的升級，再來執行worker node滾動升級\n我的服務replica 只有設定1，所以服務會暫時中斷\nCluster內也會有服務重啟，Kubenetes 已升級完成，因為只做Kubernetes升級所以Kernel 跟Container-runtime都不會升級\n約30分鐘內即升級完畢\n因redmine服務需要先啟動DB再啟動redmine，所以將redmine的pod刪除，服務才會正常\n這個可以去設定啟動順序，但因測試使用，故沒特別設定\n確認服務正常\nAdvanced Kubernetes (TP) 進階的Kubernetes是Nutanix目前正在開發中的功能，啟用方式是會有一個管理用的NKE Cluster，然後去部署Agent到被管理的叢集\n運作模式有點像是Rancher的管理方式，都需要一個Management的Cluster，這樣好處是Management掛了不會去影響現有服務。\n新增Management Cluster 官方建議是要至少兩個worker node，先用一個來測試功能\n給予名稱、版本\n選擇網段、worker node數量\n選擇 Network Provider\n選擇Storage class，之後點選Create\n建立中\nNKE Cluster 建立完成\nssh 進入PCVM ， 針對ken-mgmt cluster啟用Advance Management\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 nutanix@NTNX-172-16-90-75-A-PCVM:~$ /home/nutanix/karbon/karbonctl login --pc-username ken.wang@nutanixlab.local Please enter the password for the PC user: ken.wang@nutanixlab.local Login successful nutanix@NTNX-172-16-90-75-A-PCVM:~$ /home/nutanix/karbon/karbonctl karbon-management enable --cluster-name ken-mgmt 2023-10-11T03:32:34.711Z client_wrapper.go:300: [INFO] Successfully created new resource: \u0026amp;{karbon-mgmt false 0xc0005340e0 0xc0008925e0 /v1, Kind=Namespace karbon-mgmt} 2023-10-11T03:32:34.784Z client_wrapper.go:317: [INFO] Successfully patched existing resource. NS: karbon-mgmt, Name: servicemonitors.monitoring.coreos.com 2023-10-11T03:32:34.805Z client_wrapper.go:300: [INFO] Successfully created new resource: \u0026amp;{karbon-mgmt true 0xc000e66ee0 0xc00103e1f0 networking.k8s.io/v1, Kind=NetworkPolicy traefik} 2023-10-11T03:32:34.832Z client_wrapper.go:300: [INFO] Successfully created new resource: \u0026amp;{karbon-mgmt true 0xc0001f7490 0xc000c5a300 /v1, Kind=ConfigMap traefik-dynamic-configmap} 2023-10-11T03:32:34.881Z client_wrapper.go:300: [INFO] Successfully created new resource: \u0026amp;{karbon-mgmt false 0xc00062b3b0 0xc000c5a428 apiextensions.k8s.io/v1, Kind=CustomResourceDefinition ingressroutes.traefik.containo.us} 2023-10-11T03:32:34.92Z client_wrapper.go:300: [INFO] Successfully created new resource: \u0026amp;{karbon-mgmt false 0xc0001dc7e0 0xc000c5a4b8 apiextensions.k8s.io/v1, Kind=CustomResourceDefinition ingressroutetcps.traefik.containo.us} 2023-10-11T03:32:34.999Z client_wrapper.go:300: [INFO] Successfully created ... Waiting on 172.16.90.75 (Up, ZeusLeader) to start: KarbonUI KarbonCore Waiting on 172.16.90.75 (Up, ZeusLeader) to start: KarbonUI KarbonCore Waiting on 172.16.90.75 (Up, ZeusLeader) to start: ... 2b7839f469 karbon-ui:v2.8.0 Up 38 seconds (healthy) 2ef4e4d10f karbon-core:v2.8.0 Up 35 seconds (healthy) 2b7839f469 karbon-ui:v2.8.0 Up 43 seconds (healthy) Successfully enabled karbon management! Deploy Agent 在ken-nke1 Cluster\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 nutanix@NTNX-172-16-90-75-A-PCVM:~$ /home/nutanix/karbon/karbonctl karbon-agent enable --cluster-name ken-nke1 --mgmt-name ken-mgmt 2023-10-11T03:38:30.244Z k8s_versioning.go:181: [ERROR] Failed to get Namespace for verification : namespaces \u0026#34;karbon-agent\u0026#34; not found 2023-10-11T03:38:30.28Z client_wrapper.go:300: [INFO] Successfully created new resource: \u0026amp;{ false 0xc001142850 0xc001136038 /v1, Kind=Namespace karbon-agent} 2023-10-11T03:38:30.302Z karbon_mgmt_sentry.go:169: [INFO] The Karbon-Mgmt pod is ready 2023-10-11T03:38:30.303Z karbon_mgmt_sentry.go:173: [ERROR] Empty payload return 2023-10-11T03:38:30.325Z karbon_mgmt_sentry.go:169: [INFO] The Karbon-Mgmt pod is ready 2023-10-11T03:38:31Z karbon_mgmt_sentry.go:169: [INFO] The Karbon-Mgmt pod is ready 2023-10-11T03:38:33.758Z agent_installer.go:187: [INFO] Edgemgmt image passed is: sherlock-edgemgmt:3274 2023-10-11T03:38:33.758Z agent_installer.go:194: [INFO] Controller image passed is: datastream-controller:2490 2023-10-10 20:38:33.947060 I | creating 3 resource(s) 2023-10-10 20:38:33.971481 I | Clearing discovery cache 2023-10-10 20:38:33.971530 I | beginning wait for 3 resources with timeout of 1m0s 2023-10-10 20:38:37.401949 I | creating 29 resource(s) 2023-10-10 20:38:37.635612 I | beginning wait for 29 resources with timeout of 15m0s 2023-10-10 20:38:37.803750 I | Deployment is not ready: karbon-agent/controller-deployment. 0 out of 1 expected pods are ready 2023-10-10 20:38:39.845704 I | Deployment is not ready: karbon-agent/controller-deployment. 0 out of 1 expected pods are ready ... 2023-10-10 20:38:53.847939 I | Deployment is not ready: karbon-agent/controller-deployment. 0 out of 1 expected pods are ready 2023-10-10 20:38:55.865197 I | Deployment is not ready: karbon-agent/edgemgmt. 0 out of 1 expected pods are ready 2023-10-10 20:38:57.851395 I | Deployment is not ready: karbon-agent/edgemgmt. 0 out of 1 expected pods are ready ... 2023-10-10 20:39:25.875878 I | release installed successfully: karbon-agent/servicedomain-2.5.0 2023-10-11T03:39:25.875Z helm_agent_installer.go:66: [INFO] Deployed Agent /home/nutanix/karbon/sherlock_edge_deployer_pc.tgz using helm chart 2023-10-11T03:39:25.875Z karbonagent_enable.go:182: [INFO] Successfully deployed karbon agent. Successfully Enabled the Karbon Agent 原本的 pod\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 wangken@wangken-MAC ken-nke1 % ./nke-ctl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-7b95cd4468-5zqhg 1/1 Running 0 42m kube-system coredns-7b95cd4468-qhszt 1/1 Running 0 109m kube-system kube-apiserver-ken-nke1-906a07-master-0 3/3 Running 2 (113m ago) 113m kube-system kube-apiserver-ken-nke1-906a07-master-1 3/3 Running 1 (113m ago) 113m kube-system kube-flannel-ds-dp5jv 1/1 Running 0 7d21h kube-system kube-flannel-ds-gqz5j 1/1 Running 0 7d21h kube-system kube-flannel-ds-slmqt 1/1 Running 0 7d21h kube-system kube-flannel-ds-z7hnh 1/1 Running 0 7d21h kube-system kube-proxy-ds-75tnd 1/1 Running 0 104m kube-system kube-proxy-ds-lqmzt 1/1 Running 0 104m kube-system kube-proxy-ds-qwzcd 1/1 Running 0 104m kube-system kube-proxy-ds-rcqfv 1/1 Running 0 105m ntnx-system alertmanager-main-0 2/2 Running 0 42m ntnx-system alertmanager-main-1 2/2 Running 1 (103m ago) 103m ntnx-system alertmanager-main-2 2/2 Running 1 (103m ago) 103m ntnx-system blackbox-exporter-7d494dffb6-5w2zm 3/3 Running 0 42m ntnx-system csi-snapshot-controller-5fc6f8f5c4-hvcsj 1/1 Running 0 109m ntnx-system csi-snapshot-webhook-78957b8ddd-pwggj 1/1 Running 0 109m ntnx-system elasticsearch-logging-0 1/1 Running 0 104m ntnx-system fluent-bit-6cjst 1/1 Running 0 5d18h ntnx-system fluent-bit-74fw9 1/1 Running 0 5d18h ntnx-system fluent-bit-dn9qc 1/1 Running 0 5d18h ntnx-system fluent-bit-gwrsq 1/1 Running 0 5d18h ntnx-system kibana-logging-6ccd475856-xgmjz 2/2 Running 1 (104m ago) 109m ntnx-system kube-state-metrics-758b544d66-ns667 3/3 Running 0 42m ntnx-system kubernetes-events-printer-8d4c49454-kjtkm 1/1 Running 0 42m ntnx-system node-exporter-9698d 2/2 Running 0 102m ntnx-system node-exporter-hhqzn 2/2 Running 0 103m ntnx-system node-exporter-wkg4z 2/2 Running 0 104m ntnx-system node-exporter-z5d8f 2/2 Running 0 102m ntnx-system nutanix-csi-controller-864cc4767b-mhftm 5/5 Running 0 109m ntnx-system nutanix-csi-node-8c2cc 3/3 Running 0 7d21h ntnx-system nutanix-csi-node-sf6wd 3/3 Running 0 7d21h ntnx-system prometheus-adapter-56755b575c-clhp2 1/1 Running 0 104m ntnx-system prometheus-adapter-56755b575c-fb979 1/1 Running 0 42m ntnx-system prometheus-k8s-0 2/2 Running 0 42m ntnx-system prometheus-k8s-1 2/2 Running 0 103m ntnx-system prometheus-operator-655cf4b548-tr7kv 2/2 Running 0 42m redmine redmine-7fd78679f9-7682g 1/1 Running 0 42m redmine redmine-postgres-deployment-7bd9859c6d-thrcv 1/1 Running 0 42m 部署Agent完成後的pod，新增了兩個Namespace: karbon-agent、project-ingress\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 wangken@wangken-MAC ken-nke1 % ./nke-ctl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE karbon-agent controller-deployment-788c9df5b-rg47k 1/1 Running 0 119s karbon-agent edgemgmt-c97765dc7-fhs6d 2/2 Running 0 119s karbon-agent nats-c9bd445bb-lzvdg 1/1 Running 0 72s kube-system coredns-7b95cd4468-5zqhg 1/1 Running 0 46m kube-system coredns-7b95cd4468-qhszt 1/1 Running 0 113m kube-system kube-apiserver-ken-nke1-906a07-master-0 3/3 Running 2 (117m ago) 117m kube-system kube-apiserver-ken-nke1-906a07-master-1 3/3 Running 1 (117m ago) 117m kube-system kube-flannel-ds-dp5jv 1/1 Running 0 7d21h kube-system kube-flannel-ds-gqz5j 1/1 Running 0 7d21h kube-system kube-flannel-ds-slmqt 1/1 Running 0 7d21h kube-system kube-flannel-ds-z7hnh 1/1 Running 0 7d21h kube-system kube-proxy-ds-75tnd 1/1 Running 0 108m kube-system kube-proxy-ds-lqmzt 1/1 Running 0 108m kube-system kube-proxy-ds-qwzcd 1/1 Running 0 108m kube-system kube-proxy-ds-rcqfv 1/1 Running 0 109m ntnx-system alertmanager-main-0 2/2 Running 0 46m ntnx-system alertmanager-main-1 2/2 Running 1 (107m ago) 108m ntnx-system alertmanager-main-2 2/2 Running 1 (107m ago) 108m ntnx-system blackbox-exporter-7d494dffb6-5w2zm 3/3 Running 0 46m ntnx-system csi-snapshot-controller-5fc6f8f5c4-hvcsj 1/1 Running 0 113m ntnx-system csi-snapshot-webhook-78957b8ddd-pwggj 1/1 Running 0 113m ntnx-system elasticsearch-logging-0 1/1 Running 0 109m ntnx-system fluent-bit-6cjst 1/1 Running 0 5d19h ntnx-system fluent-bit-74fw9 1/1 Running 0 5d19h ntnx-system fluent-bit-dn9qc 1/1 Running 0 5d19h ntnx-system fluent-bit-gwrsq 1/1 Running 0 5d19h ntnx-system kibana-logging-6ccd475856-xgmjz 2/2 Running 1 (108m ago) 113m ntnx-system kube-state-metrics-758b544d66-ns667 3/3 Running 0 46m ntnx-system kubernetes-events-printer-8d4c49454-kjtkm 1/1 Running 0 46m ntnx-system node-exporter-9698d 2/2 Running 0 106m ntnx-system node-exporter-hhqzn 2/2 Running 0 107m ntnx-system node-exporter-wkg4z 2/2 Running 0 108m ntnx-system node-exporter-z5d8f 2/2 Running 0 106m ntnx-system nutanix-csi-controller-864cc4767b-mhftm 5/5 Running 0 113m ntnx-system nutanix-csi-node-8c2cc 3/3 Running 0 7d21h ntnx-system nutanix-csi-node-sf6wd 3/3 Running 0 7d21h ntnx-system prometheus-adapter-56755b575c-clhp2 1/1 Running 0 108m ntnx-system prometheus-adapter-56755b575c-fb979 1/1 Running 0 46m ntnx-system prometheus-k8s-0 2/2 Running 0 46m ntnx-system prometheus-k8s-1 2/2 Running 0 108m ntnx-system prometheus-operator-655cf4b548-tr7kv 2/2 Running 0 46m project-ingress datastream-mqtt-ingress-54b8c67bdd-d7kbl 1/1 Running 0 56s project-ingress datastream-rtsp-ingress-59c47c8f7c-pgwpc 1/1 Running 0 56s project-ingress nats-8bc8494cd-vxs4b 1/1 Running 0 72s redmine redmine-7fd78679f9-7682g 1/1 Running 0 46m redmine redmine-postgres-deployment-7bd9859c6d-thrcv 1/1 Running 0 46m 完成前後對照 原本的畫面\n因爲測試使用，故把Worker Node數量降到2\n完成後多了一個Namespace的選項\n權限部分應該是還沒做好，登出後用local admin帳號登入就看得到namespace內容\n然後ken-mgmt的worker node數量調整為2後，就可以看到namespace裡面的Deployment跟其他資訊\nWorkloads\nWorkloads : Deployments\nDeployment詳細資訊\nDeployment詳細資訊，Pod名稱跟Yaml格式\nConfig有 ConfigMaps跟Secrets\nNetwork裡面的Service，看是走Nodeport or ClusterIP or Ingress\nEndpoints\nPVC\n點進去都可以看到Yaml格式，就跟使用kubectl edit 會出現的yaml是一樣的，但這邊無法編輯\nCharts還不確定會有什麼可以呈現\n總結 NKE 的部署很簡單、快速，透過CSI Driver 可以結合Nutanix本身的檔案系統 ( Volume / Files ) 來做儲存\n其實是很適合現有使用Nutanix ，但又同時想要擁有Kuberntes來做開發測試，或是高階玩家想省授權費用的人，\n因為NKE 本身的監控、Log沒有像Redhat Openshift 有很多預設做好的介面跟Operator，\n所以若是正式環境會需要自己部署ELK、Prometheus、Service Mesh等軟體。\n","date":"2023-10-11T05:34:51+08:00","permalink":"https://wangken0129.github.io/p/nutanix-nke-v2.8/","title":"Nutanix-NKE-v2.8"},{"content":"製作Blog Part3 工欲善其事，必先利其器\n我依照我目前的使用情境寫了三個Shell Script，\n來達到半自動上傳筆記到Github的效果，\n為何是半自動呢? 因為寫筆記時覺得還要特別加上Hugo的Front Matter有點麻煩，\n所以用後製的方式來撰寫Front Matter，而Script執行過程會有交互式的問答，\n如此一來就可以順便填入Date、Tags、Category等參數，\n而有時候也會有只執行其中一個腳本的時候，所以分了三個來寫。\n半自動上傳筆記到Github 資料夾結構 我的資料夾會依照不同的廠牌or產品來分類，\n所以每個廠牌資料夾內都會有個Notes的資料夾，\n目前總共分為SUSE、Redhat、Nutanix等三個資料夾，\n例如：\n1 2 3 4 5 6 7 wangken@wangken-MAC Desktop % tree -L 1 RedHat RedHat ├── Certification ├── Courses ├── Document ├── Notes ├── iso 再來會有個工作目錄是Pull下來Repository的上層資料夾，\n會在上面執行腳本。\n工作目錄：\n1 2 3 4 5 6 7 8 wangken@wangken-MAC myblog % pwd /Users/wangken/myblog wangken@wangken-MAC myblog % ls -l total 32 -rwxr-xr-x@ 1 wangken staff 4342 9 23 17:35 cp_to_blog.sh -rwxr-xr-x@ 1 wangken staff 499 9 23 17:44 deploy-blog.sh -rwxr-xr-x 1 wangken staff 457 9 19 15:20 gitpush.sh drwxr-xr-x 16 wangken staff 512 9 21 10:26 hugo-blog Script 1 複製到工作目錄 這個稍微有點戎長，但也是讓我在腳本執行時看得比較清楚一點\n主要內容為：\n定義廠牌筆記的資料夾路徑 選擇廠牌的資料夾為何 尋找該資料夾內的.md檔案 利用該檔案的名稱建立文章的資料夾在工作目錄上 輸入Front Matter的參數 ( Category、Tags ) 建立含Front Matter的預設index.md在文章的資料夾內 將筆記的內容貼到index.md的下方 cp_to_blog.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 #!/bin/bash # If a command fails then the deploy stops set -e # Define Path blog_path=\u0026#39;/Users/wangken/myblog/hugo-blog/content/post/\u0026#39; ntnx_path=\u0026#39;/Users/wangken/Desktop/Nutanix/Notes\u0026#39; redhat_path=\u0026#39;/Users/wangken/Desktop/RedHat/Notes/*\u0026#39; suse_path=\u0026#39;/Users/wangken/Desktop/SUSE/Notes\u0026#39; # Enter product to identify notes path echo \u0026#34;Choose product to identify notes path\u0026#34; pdtype=(\u0026#34;suse\u0026#34; \u0026#34;redhat\u0026#34; \u0026#34;nutanix\u0026#34; \u0026#34;Quit\u0026#34;) select type in \u0026#34;${pdtype[@]}\u0026#34;; do case $type in \u0026#34;suse\u0026#34;) pd=\u0026#39;suse\u0026#39; break ;; \u0026#34;redhat\u0026#34;) pd=\u0026#39;redhat\u0026#39; break ;; \u0026#34;nutanix\u0026#34;) pd=\u0026#39;nutanix\u0026#39; break ;; \u0026#34;Quit\u0026#34;) echo \u0026#34;end selection\u0026#34; exit ;; *) echo \u0026#34;invalid option $REPLY\u0026#34;;; esac done echo \u0026#34;choosing $pd to use \u0026#34; echo \u0026#34;--------------------\u0026#34; echo \u0026#34;--------------------\u0026#34; sleep 0.5 # Check if name input with error # And process the next step if [[ $pd != \u0026#39;suse\u0026#39; \u0026amp;\u0026amp; $pd != \u0026#39;rhel\u0026#39; \u0026amp;\u0026amp; $pd != \u0026#39;nutanix\u0026#39; \u0026amp;\u0026amp; $pd == \u0026#39;\u0026#39; ]];then echo \u0026#34;Please check again \u0026#34; elif [[ $pd == \u0026#39;suse\u0026#39; \u0026amp;\u0026amp; $pd != \u0026#34;\u0026#34; ]];then echo \u0026#34;Check OK\u0026#34; echo \u0026#34;--------------------\u0026#34; echo \u0026#34;--------------------\u0026#34; echo \u0026#34;Notes directory is $suse_path\u0026#34; sleep 0.5 echo \u0026#34;--------------------\u0026#34; echo \u0026#34;--------------------\u0026#34; echo \u0026#34;Select md file to blog\u0026#34; sleep 0.5 #Select notes path .md file to blog files=($suse_path/*.md) elif [[ $pd == \u0026#39;redhat\u0026#39; \u0026amp;\u0026amp; $pd != \u0026#34;\u0026#34; ]];then echo \u0026#34;Check OK\u0026#34; echo \u0026#34;--------------------\u0026#34; echo \u0026#34;--------------------\u0026#34; echo \u0026#34;Notes directory is $redhat_path\u0026#34; sleep 0.5 echo \u0026#34;--------------------\u0026#34; echo \u0026#34;--------------------\u0026#34; echo \u0026#34;Select md file to blog\u0026#34; sleep 0.5 #Select notes path .md file to blog files=($redhat_path/*.md) elif [[ $pd == \u0026#39;nutanix\u0026#39; \u0026amp;\u0026amp; $pd != \u0026#34;\u0026#34; ]];then echo \u0026#34;Check OK\u0026#34; echo \u0026#34;--------------------\u0026#34; echo \u0026#34;--------------------\u0026#34; echo \u0026#34;Notes directory is $ntnx_path\u0026#34; sleep 0.5 echo \u0026#34;--------------------\u0026#34; echo \u0026#34;--------------------\u0026#34; echo \u0026#34;Select md file to blog\u0026#34; sleep 0.5 #Select notes path .md file to blog files=($ntnx_path/*.md) # End loop fi PS3=\u0026#39;Select file to blog, or 0 to exit: \u0026#39; select file in \u0026#34;${files[@]}\u0026#34;; do if [[ $REPLY == \u0026#34;0\u0026#34; ]]; then echo \u0026#39;Bye!\u0026#39; \u0026gt;\u0026amp;2 exit elif [[ -z $file ]]; then echo \u0026#39;Invalid choice, try again\u0026#39; \u0026gt;\u0026amp;2 else break fi done # Using file name to path_name path_name=$(basename $file .md) echo \u0026#34;Choosing $path_name\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34; \u0026#34; # Create blog post directory echo \u0026#34;Create blog post directory using $path_name\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34; \u0026#34; mkdir -p \u0026#34;$blog_path/$path_name\u0026#34; \u0026amp;\u0026amp; post_path=$_ echo \u0026#34;$post_path has been created\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34; \u0026#34; echo \u0026#34;Input Front Matter Attribute\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34; \u0026#34; sleep 0.5 # Front Matter attribute # date now=$(date -u +\u0026#34;%Y-%m-%dT%T+08:00\u0026#34;) # category categories=(\u0026#34;Lab Category\u0026#34; \u0026#34;Knowledge Base Category\u0026#34; \u0026#34;Quit\u0026#34;) select type in \u0026#34;${categories[@]}\u0026#34;; do case $type in \u0026#34;Lab Category\u0026#34;) category=\u0026#39;Lab Category\u0026#39; break ;; \u0026#34;Knowledge Base Category\u0026#34;) category=\u0026#39;Knowledge Base Category\u0026#39; break ;; \u0026#34;Quit\u0026#34;) echo \u0026#34;end selection\u0026#34; exit ;; *) echo \u0026#34;invalid option $REPLY\u0026#34;;; esac done echo \u0026#34;choosing $category\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34; \u0026#34; # tag1 read -p \u0026#34;Input main tag: \u0026#34; tag echo \u0026#34;-------------------\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34; \u0026#34; # tag2 read -p \u0026#34;Input second tag: \u0026#34; tag2 echo \u0026#34;-------------------\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34; \u0026#34; # tag3 read -p \u0026#34;Input third tag: \u0026#34; tag3 echo \u0026#34;-------------------\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34; \u0026#34; # Generate default index.md echo \u0026#34;Generate Default Front Matter In $post_path\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34;-------------------\u0026#34; sleep 0.5 cat \u0026gt; $post_path/index.md \u0026lt;\u0026lt;EOF --- title: $path_name description: $path_name slug: $path_name date: $now categories: - $category tags: - $tag - $tag2 - $tag3 weight: 1 # You can add weight to some posts to override the default sorting (date descending) --- EOF echo \u0026#34; \u0026#34; echo \u0026#34;Copy $file to $post_path\u0026#34; cat $file \u0026gt;\u0026gt; $post_path/index.md echo \u0026#34; \u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34;-------------------\u0026#34; echo \u0026#34;End of the create index.md\u0026#34; echo \u0026#34; \u0026#34; sleep 1 Script 2 上傳到Github 主要內容為：\n進入工作目錄 新增改變的內容到git ( git add ) 確認改變的項目並加入訊息 ( git commit ) 推送到Github的Master branch ( git push ) gitpush.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/sh # If a command fails then the deploy stops set -e printf \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\u0026#34; # Build the project. # hugo # if using a theme, replace with `hugo -t \u0026lt;YOURTHEME\u0026gt;` # Go To Public folder cd hugo-blog # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site $(date)\u0026#34; if [ -n \u0026#34;$*\u0026#34; ]; then msg=\u0026#34;$*\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master # come back zero cd .. Script 3 結合Script 1\u0026amp;2 主要內容為：\n定義前面兩個Script作為變數 詢問是否要複製筆記 ( 執行Script 1 ) 若回答y (yes)，會再問一次，回答n (no)，則進行下一步 詢問是否執行推送到Github上 ( 執行Script 2 ) deploy-blog.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash #Define Script cp=/Users/wangken/myblog/cp_to_blog.sh push=/Users/wangken/myblog/gitpush.sh # Ask for copy or not while true; do read -p \u0026#34;Do you want to copy md file? (y/n) \u0026#34; yn1 case $yn1 in [Yy]* ) $cp;; [Nn]* ) break;; * ) echo \u0026#34;Please answer yes or no.\u0026#34;;; esac done # Ask for Push or not while true; do read -p \u0026#34;Do you want to push to github? (y/n) \u0026#34; yn2 case $yn2 in [Yy]* ) $push; break;; [Nn]* ) break;; * ) echo \u0026#34;Please answer yes or no.\u0026#34;;; esac done 執行結果截圖 執行deply-blog.sh ，詢問廠牌後會用前面定義好的筆記資料夾來搜尋筆記\n填寫參數後選擇自動上傳到github (y)\n確認 index.md 前面已自動加入了Front Matter\n以上就是我半自動化讓筆記上傳到Github的過程，再加上原先Hugo Stack的範本，\n讓部落格網站可以自動更新文章，如有其他建議歡迎不吝指教，謝謝。\n","date":"2023-09-23T15:47:05+08:00","permalink":"https://wangken0129.github.io/p/blog-part3/","title":"製作Blog Part3 (半自動腳本)"},{"content":"筆記區 純粹筆記，隨時更新。\n查詢interface status 登入一台CVM\n下指令查詢 \u0026ldquo;allssh manage_ovs show_interfaces\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 nutanix@NTNX-18FM6J330088-B-CVM:172.16.90.66:~$ allssh manage_ovs show_interfaces ================== 172.16.90.65 ================= name mode link speed eth0 10000 True 1000 eth1 10000 True 1000 eth2 10000 True 10000 eth3 10000 True 10000 ================== 172.16.90.67 ================= name mode link speed eth0 10000 True 1000 eth1 10000 True 1000 eth2 10000 True 10000 eth3 10000 True 10000 ================== 172.16.90.66 ================= name mode link speed eth0 10000 True 1000 eth1 10000 True 1000 eth2 10000 True 10000 eth3 10000 True 10000 確認Cluster Status 1 2 3 4 5 6 7 8 登入CVM 1. 查看node投票機制 $nodetool -h 0 ring 2. 查看叢集狀態 $cs | grep -v UP \u0026gt;\u0026gt;\u0026gt;\u0026gt; 有down的會show出來 $cluster status 3. 確認PE上的ata Resiliency Status or $ncli cluster get-domain-fault-tolerance-status type=node 關機流程 (換記憶體) 1 2 3 4 5 6 登入CVM 1. 查看叢集狀態 $cs |grep -v UP $nodetool -h 0 ring 2. 關閉CVM $cvm_shutdown -P now Move Windows 2003 步驟 1 2 3 4 5 6 7 8 9 10 https://portal.nutanix.com/page/documents/kbs/details?targetId=kA00e000000Cr6CCAS 1. MergeIDE.bat 2. MOVE vm data (uncheck preparation, check only data seed) 3. poweroff vm 4. acli change scsi to ide , delete scsi.0 5. boot to windows 6. install virtIO (NetKVM、Balloon、VIOSCSI) 7. acli change hdd bus to pci sata 8. boot again 將MergeIDE.bat的資料夾上傳到Windows 2003,並執行MergeIDE.bat 開啟Move將vCenter上的VM移轉到Nutanix Cluster\n勾選Bypass Guest Operations\n關閉移轉過後的VM\nssh 登入Cluster內其中一台CVM\n1 2 3 4 5 6 7 8 9 10 執行指令取得scsi.0的硬碟uuid $acli vm.get VM_NAME $acli vm.get VM_win2003 ------------scsi to IDE ---------------- ＄acli vm.disk_create VM_NAME clone_from_vmdisk=uuid bus=ide ------------delete scsi.0 ---------------- $acli vm.disk_delete VM_NAME disk_addr=scsi.0 ------------vm.get VM_NAME---------------- 開啟VM,執行Device Clenup\n掛載ISO檔,把檔案複製到C槽再安裝Driver\n1 2 3 4 5 6 7 8 9 ## 開啟裝置管理員新增以下路徑的Driver Ballon Controller、PCI Device 1. Windows_2003_Tools/virtio-win-0.1-81/WLH/x86 console滑鼠、SCSI Passthrough Controller 2. Windows_2003_Tools/virtio-win-0.1-81/WLH/x86 網卡 3. Windows_2003_Tools/virtio-win-0.1.189/NetKVM/2k3/x86 USB Device、PCI Device 4. Windows_2003_Tools/virtio-win-0.1.189/Balloon/2k3/x86 調整ide to pci or scsi (prefer)\n1 2 3 4 5 6 7 8 9 10 11 執行指令取得ide.0的硬碟uuid $acli vm.get VM_NAME $acli vm.get VM_win2003 ------------ide to pci ---------------- ＄acli vm.disk_create VM_NAME clone_from_vmdisk=uuid bus=pci ------------delete ide.0 ---------------- $acli vm.disk_delete VM_NAME disk_addr=ide.0 ------------vm.get VM_NAME---------------- $acli vm.get VM_NAME boot win2003\n把Disk Clone到image service https://next.nutanix.com/ncm-intelligent-operations-formerly-prism-pro-ultimate-26/ahv-vdisk-part-3-creating-an-image-of-or-from-an-existing-vdisk-33686\nEKS on Nutanix https://rural-gazelle-46d.notion.site/EKS-Anywhere-On-Nutanix-e6e7764e3daa4267a12cefe6c7a4b7e9\nLAB Welcome Banner 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026lt;h1 style=\u0026#34;text-align:center; color:black;\u0026#34;\u0026gt;NetFos Nutanix Lab公告\u0026lt;/h1\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;p style=\u0026#34;text-align:justify; color:blue;\u0026#34;\u0026gt;1. 請將您建的虛擬機加上前輟_VM名稱，如Arthur_Jump，方便管理員管理。\u0026lt;/p\u0026gt; \u0026lt;p style=\u0026#34;text-align:justify; color:red;\u0026#34;\u0026gt;2. 2023-02-06調整網路架構，以能配置VPC為目標。\u0026lt;/p\u0026gt; \u0026lt;p style=\u0026#34;text-align:justify; color:black;\u0026#34;\u0026gt; Netfos Nutanix Team 敬上，更新於2023-02-07\u0026lt;/p\u0026gt; \u0026lt;img src=\u0026#34;/console/nutanix_welcome.jpg\u0026#34; style=\u0026#34;width:400px:height:200px\u0026#34;\u0026gt; --- /home/apache/www/console nutanix@NTNX-13SM35330024-B-CVM:172.16.90.62:/home/apache/www/console$ ll total 128 -rwxr-x---. 1 nutanix apache 4800 Mar 2 12:55 access_denied.html drwxr-x---. 2 nutanix apache 4096 Mar 2 12:55 api drwxr-x---. 4 nutanix apache 4096 Mar 2 12:55 api-explorer drwxr-x---. 7 nutanix apache 4096 Mar 2 12:55 app drwxr-x---. 2 nutanix apache 4096 Mar 2 12:55 dev-mode drwxr-x---. 2 nutanix apache 4096 Mar 2 12:55 downloads -rwxr-x---. 1 nutanix apache 2532 Mar 2 12:55 index.html -rwxr-x---. 1 nutanix apache 2532 Mar 2 12:55 index_qa.html drwxr-x---. 41 nutanix apache 4096 Mar 2 12:55 lib drwxr-x---. 5 nutanix apache 4096 Mar 2 12:55 minerva -rwxr-x---. 1 nutanix apache 18753 Mar 2 12:55 not_found.html drwxr-x---. 2 nutanix apache 4096 Mar 2 12:55 nutanixapi -rw-------. 1 nutanix nutanix 48351 Mar 2 16:45 nutanix_welcome.jpg drwxr-x---. 3 nutanix apache 4096 Mar 2 12:55 prism drwxr-x---. 2 nutanix apache 4096 Mar 2 12:55 v2api drwxr-x---. 2 nutanix apache 4096 Mar 2 12:55 vnc wangken@wangken-MAC ~ % scp -O nutanix@172.16.90.72:/home/apache/www/console/nutanix_welcome.jpg /Users/wangken/Desktop/Nutanix/Notes Prism Central VM nutanix@172.16.90.72\u0026#39;s password: nutanix_welcome.jpg 擴充Node Error 新的Node檢查Disk型號\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 nutanix@NTNX-23SG3G330009-A-CVM:192.168.208.43:~$ list_disks ERROR:root:Could not reach zookeeper ERROR:root:Unable to connect to zookeeper session Slot Disk Model Serial Size ---- -------- ---------------- -------------- ------ 1 /dev/sda MZILG7T6HBLA/A07 S70KNE0W500857 7.7 TB 2 /dev/sdb MZILG7T6HBLA/A07 S70KNE0W500855 7.7 TB nutanix@NTNX-23SG3G330009-A-CVM:192.168.208.43:~$ grep -A6 -B12 MZILG7T6HBLA/A07 /etc/nutanix/hcl.json { \u0026#34;approved_by\u0026#34;: \u0026#34;Nutanix\u0026#34;, \u0026#34;blacklisted_firmware\u0026#34;: [], \u0026#34;boot\u0026#34;: true, \u0026#34;capacity_gb\u0026#34;: 7681.51, \u0026#34;data\u0026#34;: true, \u0026#34;diagnostics_test\u0026#34;: 1, \u0026#34;endurance\u0026#34;: 1, \u0026#34;include_in_hcl\u0026#34;: true, \u0026#34;interface\u0026#34;: \u0026#34;SAS\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;Samsung\u0026#34;, \u0026#34;metadata\u0026#34;: true, \u0026#34;model\u0026#34;: \u0026#34;MZILG7T6HBLA/A07\u0026#34;, \u0026#34;model_string\u0026#34;: \u0026#34;SAMSUNG PM1653 SAS 7.6TB RI\u0026#34;, \u0026#34;nand_type\u0026#34;: \u0026#34;TLC\u0026#34;, \u0026#34;recommended_firmware\u0026#34;: [], \u0026#34;trim_enabled\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;SSD\u0026#34;, \u0026#34;uuid\u0026#34;: \u0026#34;772da35d-4621-4f8c-80bb-2f1a26648be0\u0026#34; 原來的Cluster內檢查/etc/nutanix/hcl.json 是否有此型號\n如果沒有，下載最新的Foundation Platform，並解壓縮 裡面有hcl.json，上傳至CVM並傳到每個Node CVM\n重啟genesis 後即可再次Expand Cluster\n","date":"2023-09-23T08:25:49+08:00","permalink":"https://wangken0129.github.io/p/nutanix-notes/","title":"Nutanix-Notes"},{"content":"Install Openshift 4.11.9 on vSphere(UPI) 說明 https://docs.openshift.com/container-platform/4.11/installing/installing_vsphere/installing-vsphere.html https://cloud.redhat.com/blog/deploying-openshift-4.4-to-vmware-vsphere-7?hsLang=en-us\n用Mac 當作Client 需要事先安裝好Bastion Bastion安裝設定Named、HAProxy、Web Server (這裡用Nginx) 用rhcos-4.11.9.ova 來ESXi 部署ovf 範本 下載OC Client https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.11.9/openshift-client-mac-4.11.9.tar.gz\n下載OCP Installer https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.11.9/openshift-install-mac-4.11.9.tar.gz\n下載CoreOS-4.11.9.ova https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.11/4.11.9/rhcos-4.11.9-x86_64-vmware.x86_64.ova\n下載Pull-Secret https://console.redhat.com/openshift/downloads#tool-pull-secret\nNamed 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 Named 設定檔 [root@bastion ~]# cat /etc/named.conf // // named.conf // // Provided by Red Hat bind package to configure the ISC BIND named(8) DNS // server as a caching only nameserver (as a localhost DNS resolver only). // // See /usr/share/doc/bind*/sample/ for example named configuration files. // options { listen-on port 53 { any; }; #\tlisten-on-v6 port 53 { ::1; }; directory \u0026#34;/var/named\u0026#34;; dump-file \u0026#34;/var/named/data/cache_dump.db\u0026#34;; statistics-file \u0026#34;/var/named/data/named_stats.txt\u0026#34;; memstatistics-file \u0026#34;/var/named/data/named_mem_stats.txt\u0026#34;; secroots-file\t\u0026#34;/var/named/data/named.secroots\u0026#34;; recursing-file\t\u0026#34;/var/named/data/named.recursing\u0026#34;; allow-query { any; }; ## 設定轉寄站 forwarders { 192.168.2.12; 192.168.2.11; }; /* - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion. - If you are building a RECURSIVE (caching) DNS server, you need to enable recursion. - If your recursive DNS server has a public IP address, you MUST enable access control to limit queries to your legitimate users. Failing to do so will cause your server to become part of large scale DNS amplification attacks. Implementing BCP38 within your network would greatly reduce such attack surface */ recursion yes; dnssec-enable yes; dnssec-validation yes; managed-keys-directory \u0026#34;/var/named/dynamic\u0026#34;; pid-file \u0026#34;/run/named/named.pid\u0026#34;; session-keyfile \u0026#34;/run/named/session.key\u0026#34;; /* https://fedoraproject.org/wiki/Changes/CryptoPolicy */ include \u0026#34;/etc/crypto-policies/back-ends/bind.config\u0026#34;; }; logging { channel default_debug { file \u0026#34;data/named.run\u0026#34;; severity dynamic; }; }; zone \u0026#34;.\u0026#34; IN { type hint; file \u0026#34;named.ca\u0026#34;; }; zone \u0026#34;ocp.infotech.com.tw\u0026#34; IN { type master; file \u0026#34;/var/named/forward.bastion.ocp\u0026#34;; allow-update { none; }; allow-query { any; }; allow-transfer { none; }; }; zone \u0026#34;3.168.192.in-addr.arpa\u0026#34; IN { type master; file \u0026#34;/var/named/reverse.bastion.ocp\u0026#34;; allow-update { none; }; allow-query { any; }; allow-transfer { none; }; }; include \u0026#34;/etc/named.rfc1912.zones\u0026#34;; include \u0026#34;/etc/named.root.key\u0026#34;; ------ 正向解析 [root@bastion ~]# cat /var/named/forward.bastion.ocp $TTL 1W @\tIN\tSOA\tns1.infotech.com.tw.\troot ( 2019070700\t; serial 3H\t; refresh (3 hours) 30M\t; retry (30 minutes) 2W\t; expiry (2 weeks) 1W )\t; minimum (1 week) IN\tNS\tns1.infotech.com.tw. IN\tMX 10\tsmtp.infotech.com.tw. ; ; ns1.infotech.com.tw.\tIN\tA\t192.168.3.37 smtp.infotech.com.tw.\tIN\tA\t192.168.3.37 ; helper.infotech.com.tw.\tIN\tA\t192.168.3.37 helper.ocp.infotech.com.tw.\tIN\tA\t192.168.3.37 ; api.ocp.infotech.com.tw.\tIN\tA\t192.168.3.37 api-int.ocp.infotech.com.tw.\tIN\tA\t192.168.3.37 ; *.apps.ocp.infotech.com.tw.\tIN\tA\t192.168.3.37 ; bootstrap.ocp.infotech.com.tw.\tIN\tA\t192.168.3.36 ; master1.ocp.infotech.com.tw.\tIN\tA\t192.168.3.31 master2.ocp.infotech.com.tw.\tIN\tA\t192.168.3.32 master3.ocp.infotech.com.tw.\tIN\tA\t192.168.3.33 ; worker1.ocp.infotech.com.tw.\tIN\tA\t192.168.3.34 worker2.ocp.infotech.com.tw.\tIN\tA\t192.168.3.35 ; ;EOF ------- 反向解析 [root@bastion ~]# cat /var/named/reverse.bastion.ocp $TTL 1W @\tIN\tSOA\tns1.infotech.com.tw.\troot ( 2019070700\t; serial 3H\t; refresh (3 hours) 30M\t; retry (30 minutes) 2W\t; expiry (2 weeks) 1W )\t; minimum (1 week) IN\tNS\tns1.infotech.com.tw. ; 37.3.168.192.in-addr.arpa.\tIN\tPTR\tapi.ocp.infotech.com.tw. 37.3.168.192.in-addr.arpa.\tIN\tPTR\tapi-int.ocp.infotech.com.tw. ; 36.3.168.192.in-addr.arpa.\tIN\tPTR\tbootstrap.ocp.infotech.com.tw. ; 31.3.168.192.in-addr.arpa.\tIN\tPTR\tmaster1.ocp.infotech.com.tw. 32.3.168.192.in-addr.arpa.\tIN\tPTR\tmaster2.ocp.infotech.com.tw. 33.3.168.192.in-addr.arpa.\tIN\tPTR\tmaster3.ocp.infotech.com.tw. ; 34.3.168.192.in-addr.arpa.\tIN\tPTR\tworker1.ocp.infotech.com.tw. 35.3.168.192.in-addr.arpa.\tIN\tPTR\tworker2.ocp.infotech.com.tw. ; ;EOF HAProxy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 [root@bastion ~]# cat /etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 pidfile /var/run/haproxy.pid maxconn 4000 daemon defaults mode http log global option dontlognull option http-server-close option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 frontend stats bind *:1936 mode http log global maxconn 10 stats enable stats hide-version stats refresh 30s stats show-node stats show-desc Stats for ocp cluster stats auth admin:ocp stats uri /stats listen api-server-6443 bind *:6443 mode tcp server bootstrap bootstrap.ocp.infotech.com.tw:6443 check inter 1s backup server master1 master1.ocp.infotech.com.tw:6443 check inter 1s server master2 master2.ocp.infotech.com.tw:6443 check inter 1s server master3 master3.ocp.infotech.com.tw:6443 check inter 1s listen machine-config-server-22623 bind *:22623 mode tcp server bootstrap bootstrap.ocp.infotech.com.tw:22623 check inter 1s backup server master1 master1.ocp.infotech.com.tw:22623 check inter 1s server master2 master2.ocp.infotech.com.tw:22623 check inter 1s server master3 master3.ocp.infotech.com.tw:22623 check inter 1s listen ingress-router-443 bind *:443 mode tcp balance source server worker1 worker1.ocp.infotech.com.tw:443 check inter 1s server worker2 worker2.ocp.infotech.com.tw:443 check inter 1s listen ingress-router-80 bind *:80 mode tcp balance source server worker1 worker1.ocp.infotech.com.tw:80 check inter 1s server worker2 worker2.ocp.infotech.com.tw:80 check inter 1s Nginx 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 根目錄 /usr/share/nginx/html Port: 8000 [root@bastion html]# cat /etc/nginx/nginx.conf # For more information on configuration, see: # * Official English Documentation: http://nginx.org/en/docs/ # * Official Russian Documentation: http://nginx.org/ru/docs/ user nginx; worker_processes auto; error_log /var/log/nginx/error.log; pid /run/nginx.pid; # Load dynamic modules. See /usr/share/doc/nginx/README.dynamic. include /usr/share/nginx/modules/*.conf; events { worker_connections 1024; } http { log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; server { listen 8000 default_server; listen [::]:8000 default_server; server_name _; root /usr/share/nginx/html; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / { } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } # Settings for a TLS enabled server. # # server { # listen 443 ssl http2 default_server; # listen [::]:443 ssl http2 default_server; # server_name _; # root /usr/share/nginx/html; # # ssl_certificate \u0026#34;/etc/pki/nginx/server.crt\u0026#34;; # ssl_certificate_key \u0026#34;/etc/pki/nginx/private/server.key\u0026#34;; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 10m; # ssl_ciphers PROFILE=SYSTEM; # ssl_prefer_server_ciphers on; # # # Load configuration files for the default server block. # include /etc/nginx/default.d/*.conf; # # location / { # } # # error_page 404 /404.html; # location = /40x.html { # } # # error_page 500 502 503 504 /50x.html; # location = /50x.html { # } # } } 產生安裝檔案 確認 cli tool 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ ls -l total 2232728 drwxr-xr-x 8 wangken staff 256 12 13 15:36 . drwxr-x---+ 41 wangken staff 1312 12 13 15:35 .. -rw-r--r--@ 1 wangken staff 706 10 8 18:41 README.md -rwxr-xr-x@ 2 wangken staff 98436592 10 6 20:06 kubectl -rwxr-xr-x@ 2 wangken staff 98436592 10 6 20:06 oc -rw-r--r--@ 1 wangken staff 44116650 12 13 15:06 openshift-client-mac-4.11.9.tar.gz -rwxr-xr-x@ 1 wangken staff 528956912 10 8 18:41 openshift-install -rw-r--r--@ 1 wangken staff 363543317 12 13 15:07 openshift-install-mac-4.11.9.tar.gz $ ./oc version Client Version: 4.11.9 Kustomize Version: v4.5.4 $ ./openshift-install version ./openshift-install 4.11.9 built from commit 01a6869a6f1208fb4d112060c5971432fdd619cf release image quay.io/openshift-release-dev/ocp-release@sha256:94b611f00f51c9acc44ca3f4634e46bd79d7d28b46047c7e3389d250698f0c99 release architecture amd64 產生SSH key (Bastion) 1 2 3 4 5 6 7 8 9 10 11 [root@bastion .ssh]# cat id_rsa -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW QyNTUxOQAAACDF/jswkiYiL+jXDMMUpwga7CGV79kXtcMnOpbzO5xOwQAAAKjY1yrT2Ncq 0wAAAAtzc2gtZWQyNTUxOQAAACDF/jswkiYiL+jXDMMUpwga7CGV79kXtcMnOpbzO5xOwQ AAAEBIWOmAtlitDdZXhb+1vtMnSbNBRQfyANPR/eHUPuisK8X+OzCSJiIv6NcMwxSnCBrs IZXv2Re1wyc6lvM7nE7BAAAAIWRldm9wQGJhc3Rpb24ub2NwLmluZm90ZWNoLmNvbS50dw ECAwQ= -----END OPENSSH PRIVATE KEY----- [root@bastion .ssh]# cat id_rsa.pub ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIMX+OzCSJiIv6NcMwxSnCBrsIZXv2Re1wyc6lvM7nE7B devop@bastion.ocp.infotech.com.tw 建立Install-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion: v1 baseDomain: infotech.com.tw compute: - hyperthreading: Enabled name: worker replicas: 0 controlPlane: hyperthreading: Enabled name: master replicas: 3 metadata: name: ocp networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: vsphere: vcenter: tpeinfovcsa7p.infotech.com.tw username: cisadmin@vsphere.local password: P@55w.rd datacenter: TPECIS_Datacenter defaultDatastore: v7000_CIS01 diskType: thin fips: false pullSecret: \u0026#39;{\u0026#34;auths\u0026#34;:{\u0026#34;cloud.openshift.com\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;b3BlbnNoaWZ0LXJlbGVhc2UtZGV2K29jbV9hY2Nlc3NfMzEwNjVjNmVmMTk5NDZmNWEwNmUzZmI4MTVmNzk2OTE6NEJRVk1FVk9CTDRZQVRBT0pONEhQVkIyUExFQTdKNDE3WExFU1IzVDk3NU1YUTJXWE9FSjBNTVk0MzFQQk9RUA==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;},\u0026#34;quay.io\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;b3BlbnNoaWZ0LXJlbGVhc2UtZGV2K29jbV9hY2Nlc3NfMzEwNjVjNmVmMTk5NDZmNWEwNmUzZmI4MTVmNzk2OTE6NEJRVk1FVk9CTDRZQVRBT0pONEhQVkIyUExFQTdKNDE3WExFU1IzVDk3NU1YUTJXWE9FSjBNTVk0MzFQQk9RUA==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;},\u0026#34;registry.connect.redhat.com\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;fHVoYy1wb29sLTc5MzgwY2YyLTUxNTEtNGRiNi04YjA3LTRiNzQ4NDcxNWE2NDpleUpoYkdjaU9pSlNVelV4TWlKOS5leUp6ZFdJaU9pSmxOak5rWVdWbU1tSXlaRFEwTVdKaVlUTXpObVUwTkdGbVl6WmtPVEUwWVNKOS5RMlp6Q1JaWWltVDIxM2dpYU9CRWtNWEVkYUtjbnc4WHN1bTY3VEVOdjEyRjBjQUgyNWNZMVdWTjZNUEY2R21CNjh3TmF4TnoxRHVpVnBtY0ZKM2JZTW0yNVVzZGJxeFc4cEd1dGszSzF5NlNUWk9VejlkU0V4UGg4aUxha05qSEQ4Tml6emNmM1lUQjRNUXdhYTVFSG1PMXVTUERUZFRBTXFYS3FFa2gwRXpHOUpKZTlUMnM4bVJXWkVIbndoZW84cktncVZuV1VCeGhqTldFNElfWG1JZnBPRl91akpFNHdXNzRJWjd5UmdoUGExQVVwS3BsY0NJQ2ZGdGtaS29WUTFwNTBzUzNabkxWYjd6ZHdXR21WcXVsNUFoZUNiSHNoNEtpSjFnYXRSbkwtMG92THZoNGdseUZqd0c0Z3lMMEdyS1ItbU5WMFZYc01mVzJDNXNVaXBzVmhKcmhmY01vQjRNSDRDbjZKcjJXS2plbEdqSVdHQzBUV1BBNlZQZDBmc0lyLTRCY2RBdUFmcm5SaWpGM3d0Mng3RFdXQ1RScFVPbUVMY3U5bDR6Wm5uZkVSejNMd01rNmhCWTFyYXV1TGhCWXJraFlzQURqWEl3bGxPTkwyc01GMUU2OVNIVG1vV0tvNU5fdHdoWE5fX1BtT1ozMzc5ZVNDVGdnTFNZd3hyR1VwS3FXRDFva3JWZlRIWmNlcTk4WHQ2QjFtZHIxcmxLV2RiRmhsRDQ3Q1pldEhrNW1xQUlianp1UzlONENJWllhd0lIZE5LWVotN2tRNFQyUFg4QWdadHFNdHdjY2c0bWx3ME40c09LbWVZN2Q2bGw0bXpXV201bVo0aktTeHdpYmJ3STEwOFcxWVFlSlQ4QW9WZFlkc09YWkdWc3ZxZVVZOU9qbzR2RQ==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;},\u0026#34;registry.redhat.io\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;fHVoYy1wb29sLTc5MzgwY2YyLTUxNTEtNGRiNi04YjA3LTRiNzQ4NDcxNWE2NDpleUpoYkdjaU9pSlNVelV4TWlKOS5leUp6ZFdJaU9pSmxOak5rWVdWbU1tSXlaRFEwTVdKaVlUTXpObVUwTkdGbVl6WmtPVEUwWVNKOS5RMlp6Q1JaWWltVDIxM2dpYU9CRWtNWEVkYUtjbnc4WHN1bTY3VEVOdjEyRjBjQUgyNWNZMVdWTjZNUEY2R21CNjh3TmF4TnoxRHVpVnBtY0ZKM2JZTW0yNVVzZGJxeFc4cEd1dGszSzF5NlNUWk9VejlkU0V4UGg4aUxha05qSEQ4Tml6emNmM1lUQjRNUXdhYTVFSG1PMXVTUERUZFRBTXFYS3FFa2gwRXpHOUpKZTlUMnM4bVJXWkVIbndoZW84cktncVZuV1VCeGhqTldFNElfWG1JZnBPRl91akpFNHdXNzRJWjd5UmdoUGExQVVwS3BsY0NJQ2ZGdGtaS29WUTFwNTBzUzNabkxWYjd6ZHdXR21WcXVsNUFoZUNiSHNoNEtpSjFnYXRSbkwtMG92THZoNGdseUZqd0c0Z3lMMEdyS1ItbU5WMFZYc01mVzJDNXNVaXBzVmhKcmhmY01vQjRNSDRDbjZKcjJXS2plbEdqSVdHQzBUV1BBNlZQZDBmc0lyLTRCY2RBdUFmcm5SaWpGM3d0Mng3RFdXQ1RScFVPbUVMY3U5bDR6Wm5uZkVSejNMd01rNmhCWTFyYXV1TGhCWXJraFlzQURqWEl3bGxPTkwyc01GMUU2OVNIVG1vV0tvNU5fdHdoWE5fX1BtT1ozMzc5ZVNDVGdnTFNZd3hyR1VwS3FXRDFva3JWZlRIWmNlcTk4WHQ2QjFtZHIxcmxLV2RiRmhsRDQ3Q1pldEhrNW1xQUlianp1UzlONENJWllhd0lIZE5LWVotN2tRNFQyUFg4QWdadHFNdHdjY2c0bWx3ME40c09LbWVZN2Q2bGw0bXpXV201bVo0aktTeHdpYmJ3STEwOFcxWVFlSlQ4QW9WZFlkc09YWkdWc3ZxZVVZOU9qbzR2RQ==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;}}}\u0026#39; sshKey: \u0026#39;ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIMX+OzCSJiIv6NcMwxSnCBrsIZXv2Re1wyc6lvM7nE7B devop@bastion.ocp.infotech.com.tw\u0026#39; capabilities: baselineCapabilitySet: vCurrent ------------- platform: vsphere: vcenter: tpeinfovcsa7p.infotech.com.tw username: cisadmin@vsphere.local password: P@55w.rd datacenter: TPECIS_Datacenter defaultDatastore: v7000_CIS01 diskType: thin 產生Ignition file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ./openshift-install create manifests --dir install ./openshift-install create ignition-configs --dir install INFO Consuming Openshift Manifests from target directory INFO Consuming Common Manifests from target directory INFO Consuming Worker Machines from target directory INFO Consuming Master Machines from target directory INFO Consuming OpenShift Install (Manifests) from target directory INFO Ignition-Configs created in: install and install/auth cd install $ ll total 2960 drwxr-xr-x 9 wangken staff 288 12 13 16:27 . drwxr-xr-x 11 wangken staff 352 12 13 15:56 .. -rw-r--r-- 1 wangken staff 67239 12 13 16:27 .openshift_install.log -rw-r----- 1 wangken staff 1154395 12 13 16:27 .openshift_install_state.json drwxr-x--- 4 wangken staff 128 12 13 16:27 auth -rw-r----- 1 wangken staff 275583 12 13 16:27 bootstrap.ign -rw-r----- 1 wangken staff 1721 12 13 16:27 master.ign -rw-r----- 1 wangken staff 94 12 13 16:27 metadata.json -rw-r----- 1 wangken staff 1721 12 13 16:27 worker.ign 複製Ignition File 到Web 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 scp *.ign root@192.168.3.37:/usr/share/nginx/html/ root@192.168.3.37\u0026#39;s password: bootstrap.ign 100% 269KB 3.5MB/s 00:00 master.ign 100% 1721 86.9KB/s 00:00 worker.ign 100% 1721 188.5KB/s 00:00 ----------- cat merge-bootstrap.ign { \u0026#34;ignition\u0026#34;: { \u0026#34;config\u0026#34;: { \u0026#34;merge\u0026#34;: [ { \u0026#34;source\u0026#34;: \u0026#34;http://192.168.3.37:8000/bootstrap.ign\u0026#34;, \u0026#34;verification\u0026#34;: {} } ] }, \u0026#34;timeouts\u0026#34;: {}, \u0026#34;version\u0026#34;: \u0026#34;3.2.0\u0026#34; }, \u0026#34;networkd\u0026#34;: {}, \u0026#34;passwd\u0026#34;: {}, \u0026#34;storage\u0026#34;: {}, \u0026#34;systemd\u0026#34;: {} } -----轉檔為Base64 chmod 644 *.ign [root@bastion html]# base64 -w0 merge-bootstrap.ign \u0026gt; merge-bootstrap.64 ewogICJpZ25pdGlvbiI6IHsKICAgICJjb25maWciOiB7CiAgICAgICJtZXJnZSI6IFsKICAgICAgICB7CiAgICAgICAgICAic291cmNlIjogImh0dHA6Ly8xOTIuMTY4LjMuMzc6ODAwMC9ib290c3RyYXAuaWduIiwKICAgICAgICAgICJ2ZXJpZmljYXRpb24iOiB7fQogICAgICAgIH0KICAgICAgXQogICAgfSwKICAgICJ0aW1lb3V0cyI6IHt9LAogICAgInZlcnNpb24iOiAiMy4yLjAiCiAgfSwKICAibmV0d29ya2QiOiB7fSwKICAicGFzc3dkIjoge30sCiAgInN0b3JhZ2UiOiB7fSwKICAic3lzdGVtZCI6IHt9Cn0K [root@bastion html]# base64 -w0 master.ign \u0026gt; master.64 [root@bastion html]# base64 -w0 worker.ign \u0026gt; worker.64 複製OVF至VM 調整參數 1 2 3 4 5 6 7 8 9 10 11 vApp 1. base64 2. bootstrap ewogICJpZ25pdGlvbiI6IHsKICAgICJjb25maWciOiB7CiAgICAgICJtZXJnZSI6IFsKICAgICAgICB7CiAgICAgICAgICAic291cmNlIjogImh0dHA6Ly8xOTIuMTY4LjMuMzc6ODAwMC9ib290c3RyYXAuaWduIiwKICAgICAgICAgICJ2ZXJpZmljYXRpb24iOiB7fQogICAgICAgIH0KICAgICAgXQogICAgfSwKICAgICJ0aW1lb3V0cyI6IHt9LAogICAgInZlcnNpb24iOiAiMy4yLjAiCiAgfSwKICAibmV0d29ya2QiOiB7fSwKICAicGFzc3dkIjoge30sCiAgInN0b3JhZ2UiOiB7fSwKICAic3lzdGVtZCI6IHt9Cn0K master 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 worker eyJpZ25pdGlvbiI6eyJjb25maWciOnsibWVyZ2UiOlt7InNvdXJjZSI6Imh0dHBzOi8vYXBpLWludC5vY3AuaW5mb3RlY2guY29tLnR3OjIyNjIzL2NvbmZpZy93b3JrZXIifV19LCJzZWN1cml0eSI6eyJ0bHMiOnsiY2VydGlmaWNhdGVBdXRob3JpdGllcyI6W3sic291cmNlIjoiZGF0YTp0ZXh0L3BsYWluO2NoYXJzZXQ9dXRmLTg7YmFzZTY0LExTMHRMUzFDUlVkSlRpQkRSVkpVU1VaSlEwRlVSUzB0TFMwdENrMUpTVVJGUkVORFFXWnBaMEYzU1VKQlowbEpXV1JoZUZwMFZXVkVha2wzUkZGWlNrdHZXa2xvZG1OT1FWRkZURUpSUVhkS2FrVlRUVUpCUjBFeFZVVUtRM2hOU21JelFteGliazV2WVZkYU1FMVNRWGRFWjFsRVZsRlJSRVYzWkhsaU1qa3dURmRPYUUxQ05GaEVWRWw1VFZSSmVFMTZRVFJPUkZVeFRteHZXQXBFVkUxNVRWUkplRTFFUVRST1JGVXhUbXh2ZDBwcVJWTk5Ra0ZIUVRGVlJVTjRUVXBpTTBKc1ltNU9iMkZYV2pCTlVrRjNSR2RaUkZaUlVVUkZkMlI1Q21JeU9UQk1WMDVvVFVsSlFrbHFRVTVDWjJ0eGFHdHBSemwzTUVKQlVVVkdRVUZQUTBGUk9FRk5TVWxDUTJkTFEwRlJSVUV6U0hWcVNHc3haaTlCWWxNS2FHcEdhemxtV0hOelZqZFJNVzlJUVRkVFNqQnVlVkpqTDJNMmR6aERPV1ZUV1dkTWNIWnJjSEZEY3pCdFR5dE9NMHRGTW5Kb2VTdDVVVUZyUVN0eEt3cE5TVkJXVURWRU9HRjFkM3BwWTFOUFZERm5WR2xTTmpreWFFVTFVVEUwYmtaS1N6bE5UMGRaWkdJeFJFTklNazlJYkZkMWRqRktlVlJvUW5admJuRXhDamw2VG01SldreG5iMWRUUm5SWGNtMVFhR05vY3pJeE5sVTVhREpDY0U1QmIwbzJXbU01TjBZM09UWnRLMFpLVlVKeGRFSnhVMVU1UW0xMmEwbHZSa3dLVFd0dE5HczVORk50UzFCRVpURnpPSEZGY0ZGM2FDdDVhbGRPV214aFkyeHRaMGRTTVdkRFptbGxPRGt3UjJ4dWRUazFSbWMwWjBORVFXMHpNRkZ4V2dvdk1tVkNXVEJ2ZW5kMFkyOTZVakZVVkU1cVpUSnpXaTlPZEhBMUwyRTBiR3RMYjNweGJXSnpURE13U25aaFNtbzVlVmh4T1VRelZtRnZVV2xIWkhWbENsQkpMMlJ6WkZsTWEzZEpSRUZSUVVKdk1FbDNVVVJCVDBKblRsWklVVGhDUVdZNFJVSkJUVU5CY1ZGM1JIZFpSRlpTTUZSQlVVZ3ZRa0ZWZDBGM1JVSUtMM3BCWkVKblRsWklVVFJGUm1kUlZXdEljR1IyVG5sUlVuZzFkVWhoTjJoWmJ6Tm9RM0Y2ZDJ4M1VYZEVVVmxLUzI5YVNXaDJZMDVCVVVWTVFsRkJSQXBuWjBWQ1FVbzVlRnBRTlRNeVZqWjBaa3RVVGxSVGFGTllWbEZVTlRCdmF6VkRUR3h1ZVVaNWRtRkNjV2huZEhwVlZsZGpPR2xGVTFocWFYbHJSRlZxQ2twallXcFZOSEZaU1ZkSmVGWlJNVmR3Um5GcFNqQlpkbTV4TXpOa2JFMUdORTg1U1VWT1owTXdkRGx3UjI1R1NGSlpLMUV6YjA5WVkzUXdWRlF2UkhVS1JuSnpXbFpoTTJsVE1rZDRaSGc1ZDFsV1pFbHVUVzV2V0VscFFrRlliMDlaTTBaNVYwRlROakJOV1RkMmVXTkhObXRLVFZWdVpUbDRVbGRyWjBsRFJBcDZOVXBXU0RSUE1FZ3pZVGxaTVhsSFowODBTVVpSVVN0c1JsZENiMmhOU2xrelpXaEpXR05oVDJORGJsWTNZV1pvUXpKcVVHbFpPRTFrZFdsWU9HRkRDblExUkdVM1psQjRUMmsxVWpSclZWQmtWRFYwYkhGeldHdEliWEJFU3lzM1ZVaFZLMk13ZFdGd1FVTjVObVUzU1U1U1dGbHViVkpMTlVadFJUSkZPVVFLUmxoc2NXdEtTM1JITVVoVU5rSXdZbnB5UzBsU05ucDVPVVJyUFFvdExTMHRMVVZPUkNCRFJWSlVTVVpKUTBGVVJTMHRMUzB0Q2c9PSJ9XX19LCJ2ZXJzaW9uIjoiMy4yLjAifX0= 設定IP 1 2 3 4 5 6 7 guestinfo.afterburn.initrd.network-kargs ip=192.168.3.36::192.168.3.251:255.255.255.0:::none nameserver=192.168.3.37 ip=192.168.3.31::192.168.3.251:255.255.255.0:::none nameserver=192.168.3.37 ip=192.168.3.32::192.168.3.251:255.255.255.0:::none nameserver=192.168.3.37 ip=192.168.3.33::192.168.3.251:255.255.255.0:::none nameserver=192.168.3.37 ip=192.168.3.34::192.168.3.251:255.255.255.0:::none nameserver=192.168.3.37 ip=192.168.3.35::192.168.3.251:255.255.255.0:::none nameserver=192.168.3.37 先啟動bootstrap 再啟動Master 啟動Worker後 Approve CSR 1 2 3 $oc adm certificate approve \u0026lt;csr_name\u0026gt; ---- $oc get csr -o go-template=\u0026#39;{{range .items}}{{if not .status}}{{.metadata.name}}{{\u0026#34;\\n\u0026#34;}}{{end}}{{end}}\u0026#39; | xargs --no-run-if-empty oc adm certificate approve Storage Class https://access.redhat.com/solutions/4618011\nhttps://access.redhat.com/solutions/5900501 https://www.ibm.com/support/pages/creating-persistent-volume-claim-fails-not-found-openshift https://docs.openshift.com/container-platform/4.9/storage/persistent_storage/persistent-storage-vsphere.html\nhttps://github.com/rancher/rancher/issues/24258\non vmware vsphere\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 [devop@bastion ~]$ oc get machineset ocp-7hb6g-worker -n openshift-machine-api -o yaml apiVersion: machine.openshift.io/v1beta1 kind: MachineSet metadata: annotations: machine.openshift.io/memoryMb: \u0026#34;16384\u0026#34; machine.openshift.io/vCPU: \u0026#34;4\u0026#34; creationTimestamp: \u0026#34;2022-12-13T09:05:14Z\u0026#34; generation: 1 labels: machine.openshift.io/cluster-api-cluster: ocp-7hb6g name: ocp-7hb6g-worker namespace: openshift-machine-api resourceVersion: \u0026#34;12244\u0026#34; uid: e52a023d-bc9f-4fb5-a66a-776768e1aa7c spec: replicas: 0 selector: matchLabels: machine.openshift.io/cluster-api-cluster: ocp-7hb6g machine.openshift.io/cluster-api-machineset: ocp-7hb6g-worker template: metadata: labels: machine.openshift.io/cluster-api-cluster: ocp-7hb6g machine.openshift.io/cluster-api-machine-role: worker machine.openshift.io/cluster-api-machine-type: worker machine.openshift.io/cluster-api-machineset: ocp-7hb6g-worker spec: lifecycleHooks: {} metadata: {} providerSpec: value: apiVersion: machine.openshift.io/v1beta1 credentialsSecret: name: vsphere-cloud-credentials diskGiB: 120 kind: VSphereMachineProviderSpec memoryMiB: 16384 metadata: creationTimestamp: null network: devices: - networkName: \u0026#34;\u0026#34; numCPUs: 4 numCoresPerSocket: 4 snapshot: \u0026#34;\u0026#34; template: ocp-7hb6g-rhcos userDataSecret: name: worker-user-data workspace: datacenter: TPECIS_Datacenter datastore: v7000_CIS01 folder: /TPECIS_Datacenter/vm/ocp-7hb6g resourcePool: /TPECIS_Datacenter/host//Resources server: tpeinfovcsa7p.infotech.com.tw status: observedGeneration: 1 replicas: 0 [devop@bastion ~]$ oc get cm cloud-provider-config -n openshift-config -o yaml | grep folder folder = \u0026#34;/TPECIS_Datacenter/vm/ocp-7hb6g\u0026#34; ","date":"2023-09-23T08:19:11+08:00","permalink":"https://wangken0129.github.io/p/ocp-4.11.9_on_vsphereupi/","title":"OCP-4.11.9_on_vSphere(UPI)"},{"content":"Install Openshift v4.11.7 on Bare-metal Ken Wang 2022/10/03\nKen Wang 2022/11/23 v4.11.9\n參考連結 https://docs.openshift.com/container-platform/4.11/welcome/index.html\nhttps://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html/installing/installing-on-bare-metal#installing-bare-metal\nhttps://docs.openshift.com/container-platform/4.11/installing/installing_bare_metal/installing-bare-metal.html#installing-bare-metal\nCoreOS: https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.11/4.11.2/ https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.11/4.11.9/rhcos-4.11.9-x86_64-live.x86_64.iso\nOC Client: https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.11.9/openshift-client-linux.tar.gz\nhttps://docs.openshift.com/container-platform/4.11/post_installation_configuration/index.html\nhttps://github.com/I8C/installing-openshift-on-single-vmware-esxi-host/blob/master/installBastion.sh\n僅供參考 https://github.com/alanadiprastyo/openshift-4.6\nautomated\nhttps://cloud.redhat.com/blog/fully-automated-openshift-deployments-with-vmware-vsphere\n架構 針對各版本的coreOS、Openshift-install 都要更新\n主機名稱 IP Mac 版本 功能 磁碟空間 bastion.ken.lab 192.168.33.21 00:50:56:91:7c:24 RHEL 9.0 management 100G、200G bootstrap.ocp.ken.lab 192.168.33.22 00:50:56:91:47:e9 core 4.11-2 bootstrap 100G master1.ocp.ken.lab 192.168.33.23 00:50:56:91:88:f4 core 4.11-2 master 100G master2.ocp.ken.lab 192.168.33.24 00:50:56:91:27:ed core 4.11-2 master 100G master3.ocp.ken.lab 192.168.33.25 00:50:56:91:08:33 core 4.11-2 master 100G worker1.ocp.ken.lab 192.168.33.26 00:50:56:91:c6:29 core 4.11-2 worker 100G worker2.ocp.ken.lab 192.168.33.27 00:50:56:91:d7:b4 core 4.11-2 worker 100G infra1.ocp.ken.lab 192.168.33.31 00:50:56:91:14:ab core 4.11-2 infra node 100G infra2.ocp.ken.lab 192.168.33.32 00:50:56:91:0a:53 core 4.11-2 infra node 100G infra3.ocp.ken.lab 192.168.33.33 00:50:56:91:78:05 core 4.11-2 infra node 100G 主機名稱 使用者帳密 安裝目錄 bastion.ken.lab devop / redhat /home/devop/ocp 安裝Bastion 安裝RHEL 9.0 on ESXi https://kb.vmware.com/s/article/57122\n用iso 檔開機進入安裝引導畫面 設定磁碟、註冊、安裝軟體\n磁碟配置 / \u0026raquo; 65GiB /home \u0026raquo; 29GiB /boot \u0026raquo; 500MiB /boot/efi \u0026raquo; 1GiB 軟體安裝 使用GUI畫面 Server With GUI 註冊 ken.wang@infotech.com.tw 開始安裝，安裝完畢後進入OS 設定sudo\n1 [root@bastion ~]# echo \u0026#34;devop ALL=(ALL) ALL\u0026#34; \u0026gt;\u0026gt; /etc/sudoers.d/devop 設定 SSH Key 產生ssh keygen \u0026ndash; devop user 用來登入各節點使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [devop@bastion ~]$ ssh-keygen -t ed25519 -N \u0026#39;\u0026#39; -f ~/.ssh/id_ed25519 Generating public/private ed25519 key pair. Created directory \u0026#39;/home/devop/.ssh\u0026#39;. Your identification has been saved in /home/devop/.ssh/id_ed25519 Your public key has been saved in /home/devop/.ssh/id_ed25519.pub The key fingerprint is: SHA256:G91wVoISB5qA881zCUi71a2hHj2Y+3zPD6RJQW5+edM devop@bastion.ocp.ken.lab The key\u0026#39;s randomart image is: +--[ED25519 256]--+ | oo. ooo.. . | | o .o.+.=. o | | o.o+.oo* o | | .o+=o* * . . | | . =oS + = o E| | . o = = . . | | o . o . | | o .. . | | o. .o.. | +----[SHA256]-----+ 查看public key \u0026raquo; 之後會寫入install-config.yaml\n1 2 [devop@bastion ~]$ cat .ssh/id_ed25519.pub ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIN1Z2jXmGnPWGAVrtltN9A2q5D8QtJlRvhvdB2QXoZG devop@bastion.ocp.ken.lab 加入私密金鑰到ssh agent\n1 2 [devop@bastion ~]$ ssh-add .ssh/id_ed25519 Identity added: .ssh/id_ed25519 (devop@bastion.ocp.ken.lab) 安裝OC CLI 從此處下載OC Client https://access.redhat.com/downloads/content/290/ver=4.11/rhel---8/4.11.7/x86_64/product-software https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.11.9/openshift-client-linux.tar.gz\n安裝OC Client\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [devop@bastion oc]$ pwd /home/devop/oc [devop@bastion oc]$ tar xvzf oc-4.11.7-linux.tar.gz README.md kubectl oc [devop@bastion oc]$ sudo cp oc kubectl /usr/local/bin/ [sudo] password for devop: [devop@bastion oc]$ oc OpenShift Client This client helps you develop, build, deploy, and run your applications on any OpenShift or Kubernetes cluster. It also includes the administrative ... 啟用 tab 自動完成功能\n1 2 3 4 [devop@bastion ocp]$ oc completion bash \u0026gt; oc_bash_completion [devop@bastion ocp]$ sudo cp oc_bash_completion /etc/bash_completion.d/ [sudo] password for devop: ## 重新啟動terminal 安裝DHCP Server (optional) 安裝dhcpd\n1 [root@bastion ~]# yum install dhcp-server 修改dhcpd設定檔 /etc/dhcp/dhcpd.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 [root@bastion dhcp]# cat dhcpd.conf # # DHCP Server Configuration file. # see /usr/share/doc/dhcp-server/dhcpd.conf.example # see dhcpd.conf(5) man page # # dhcpd.conf # 2022.10.11 ken wang default-lease-time 600; max-lease-time 7200; option broadcast-address 192.168.33.255; subnet 192.168.33.0 netmask 255.255.255.0 {} ## gateway option routers 192.168.33.251; option domain-name \u0026#34;ocp.ken.lab\u0026#34;; option domain-name-servers 192.168.33.21, 168.95.1.1; # Use this to send dhcp log messages to a different log file (you also # have to hack syslog.conf to complete the redirection). log-facility local7; # range 192.168.33.22 192.168.33.40; host bootstrap { hardware ethernet 00:50:56:91:47:e9; fixed-address 192.168.33.22; } host master1 { hardware ethernet 00:50:56:91:88:f4; fixed-address 192.168.33.23; } host master2 { hardware ethernet 00:50:56:91:27:ed; fixed-address 192.168.33.24; } host master3 { hardware ethernet 00:50:56:91:08:33; fixed-address 192.168.33.25; } host worker1 { hardware ethernet 00:50:56:91:c6:29; fixed-address 192.168.33.26; } host worker2 { hardware ethernet 00:50:56:91:d7:b4; fixed-address 192.168.33.27; } host infra1 { hardware ethernet 00:50:56:91:14:ab; fixed-address 192.168.33.31; } host infra2 { hardware ethernet 00:50:56:91:0a:53; fixed-address 192.168.33.32; } host infra3 { hardware ethernet 00:50:56:91:78:05; fixed-address 192.168.33.33; } 啟動dhcp-server測試\n1 2 3 4 [root@bastion dhcp]# systemctl status dhcpd ● dhcpd.service - DHCPv4 Server Daemon Loaded: loaded (/usr/lib/systemd/system/dhcpd.service; disabled; vendor preset: disabled) Active: active (running) since Tue 2022-10-11 11:07:02 CST; 10s ago 安裝DNS Server (named) 安裝named\n1 [root@bastion dhcp]# yum install bind bind-libs bind-chroot bind-utils 修改named 設定檔 /etc/named.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 [root@bastion data]# cat /etc/named.conf // // named.conf // // Provided by Red Hat bind package to configure the ISC BIND named(8) DNS // server as a caching only nameserver (as a localhost DNS resolver only). // // See /usr/share/doc/bind*/sample/ for example named configuration files. // options { listen-on port 53 { 192.168.33.21; }; //listen-on-v6 port 53 { ::1; }; directory \u0026#34;/var/named\u0026#34;; dump-file \u0026#34;/var/named/data/cache_dump.db\u0026#34;; statistics-file \u0026#34;/var/named/data/named_stats.txt\u0026#34;; memstatistics-file \u0026#34;/var/named/data/named_mem_stats.txt\u0026#34;; secroots-file\t\u0026#34;/var/named/data/named.secroots\u0026#34;; recursing-file\t\u0026#34;/var/named/data/named.recursing\u0026#34;; allow-query { any; }; ## 設定轉寄站 forwarders { 192.168.2.12; 192.168.2.11; }; /* - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion. - If you are building a RECURSIVE (caching) DNS server, you need to enable recursion. - If your recursive DNS server has a public IP address, you MUST enable access control to limit queries to your legitimate users. Failing to do so will cause your server to become part of large scale DNS amplification attacks. Implementing BCP38 within your network would greatly reduce such attack surface */ recursion yes; dnssec-validation yes; dnssec-enable yes; managed-keys-directory \u0026#34;/var/named/dynamic\u0026#34;; geoip-directory \u0026#34;/usr/share/GeoIP\u0026#34;; pid-file \u0026#34;/run/named/named.pid\u0026#34;; session-keyfile \u0026#34;/run/named/session.key\u0026#34;; /* https://fedoraproject.org/wiki/Changes/CryptoPolicy */ include \u0026#34;/etc/crypto-policies/back-ends/bind.config\u0026#34;; }; logging { channel default_debug { file \u0026#34;data/named.run\u0026#34;; severity dynamic; }; }; zone \u0026#34;.\u0026#34; IN { type hint; file \u0026#34;named.ca\u0026#34;; }; include \u0026#34;/etc/named.rfc1912.zones\u0026#34;; include \u0026#34;/etc/named.root.key\u0026#34;; zone \u0026#34;ocp.ken.lab.\u0026#34; IN { type master; file \u0026#34;ken.zone\u0026#34;; allow-update { none; }; allow-query { any; }; allow-transfer { none; }; }; zone \u0026#34;33.168.192.in-addr.arpa\u0026#34; IN { type master; file \u0026#34;ken.reverse\u0026#34;; allow-update { none; }; allow-query { any; }; allow-transfer { none; }; }; 修改正向解析 /var/named/ken.zone\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 [root@bastion named]# cat ken.zone $TTL 1W @\tIN\tSOA\tbastion.ken.lab.\troot ( 2019070700\t; serial 3H\t; refresh (3 hours) 30M\t; retry (30 minutes) 2W\t; expiry (2 weeks) 1W )\t; minimum (1 week) IN\tNS\tbastion.ken.lab. ; ; bastion.ken.lab.\tIN\tA\t192.168.33.21 ; helper.ken.lab.\tIN\tA\t192.168.33.21 helper.ocp.ken.lab.\tIN\tA\t192.168.33.21 ; api.ocp.ken.lab.\tIN\tA\t192.168.33.21 api-int.ocp.ken.lab.\tIN\tA\t192.168.33.21 ; *.apps.ocp.ken.lab.\tIN\tA\t192.168.33.21 ; bootstrap.ocp.ken.lab.\tIN\tA\t192.168.33.22 ; master1.ocp.ken.lab.\tIN\tA\t192.168.33.23 master2.ocp.ken.lab.\tIN\tA\t192.168.33.24 master3.ocp.ken.lab.\tIN\tA\t192.168.33.25 ; worker1.ocp.ken.lab.\tIN\tA\t192.168.33.26 worker2.ocp.ken.lab.\tIN\tA\t192.168.33.27 ; infra1.ocp.ken.lab.\tIN\tA\t192.168.33.31 infra2.ocp.ken.lab.\tIN\tA\t192.168.33.32 infra3.ocp.ken.lab.\tIN\tA\t192.168.33.33 ; ;EOF 修改反解析 /var/named/ken.reverse\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [root@bastion named]# cat ken.reverse $TTL 10 @\tIN\tSOA\tbastion.ken.lab.\troot ( 2019070700\t; serial 3H\t; refresh (3 hours) 30M\t; retry (30 minutes) 2W\t; expiry (2 weeks) 1W )\t; minimum (1 week) IN\tNS\tbastion.ken.lab. ; ; 21.33.168.192.in-addr.arpa.\tIN\tPTR\tbastion.ocp.ken.lab. 21.33.168.192.in-addr.arpa.\tIN\tPTR\tapi.ocp.ken.lab. 21.33.168.192.in-addr.arpa.\tIN\tPTR\tapi-int.ocp.ken.lab. ; 22.33.168.192.in-addr.arpa.\tIN\tPTR\tbootstrap.ocp.ken.lab. ; 23.33.168.192.in-addr.arpa.\tIN\tPTR\tmaster1.ocp.ken.lab. 24.33.168.192.in-addr.arpa.\tIN\tPTR\tmaster2.ocp.ken.lab. 25.33.168.192.in-addr.arpa.\tIN\tPTR\tmaster3.ocp.ken.lab. ; 26.33.168.192.in-addr.arpa.\tIN\tPTR\tworker1.ocp.ken.lab. 27.33.168.192.in-addr.arpa.\tIN\tPTR\tworker2.ocp.ken.lab. ; 31.33.168.192.in-addr.arpa.\tIN\tPTR\tinfra1.ocp.ken.lab. 32.33.168.192.in-addr.arpa.\tIN\tPTR\tinfra2.ocp.ken.lab. 33.33.168.192.in-addr.arpa.\tIN\tPTR\tinfra3.ocp.ken.lab. ; ;EOF 調整防火牆\n1 2 [root@bastion ~]# firewall-cmd --add-service=dns --permanent [root@bastion ~]# firewall-cmd --reload 啟動服務\n1 [root@bastion ~]# systemctl enable named --now 驗證DNS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ##測試api [root@bastion ~]# dig +noall +answer @192.168.33.21 api.ocp.ken.lab api.ocp.ken.lab.\t604800\tIN\tA\t192.168.33.21 [root@bastion ~]# nslookup 192.168.33.21 21.33.168.192.in-addr.arpa\tname = api.ocp.ken.lab. 21.33.168.192.in-addr.arpa\tname = bastion.ocp.ken.lab. 21.33.168.192.in-addr.arpa\tname = api-int.ocp.ken.lab. ## 測試 wild card [root@bastion ~]# dig +noall +answer @192.168.33.21 random.apps.ocp.ken.lab random.apps.ocp.ken.lab. 604800\tIN\tA\t192.168.33.21 ##測試反解析 [root@bastion ~]# dig +noall +answer @192.168.33.21 -x 192.168.33.21 21.33.168.192.in-addr.arpa. 10\tIN\tPTR\tapi.ocp.ken.lab. 21.33.168.192.in-addr.arpa. 10\tIN\tPTR\tapi-int.ocp.ken.lab. 21.33.168.192.in-addr.arpa. 10\tIN\tPTR\tbastion.ocp.ken.lab. 安裝HA Proxy 安裝haproxy\n1 [root@bastion ~]# yum install haproxy -y 設定HAProxy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 將文件上的設定檔複製貼上，再用sed 指令修改domain name [root@bastion haproxy]# sed -i \u0026#34;s/ocp4.example.com/ocp.ken.lab/g\u0026#34; /etc/haproxy/haproxy.cfg [root@bastion ~]# cat /etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 pidfile /var/run/haproxy.pid maxconn 4000 daemon defaults mode http log global option dontlognull option http-server-close option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 frontend stats bind *:1936 mode http log global maxconn 10 stats enable stats hide-version stats refresh 30s stats show-node stats show-desc Stats for ocp4 cluster stats auth admin:ocp4 stats uri /stats listen api-server-6443 bind *:6443 mode tcp server bootstrap bootstrap.ocp.ken.lab:6443 check inter 1s backup server master1 master1.ocp.ken.lab:6443 check inter 1s server master2 master3.ocp.ken.lab:6443 check inter 1s server master3 master3.ocp.ken.lab:6443 check inter 1s listen machine-config-server-22623 bind *:22623 mode tcp server bootstrap bootstrap.ocp.ken.lab:22623 check inter 1s backup server master1 master1.ocp.ken.lab:22623 check inter 1s server master2 master2.ocp.ken.lab:22623 check inter 1s server master3 master3.ocp.ken.lab:22623 check inter 1s listen ingress-router-443 bind *:443 mode tcp balance source server worker1 worker1.ocp.ken.lab:443 check inter 1s server worker2 worker2.ocp.ken.lab:443 check inter 1s server infra1 infra1.ocp.ken.lab:443 check inter 1s server infra2 infra2.ocp.ken.lab:443 check inter 1s server infra3 infra3.ocp.ken.lab:443 check inter 1s listen ingress-router-80 bind *:80 mode tcp balance source server worker1 worker1.ocp.ken.lab:80 check inter 1s server worker2 worker2.ocp.ken.lab:80 check inter 1s server infra1 infra1.ocp.ken.lab:80 check inter 1s server infra2 infra2.ocp.ken.lab:80 check inter 1s server infra3 infra3.ocp.ken.lab:80 check inter 1s 防火牆開通、SELinux設定\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 [root@bastion ~]# firewall-cmd --add-port=80/tcp --permanent success [root@bastion ~]# firewall-cmd --add-port=443/tcp --permanent success [root@bastion ~]# firewall-cmd --add-port=6443/tcp --permanent success [root@bastion ~]# firewall-cmd --add-port=1936/tcp --permanent success [root@bastion ~]# firewall-cmd --add-port=22623/tcp --permanent success [root@bastion ~]# firewall-cmd --add-port=9000-9999/tcp --permanent success [root@bastion ~]# firewall-cmd --add-port=10250-10259/tcp --permanent success [root@bastion ~]# firewall-cmd --add-port=4789/udp --permanent success [root@bastion ~]# firewall-cmd --add-port=6081/udp --permanent success [root@bastion ~]# firewall-cmd --add-port=9000-9999/udp --permanent success [root@bastion ~]# firewall-cmd --add-port=500/udp --permanent success [root@bastion ~]# firewall-cmd --add-port=4500/udp --permanent success [root@bastion ~]# firewall-cmd --add-port=30000-32767/udp --permanent success [root@bastion ~]# firewall-cmd --add-port=30000-32767/tcp --permanent success [root@bastion ~]# firewall-cmd --reload success [root@bastion ~]# firewall-cmd --list-all public (active) target: default icmp-block-inversion: no interfaces: ens192 sources: services: cockpit dhcpv6-client dns ssh ports: 53/udp 53/tcp 80/tcp 443/tcp 6443/tcp 1936/tcp 22623/tcp 9000-9999/tcp 10250-10259/tcp 4789/udp 6081/udp 9000-9999/udp 500/udp 4500/udp 30000-32767/udp 30000-32767/tcp protocols: forward: yes masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: ## SELinux [root@bastion ocp]# semanage port --add --type http_port_t --proto tcp 1936 [root@bastion ocp]# semanage port --add --type http_port_t --proto tcp 6443 [root@bastion ocp]# semanage port --add --type http_port_t --proto tcp 22623 or [root@bastion ocp]# setsebool -P haproxy_connect_any=on 啟動服務\n1 [root@bastion ~]# systemctl enable haproxy --now 下載Pull Secret Token 網頁連線至https://console.redhat.com/openshift/downloads\n下載 Pull Secret Token 將 pull-secret放到安裝目錄下\n1 2 3 [devop@bastion ocp]$ cp ../pull-secret . [devop@bastion ocp]$ ll -rw-r--r--. 1 devop devop 2795 Oct 11 15:07 pull-secret 下載安裝openshift installer 網頁連線至 https://console.redhat.com/openshift/downloads#tool-pull-secret\n下載 OpenShift for x86_64 Installer 創建目錄並解壓縮\n1 2 3 4 5 6 7 8 9 [devop@bastion ~]$ mkdir ocp [devop@bastion ~]$ mv openshift-install-linux.tar.gz ocp [devop@bastion ~]$ cd ocp [devop@bastion ocp]$ ll total 336508 -rw-r--r--. 1 devop devop 344582118 Oct 11 14:59 openshift-install-linux.tar.gz [devop@bastion ocp]$ tar xzvf openshift-install-linux.tar.gz README.md openshift-install 將openshift installer 放到 /usr/local/bin\n1 2 3 4 5 6 7 8 [devop@bastion ocp]$ sudo cp openshift-install /usr/local/bin/ [sudo] password for devop: [devop@bastion ocp]$ openshift-install --help Creates OpenShift clusters Usage: openshift-install [command] .. 下載 CoreOS 連線至 https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.11/4.11.2/ 下載 https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.11/4.11.2/rhcos-4.11.2-x86_64-live.x86_64.iso 放到ESXi主機上並掛給各虛擬機 準備安裝Soruce 創建install-config.yaml 在/home/devop/ocp目錄下 新增install-config.yaml 重裝時記得將此目錄清除乾淨\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 [devop@bastion ocp]$ vim install-config.yaml [devop@bastion ocp]$ cat install-config.yaml apiVersion: v1 baseDomain: ken.lab compute: - hyperthreading: Enabled name: worker replicas: 0 controlPlane: hyperthreading: Enabled name: master replicas: 3 metadata: name: ocp networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: none: {} fips: false pullSecret: \u0026#39;{\u0026#34;auths\u0026#34;:{\u0026#34;cloud.openshift.com\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;b3BlbnNoaWZ0LXJlbGVhc2UtZGV2K29jbV9hY2Nlc3NfMzEwNjVjNmVmMTk5NDZmNWEwNmUzZmI4MTVmNzk2OTE6NEJRVk1FVk9CTDRZQVRBT0pONEhQVkIyUExFQTdKNDE3WExFU1IzVDk3NU1YUTJXWE9FSjBNTVk0MzFQQk9RUA==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;},\u0026#34;quay.io\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;b3BlbnNoaWZ0LXJlbGVhc2UtZGV2K29jbV9hY2Nlc3NfMzEwNjVjNmVmMTk5NDZmNWEwNmUzZmI4MTVmNzk2OTE6NEJRVk1FVk9CTDRZQVRBT0pONEhQVkIyUExFQTdKNDE3WExFU1IzVDk3NU1YUTJXWE9FSjBNTVk0MzFQQk9RUA==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;},\u0026#34;registry.connect.redhat.com\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;fHVoYy1wb29sLTc5MzgwY2YyLTUxNTEtNGRiNi04YjA3LTRiNzQ4NDcxNWE2NDpleUpoYkdjaU9pSlNVelV4TWlKOS5leUp6ZFdJaU9pSmxOak5rWVdWbU1tSXlaRFEwTVdKaVlUTXpObVUwTkdGbVl6WmtPVEUwWVNKOS5RMlp6Q1JaWWltVDIxM2dpYU9CRWtNWEVkYUtjbnc4WHN1bTY3VEVOdjEyRjBjQUgyNWNZMVdWTjZNUEY2R21CNjh3TmF4TnoxRHVpVnBtY0ZKM2JZTW0yNVVzZGJxeFc4cEd1dGszSzF5NlNUWk9VejlkU0V4UGg4aUxha05qSEQ4Tml6emNmM1lUQjRNUXdhYTVFSG1PMXVTUERUZFRBTXFYS3FFa2gwRXpHOUpKZTlUMnM4bVJXWkVIbndoZW84cktncVZuV1VCeGhqTldFNElfWG1JZnBPRl91akpFNHdXNzRJWjd5UmdoUGExQVVwS3BsY0NJQ2ZGdGtaS29WUTFwNTBzUzNabkxWYjd6ZHdXR21WcXVsNUFoZUNiSHNoNEtpSjFnYXRSbkwtMG92THZoNGdseUZqd0c0Z3lMMEdyS1ItbU5WMFZYc01mVzJDNXNVaXBzVmhKcmhmY01vQjRNSDRDbjZKcjJXS2plbEdqSVdHQzBUV1BBNlZQZDBmc0lyLTRCY2RBdUFmcm5SaWpGM3d0Mng3RFdXQ1RScFVPbUVMY3U5bDR6Wm5uZkVSejNMd01rNmhCWTFyYXV1TGhCWXJraFlzQURqWEl3bGxPTkwyc01GMUU2OVNIVG1vV0tvNU5fdHdoWE5fX1BtT1ozMzc5ZVNDVGdnTFNZd3hyR1VwS3FXRDFva3JWZlRIWmNlcTk4WHQ2QjFtZHIxcmxLV2RiRmhsRDQ3Q1pldEhrNW1xQUlianp1UzlONENJWllhd0lIZE5LWVotN2tRNFQyUFg4QWdadHFNdHdjY2c0bWx3ME40c09LbWVZN2Q2bGw0bXpXV201bVo0aktTeHdpYmJ3STEwOFcxWVFlSlQ4QW9WZFlkc09YWkdWc3ZxZVVZOU9qbzR2RQ==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;},\u0026#34;registry.redhat.io\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;fHVoYy1wb29sLTc5MzgwY2YyLTUxNTEtNGRiNi04YjA3LTRiNzQ4NDcxNWE2NDpleUpoYkdjaU9pSlNVelV4TWlKOS5leUp6ZFdJaU9pSmxOak5rWVdWbU1tSXlaRFEwTVdKaVlUTXpObVUwTkdGbVl6WmtPVEUwWVNKOS5RMlp6Q1JaWWltVDIxM2dpYU9CRWtNWEVkYUtjbnc4WHN1bTY3VEVOdjEyRjBjQUgyNWNZMVdWTjZNUEY2R21CNjh3TmF4TnoxRHVpVnBtY0ZKM2JZTW0yNVVzZGJxeFc4cEd1dGszSzF5NlNUWk9VejlkU0V4UGg4aUxha05qSEQ4Tml6emNmM1lUQjRNUXdhYTVFSG1PMXVTUERUZFRBTXFYS3FFa2gwRXpHOUpKZTlUMnM4bVJXWkVIbndoZW84cktncVZuV1VCeGhqTldFNElfWG1JZnBPRl91akpFNHdXNzRJWjd5UmdoUGExQVVwS3BsY0NJQ2ZGdGtaS29WUTFwNTBzUzNabkxWYjd6ZHdXR21WcXVsNUFoZUNiSHNoNEtpSjFnYXRSbkwtMG92THZoNGdseUZqd0c0Z3lMMEdyS1ItbU5WMFZYc01mVzJDNXNVaXBzVmhKcmhmY01vQjRNSDRDbjZKcjJXS2plbEdqSVdHQzBUV1BBNlZQZDBmc0lyLTRCY2RBdUFmcm5SaWpGM3d0Mng3RFdXQ1RScFVPbUVMY3U5bDR6Wm5uZkVSejNMd01rNmhCWTFyYXV1TGhCWXJraFlzQURqWEl3bGxPTkwyc01GMUU2OVNIVG1vV0tvNU5fdHdoWE5fX1BtT1ozMzc5ZVNDVGdnTFNZd3hyR1VwS3FXRDFva3JWZlRIWmNlcTk4WHQ2QjFtZHIxcmxLV2RiRmhsRDQ3Q1pldEhrNW1xQUlianp1UzlONENJWllhd0lIZE5LWVotN2tRNFQyUFg4QWdadHFNdHdjY2c0bWx3ME40c09LbWVZN2Q2bGw0bXpXV201bVo0aktTeHdpYmJ3STEwOFcxWVFlSlQ4QW9WZFlkc09YWkdWc3ZxZVVZOU9qbzR2RQ==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;}}}\u0026#39; sshKey: \u0026#39;ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIN1Z2jXmGnPWGAVrtltN9A2q5D8QtJlRvhvdB2QXoZG devop@bastion.ocp.ken.lab\u0026#39; capabilities: baselineCapabilitySet: vCurrent -------### 只裝Operator Hub ###--------- capabilities: baselineCapabilitySet: None additionalEnabledCapabilities: - marketplace 建立K8S manifest files 在相同目錄下執行 重裝時記得將此目錄清除乾淨\n1 2 3 4 [devop@bastion ocp]$ openshift-install create manifests --dir /home/devop/ocp/install/ INFO Consuming Install Config from target directory WARNING Making control-plane schedulable by setting MastersSchedulable to true for Scheduler cluster settings INFO Manifests created in: /home/devop/ocp/install/manifests and /home/devop/ocp/install/openshift ＭastersSchedulable:false 1 2 3 4 5 6 7 8 9 10 11 12 [devop@bastion ocp]$ cd manifests/ [devop@bastion manifests]$ cat cluster-scheduler-02-config.yml apiVersion: config.openshift.io/v1 kind: Scheduler metadata: creationTimestamp: null name: cluster spec: mastersSchedulable: false policy: name: \u0026#34;\u0026#34; status: {} 創建Ignition config file 重裝時記得將此目錄清除乾淨\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [devop@bastion ocp]$ openshift-install create ignition-configs --dir /home/devop/ocp/install/ INFO Consuming Worker Machines from target directory INFO Consuming Openshift Manifests from target directory INFO Consuming Common Manifests from target directory INFO Consuming OpenShift Install (Manifests) from target directory INFO Consuming Master Machines from target directory INFO Ignition-Configs created in: /home/devop/ocp/install and /home/devop/ocp/install/auth --- [devop@bastion ocp]$ tree install/ install/ ├── auth │ ├── kubeadmin-password │ └── kubeconfig ├── bootstrap.ign ├── master.ign ├── metadata.json └── worker.ign 1 directory, 6 files ##確認 sha512sum [root@bastion ocp]# sha512sum bootstrap.ign 460f76f9bc8b1df7095326c6b581b60f1150e477ae416b065c7b18ae040dc530a3086857d649ae04f646ccf9cce16ea9a4c32c8fe48b3630413f8ebe3f924329 bootstrap.ign 安裝Http Web Server 安裝httpd\n1 [root@bastion ~]# yum install httpd -y 設定httpd 預設port 8989、修改SELinux\n1 2 3 4 5 6 7 8 9 [root@bastion ~]# vim /etc/httpd/conf/httpd.conf Listen 8989 [root@bastion ocp]# firewall-cmd --add-port=8989/tcp --permanent success [root@bastion ocp]# firewall-cmd --reload success [root@bastion ocp]# semanage port -a -t http_port_t -p tcp 8989 將ignition檔案複製到 /var/www/html\n1 2 3 [root@bastion ~]# cp /home/devop/ocp/*.ign /var/www/html/ocp/ [root@bastion ~]# cp /var/www/html/ocp [root@bastion ocp]# chmod 644 *.ign 啟動httpd並測試連線\n1 2 [root@bastion ocp]# systemctl enable httpd --now [root@bastion ocp]# curl http://192.168.33.21:8989/ocp/bootstrap.ign 安裝BootStrap 將虛擬機開機，並使用rhel core os 的iso檔開機， 若前面dhcp server、dns server設定成功，即會自動拿到FQDN、IP 在bootstrap執行安裝指令\n1 2 sudo coreos-installer install --ignition-url=http://192.168.33.21:8989/ocp/bootstrap.ign /dev/sda --insecure-ignition (--insecure-ignition can replace using --ignition-hash=sha512-xxxxxxxxxxxxx) 出現此畫面後 執行重新啟動 reboot 開機後確認已寫入硬碟 在bastion登入bootstrap\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [devop@bastion ~]$ ssh core@bootstrap The authenticity of host \u0026#39;bootstrap (192.168.33.22)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:pnQoMRtW8f/3EjIQ7qNG4+NIB+ObrTd4E8aEbvRqMQw. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;bootstrap\u0026#39; (ED25519) to the list of known hosts. client_global_hostkeys_private_confirm: server gave bad signature for RSA key 0: error in libcrypto Red Hat Enterprise Linux CoreOS 411.86.202208112011-0 Part of OpenShift 4.11, RHCOS is a Kubernetes native operating system managed by the Machine Config Operator (`clusteroperator/machine-config`). WARNING: Direct SSH access to machines is not recommended; instead, make configuration changes via `machineconfig` objects: https://docs.openshift.com/container-platform/4.11/architecture/architecture-rhcos.html --- This is the bootstrap node; it will be destroyed when the master is fully up. The primary services are release-image.service followed by bootkube.service. To watch their status, run e.g. journalctl -b -f -u release-image.service -u bootkube.service 查看log，到出現machine api時才可接續安裝\n1 2 [core@bootstrap ~]$ journalctl -b -f -u release-image.service -u bootkube.service Create \u0026#34;99_openshift-cluster-api_worker-user-data-secret.yaml\u0026#34; secrets.v1./worker-user-data -n openshift-machine-api 此時才可繼續安裝 master01、master02、master03 出現此畫面即可關閉bootstrap\n安裝 Ｍaster Node 將虛擬機開機，並使用rhel core os 的iso檔開機， 若前面dhcp server、dns server設定成功，即會自動拿到FQDN、IP\n在master執行安裝指令，三台相同 (若設定ＭastersSchedulable:false，則worker需一起安裝）\n1 2 sudo coreos-installer install --ignition-url=http://192.168.33.21:8989/ocp/master.ign /dev/sda --insecure-ignition 出現install complete 即可reboot 持續監控bootstrap的 Log\n1 [core@bootstrap ~]$ journalctl -b -f -u release-image.service -u bootkube.service Master Node裝完會自動重開機\n在bastion 驗證節點\n1 2 3 4 5 [devop@bastion ~]$ oc get nodes NAME STATUS ROLES AGE VERSION master1.ocp.ken.lab Ready master 4m22s v1.24.0+3882f8f master2.ocp.ken.lab Ready master 3m37s v1.24.0+3882f8f master3.ocp.ken.lab Ready master 3m21s v1.24.0+3882f8f 查看bootstrap的 Log,確認安裝完成\n在Bastion 執行oc 指令確認ocp cluster CSR\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [devop@bastion ~]$ oc get nodes NAME STATUS ROLES AGE VERSION master1.ocp.ken.lab Ready master 18m v1.24.0+3882f8f master2.ocp.ken.lab Ready master 17m v1.24.0+3882f8f master3.ocp.ken.lab Ready master 17m v1.24.0+3882f8f [devop@bastion ~]$ oc get csr NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION csr-7sgfx 18m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-lphdj 17m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-ndft5 17m kubernetes.io/kubelet-serving system:node:master3.ocp.ken.lab \u0026lt;none\u0026gt; Approved,Issued csr-p42p5 17m kubernetes.io/kubelet-serving system:node:master2.ocp.ken.lab \u0026lt;none\u0026gt; Approved,Issued csr-vgmd9 17m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-zpfw4 18m kubernetes.io/kubelet-serving system:node:master1.ocp.ken.lab \u0026lt;none\u0026gt; Approved,Issued system:openshift:openshift-authenticator-2zxkf 15m kubernetes.io/kube-apiserver-client system:serviceaccount:openshift-authentication-operator:authentication-operator \u0026lt;none\u0026gt; Approved,Issued system:openshift:openshift-monitoring-ln69x 15m kubernetes.io/kube-apiserver-client system:serviceaccount:openshift-monitoring:cluster-monitoring-operator \u0026lt;none\u0026gt; Approved,Issued 確認cluster operator\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 [devop@bastion install]$ oc get co -A NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE MESSAGE authentication 4.11.7 False False False 76s APIServicesAvailable: PreconditionNotReady... cloud-controller-manager 4.11.7 True False False 2m36s cloud-credential True False False 13m cluster-autoscaler config-operator 4.11.7 True False False 40s console csi-snapshot-controller 4.11.7 True False False 1s dns etcd image-registry ingress insights kube-apiserver Unknown False False 112s kube-controller-manager False True False 75s StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2 kube-scheduler False True False 35s StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 3 kube-storage-version-migrator 4.11.7 True False False 59s machine-api machine-approver machine-config True Working towards 4.11.7 marketplace monitoring network False True False 2m53s The network is starting up node-tuning openshift-apiserver openshift-controller-manager False True False 100s Available: no daemon pods available on any node. openshift-samples operator-lifecycle-manager operator-lifecycle-manager-catalog operator-lifecycle-manager-packageserver service-ca 4.11.7 True False False 33s storage 4.11.7 True False False 49s 關閉bootstrap，安裝worker node\n安裝 Worker＆Infra Node https://docs.openshift.com/container-platform/4.11/machine_management/user_infra/adding-bare-metal-compute-user-infra.html\n將虛擬機開機，並使用rhel core os 的iso檔開機， 若前面dhcp server、dns server設定成功，即會自動拿到FQDN、IP\n執行安裝指令 (worker1、worker2、infra1、infra2、infra3)\n1 2 sudo coreos-installer install --ignition-url=http://192.168.33.21:8989/ocp/worker.ign /dev/sda --insecure-ignition 出現install complete 即可reboot 在bastion 確認nodes、csr (會有很多需approve的 csr，後續加node後還會有)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [devop@bastion ~]$ oc get nodes NAME STATUS ROLES AGE VERSION master1.ocp.ken.lab Ready master 36m v1.24.0+3882f8f master2.ocp.ken.lab Ready master 35m v1.24.0+3882f8f master3.ocp.ken.lab Ready master 35m v1.24.0+3882f8f ---- [devop@bastion ~]$ oc get csr NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION csr-7sgfx 37m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-lphdj 37m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-ndft5 36m kubernetes.io/kubelet-serving system:node:master3.ocp.ken.lab \u0026lt;none\u0026gt; Approved,Issued csr-nwplh 0s kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Pending csr-p42p5 36m kubernetes.io/kubelet-serving system:node:master2.ocp.ken.lab \u0026lt;none\u0026gt; Approved,Issued csr-pcsl2 115s kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Pending csr-vgmd9 36m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-zpfw4 37m kubernetes.io/kubelet-serving system:node:master1.ocp.ken.lab \u0026lt;none\u0026gt; Approved,Issued system:openshift:openshift-authenticator-2zxkf 35m kubernetes.io/kube-apiserver-client system:serviceaccount:openshift-authentication-operator:authentication-operator \u0026lt;none\u0026gt; Approved,Issued system:openshift:openshift-monitoring-ln69x 34m kubernetes.io/kube-apiserver-client system:serviceaccount:openshift-monitoring:cluster-monitoring-operator \u0026lt;none\u0026gt; Approved,Issued Approve All CSR\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 $oc adm certificate approve \u0026lt;csr_name\u0026gt; ---- $oc get csr -o go-template=\u0026#39;{{range .items}}{{if not .status}}{{.metadata.name}}{{\u0026#34;\\n\u0026#34;}}{{end}}{{end}}\u0026#39; | xargs --no-run-if-empty oc adm certificate approve ---- [devop@bastion auth]$ oc get csr -A NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION csr-2trmm 18m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-9xhq4 16m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-b4jns 2m45s kubernetes.io/kubelet-serving system:node:infra1.ocp.ken.lab \u0026lt;none\u0026gt; Pending csr-b55jd 3m57s kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-bdtnw 16m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-bngbr 2m40s kubernetes.io/kubelet-serving system:node:infra2.ocp.ken.lab \u0026lt;none\u0026gt; Pending csr-kd66h 2m51s kubernetes.io/kubelet-serving system:node:infra2.ocp.ken.lab \u0026lt;none\u0026gt; Pending csr-lzk6w 2m46s kubernetes.io/kubelet-serving system:node:worker1.ocp.ken.lab \u0026lt;none\u0026gt; Pending csr-rtkr6 2m46s kubernetes.io/kubelet-serving system:node:worker2.ocp.ken.lab \u0026lt;none\u0026gt; Pending csr-swwhm 18m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-v67v6 16m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued csr-vhlqb 3m56s kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper \u0026lt;none\u0026gt; Approved,Issued --- [devop@bastion auth]$ oc get csr -o go-template=\u0026#39;{{range .items}}{{if not .status}}{{.metadata.name}}{{\u0026#34;\\n\u0026#34;}}{{end}}{{end}}\u0026#39; | xargs --no-run-if-empty oc adm certificate approve certificatesigningrequest.certificates.k8s.io/csr-2trmm approved certificatesigningrequest.certificates.k8s.io/csr-9xhq4 approved certificatesigningrequest.certificates.k8s.io/csr-b55jd approved certificatesigningrequest.certificates.k8s.io/csr-bdtnw approved certificatesigningrequest.certificates.k8s.io/csr-swwhm approved certificatesigningrequest.certificates.k8s.io/csr-v67v6 approved certificatesigningrequest.certificates.k8s.io/csr-vhlqb approved --- [devop@bastion auth]$ oc get csr -o go-template=\u0026#39;{{range .items}}{{if not .status}}{{.metadata.name}}{{\u0026#34;\\n\u0026#34;}}{{end}}{{end}}\u0026#39; | xargs --no-run-if-empty oc adm certificate approve certificatesigningrequest.certificates.k8s.io/csr-b4jns approved certificatesigningrequest.certificates.k8s.io/csr-bngbr approved certificatesigningrequest.certificates.k8s.io/csr-cq46l approved certificatesigningrequest.certificates.k8s.io/csr-kd66h approved certificatesigningrequest.certificates.k8s.io/csr-lzk6w approved certificatesigningrequest.certificates.k8s.io/csr-rtkr6 approved certificatesigningrequest.certificates.k8s.io/csr-t6l2b approved 再次確認node狀態\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [devop@bastion auth]$ oc get nodes NAME STATUS ROLES AGE VERSION infra1.ocp.ken.lab Ready worker 2m32s v1.24.0+3882f8f infra2.ocp.ken.lab Ready,SchedulingDisabled worker 2m38s v1.24.0+3882f8f master1.ocp.ken.lab NotReady master 78m v1.24.0+3882f8f master2.ocp.ken.lab NotReady master 78m v1.24.0+3882f8f master3.ocp.ken.lab NotReady master 78m v1.24.0+3882f8f worker1.ocp.ken.lab Ready worker 2m33s v1.24.0+3882f8f worker2.ocp.ken.lab Ready worker 2m34s v1.24.0+3882f8f --- [devop@bastion ~]$ oc get nodes NAME STATUS ROLES AGE VERSION master1.ocp.ken.lab Ready master 39m v1.24.0+3882f8f master2.ocp.ken.lab Ready master 39m v1.24.0+3882f8f master3.ocp.ken.lab Ready master 38m v1.24.0+3882f8f worker1.ocp.ken.lab Ready worker 95s v1.24.0+3882f8f worker2.ocp.ken.lab Ready worker 90s v1.24.0+3882f8f --- [devop@bastion ~]$ oc get po -A ... 安裝後的設定 設定NFS https://access.redhat.com/solutions/4618011\nhttps://access.redhat.com/solutions/5900501\non vmware vsphere\nhttps://vergiehadiana.medium.com/guide-deploy-nfs-storage-for-dynamic-provisioning-and-image-registry-for-red-hat-openshift-b15e16f357b4\nhttps://hackmd.io/@kd-ocp/S1EIu6V25\n1 2 3 4 5 image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 image: groundhog2k/nfs-subdir-external-provisioner:v3.2.0 設定外部Image Registry https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.6/html/images/images-configuration-file_image-configuration\n設定Htpasswd Provider https://docs.openshift.com/container-platform/4.11/authentication/identity_providers/configuring-htpasswd-identity-provider.html\n利用htpasswd 產生htpasswd檔案\n1 2 3 4 5 6 [devop@bastion ocp]$ htpasswd -c -B -b users.htpasswd admin redhat Adding password for user admin [devop@bastion ocp]$ htpasswd -B -b users.htpasswd ken xxxxxxxx Adding password for user ken [devop@bastion ocp]$ htpasswd -B -b users.htpasswd view redhat Adding password for user view 產生secret 將 htpasswd 放入secret\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [devop@bastion ocp]$ oc whoami system:admin [devop@bastion ocp]$ oc create secret generic users-htpasswd --from-file htpasswd=users.htpasswd -n openshift-config secret/users-htpasswd created [devop@bastion ocp]$ oc get secret -n openshift-config NAME TYPE DATA AGE builder-dockercfg-r46z8 kubernetes.io/dockercfg 1 8h builder-token-cgvp8 kubernetes.io/service-account-token 4 8h default-dockercfg-8mhbj kubernetes.io/dockercfg 1 8h default-token-nfc27 kubernetes.io/service-account-token 4 8h deployer-dockercfg-qdwcq kubernetes.io/dockercfg 1 8h deployer-token-bfplj kubernetes.io/service-account-token 4 8h etcd-client kubernetes.io/tls 2 8h etcd-metric-client kubernetes.io/tls 2 8h etcd-metric-signer kubernetes.io/tls 2 8h etcd-signer kubernetes.io/tls 2 8h initial-service-account-private-key Opaque 1 8h pull-secret kubernetes.io/dockerconfigjson 1 8h users-htpasswd Opaque 1 10s webhook-authentication-integrated-oauth Opaque 1 8h 新增 htpasswd provider\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 [devop@bastion ocp]$ oc get oauth cluster -o yaml \u0026gt; oauth.yaml [devop@bastion ocp]$ vim oauth.yaml [devop@bastion ocp]$ cat oauth.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: annotations: include.release.openshift.io/ibm-cloud-managed: \u0026#34;true\u0026#34; include.release.openshift.io/self-managed-high-availability: \u0026#34;true\u0026#34; include.release.openshift.io/single-node-developer: \u0026#34;true\u0026#34; release.openshift.io/create-only: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2022-10-11T17:14:37Z\u0026#34; generation: 1 name: cluster ownerReferences: - apiVersion: config.openshift.io/v1 kind: ClusterVersion name: version uid: d1c0dbf6-000f-4b1e-aa27-1bd8a667afac resourceVersion: \u0026#34;1533\u0026#34; uid: 3e95cfe7-7c02-47eb-8b4e-7f0d61766461 spec: identityProviders: - name: htpasswd_provider mappingMethod: claim type: HTPasswd htpasswd: fileData: name: users-htpasswd [devop@bastion ocp]$ oc apply -f oauth.yaml -n openshift-config Warning: resource oauths/cluster is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by oc apply. oc apply should only be used on resources created declaratively by either oc create --save-config or oc apply. The missing annotation will be patched automatically. oauth.config.openshift.io/cluster configured --- [devop@bastion ocp]$ watch oc get po -n openshift-authentication 調整角色權限\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## 將cluster-admin role 加給 admin、ken [devop@bastion ~]$ oc adm policy add-cluster-role-to-user cluster-admin admin clusterrole.rbac.authorization.k8s.io/cluster-admin added: \u0026#34;admin\u0026#34; [devop@bastion ~]$ oc adm policy add-cluster-role-to-user cluster-admin ken Warning: User \u0026#39;ken\u0026#39; not found clusterrole.rbac.authorization.k8s.io/cluster-admin added: \u0026#34;ken\u0026#34; ## 將cluster-view role 加給 view [devop@bastion ~]$ oc get clusterroles | grep view view 2022-10-11T17:13:19Z ... [devop@bastion ~]$ oc adm policy add-cluster-role-to-user view view Warning: User \u0026#39;view\u0026#39; not found clusterrole.rbac.authorization.k8s.io/view added: \u0026#34;view\u0026#34; 將預設可建立project的權限刪除，只給admin、ken\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ## 取得 self-provisioner的role [devop@bastion ~]$ oc get clusterrolebindings | grep self-provisioner self-provisioners ClusterRole/self-provisioner 8h ## 取得 self-provisioners role binding的group [devop@bastion ~]$ oc describe clusterrolebindings self-provisioners Name: self-provisioners Labels: \u0026lt;none\u0026gt; Annotations: rbac.authorization.kubernetes.io/autoupdate: true Role: Kind: ClusterRole Name: self-provisioner Subjects: Kind Name Namespace ---- ---- --------- Group system:authenticated:oauth ##移除 self-provisioner的rolebinding [devop@bastion ~]$ oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth Warning: Your changes may get lost whenever a master is restarted, unless you prevent reconciliation of this rolebinding using the following command: oc annotate clusterrolebinding.rbac self-provisioners \u0026#39;rbac.authorization.kubernetes.io/autoupdate=false\u0026#39; --overwrite clusterrole.rbac.authorization.k8s.io/self-provisioner removed: \u0026#34;system:authenticated:oauth\u0026#34; ##新增可以建立project的group [devop@bastion ~]$ oc adm groups new new-project-group admin ken group.user.openshift.io/new-project-group created ##將new-project-group再bind上self-provisioner [devop@bastion ~]$ oc adm policy add-cluster-role-to-group self-provisioner new-project-group clusterrole.rbac.authorization.k8s.io/self-provisioner added: \u0026#34;new-project-group\u0026#34; 取得console url\n1 2 3 4 5 6 [devop@bastion ocp]$ oc get route -n openshift-console NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD console console-openshift-console.apps.ocp.ken.lab console https reencrypt/Redirect None downloads downloads-openshift-console.apps.ocp.ken.lab downloads http edge/Redirect None ### 刪除 kubeadmin [devop@bastion ocp]$ oc delete secret kubeadmin -n kube-system 連線至console https://console-openshift-console.apps.ocp.ken.lab 並測試登入\nadmin / redhat (cluster-admin)、view / redhat (view only) 註冊openshift cluster 從console 點到 Administration \u0026gt; Cluster Settings \u0026gt; Manage subscription settings 點選Edit subscription settings\n選擇 Self-Support 即可 約 3~5 分鐘即可更新License\n12\n設定OC CLI CA 在openshift-authentication project內取得CA並將CA複製到bastion內\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [devop@bastion ocp]$ oc project openshift-authentication Now using project \u0026#34;openshift-authentication\u0026#34; on server \u0026#34;https://api.ocp.ken.lab:6443\u0026#34;. [devop@bastion ocp]$ oc get po NAME READY STATUS RESTARTS AGE oauth-openshift-7885f5bd5b-4m5sh 1/1 Running 0 46m oauth-openshift-7885f5bd5b-q6vbk 1/1 Running 0 46m oauth-openshift-7885f5bd5b-vh4pz 1/1 Running 0 47m [devop@bastion ocp]$ oc rsh oauth-openshift-7885f5bd5b-4m5sh cat /run/secrets/kubernetes.io/serviceaccount/ca.crt \u0026gt; ca.crt ＃＃切換root [root@bastion ocp]# cp /home/devop/ocp/ca.crt /etc/pki/ca-trust/source/anchors/ [root@bastion ocp]# update-ca-trust extract ## 切換為devop測試登入 [devop@bastion ~]$ oc login -u admin -p redhat https://api.ocp.ken.lab:6443 Login successful. You have access to 66 projects, the list has been suppressed. You can list all projects with \u0026#39;oc projects\u0026#39; Using project \u0026#34;openshift-authentication\u0026#34;. 拆分Infra Node https://docs.openshift.com/container-platform/4.11/post_installation_configuration/cluster-tasks.html#post-install-cluster-tasks\nhttps://access.redhat.com/solutions/4601131\n針對infra node 建立label\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [devop@bastion ocp]$ oc label node infra1.ocp.ken.lab node-role.kubernetes.io/infra=\u0026#34;\u0026#34; node/infra1.ocp.ken.lab labeled [devop@bastion ocp]$ oc label node infra2.ocp.ken.lab node-role.kubernetes.io/infra=\u0026#34;\u0026#34; node/infra2.ocp.ken.lab labeled [devop@bastion ocp]$ oc label node infra3.ocp.ken.lab node-role.kubernetes.io/infra=\u0026#34;\u0026#34; node/infra3.ocp.ken.lab labeled [devop@bastion ocp]$ oc get nodes NAME STATUS ROLES AGE VERSION infra1.ocp.ken.lab Ready infra,worker 4h1m v1.24.0+3882f8f infra2.ocp.ken.lab Ready infra,worker 4h1m v1.24.0+3882f8f infra3.ocp.ken.lab Ready infra,worker 3h19m v1.24.0+3882f8f master1.ocp.ken.lab Ready master 4h43m v1.24.0+3882f8f master2.ocp.ken.lab Ready master 4h42m v1.24.0+3882f8f master3.ocp.ken.lab Ready master 4h41m v1.24.0+3882f8f worker1.ocp.ken.lab Ready worker 4h17m v1.24.0+3882f8f worker2.ocp.ken.lab Ready worker 4h17m v1.24.0+3882f8f 建立machine config pool \u0026raquo; infra.mcp.yaml 並apply到 cluster\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [devop@bastion ocp]$ vim infra.mcp.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: infra spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} nodeSelector: matchLabels: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; [devop@bastion ocp]$ oc apply -f infra.mcp.yaml machineconfigpool.machineconfiguration.openshift.io/infra created [devop@bastion ocp]$ oc get machineconfig NAME GENERATEDBYCONTROLLER IGNITIONVERSION AGE 00-master b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 5h12m 00-worker b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 5h12m 01-master-container-runtime b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 5h12m 01-master-kubelet b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 5h12m 01-worker-container-runtime b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 5h12m 01-worker-kubelet b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 5h12m 99-master-generated-registries b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 5h12m 99-master-ssh 3.2.0 5h29m 99-worker-generated-registries b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 5h12m 99-worker-ssh 3.2.0 5h29m rendered-infra-407eaba9df9f726f52f84b938fce9025 b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 3s rendered-master-720c11fac56674c6fc876ab6161295aa b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 5h12m rendered-worker-407eaba9df9f726f52f84b938fce9025 b7aa1d499f15ce7d211d30a823acf83f47aabbe3 3.2.0 5h12m 建立 machine config \u0026raquo; infra.mc.yaml 並apply到cluster\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [devop@bastion ocp]$ vim infra.mc.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: 51-infra labels: machineconfiguration.openshift.io/role: infra spec: config: ignition: version: 3.2.0 storage: files: - path: /etc/infratest mode: 0644 contents: source: data:,infra [devop@bastion ocp]$ oc create -f infra.mc.yaml machineconfig.machineconfiguration.openshift.io/51-infra created [devop@bastion ocp]$ oc get mcp NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-407eaba9df9f726f52f84b938fce9025 False True False 3 0 0 0 3m17s master rendered-master-720c11fac56674c6fc876ab6161295aa True False False 3 3 3 0 5h25m worker rendered-worker-407eaba9df9f726f52f84b938fce9025 True False False 2 2 2 0 5h25m Add a Taint To Infra Node ，使Infra Node不會被app部署到 #此動作可搬完operator後再做\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 oc adm taint nodes node1 node-role.kubernetes.io/infra:NoSchedule [devop@bastion ocp]$ oc adm taint nodes infra3.ocp.ken.lab node-role.kubernetes.io/infra:NoSchedule node/infra3.ocp.ken.lab tainted [devop@bastion ocp]$ oc adm taint nodes infra2.ocp.ken.lab node-role.kubernetes.io/infra:NoSchedule node/infra2.ocp.ken.lab tainted [devop@bastion ocp]$ oc adm taint nodes infra1.ocp.ken.lab node-role.kubernetes.io/infra:NoSchedule node/infra1.ocp.ken.lab tainted [devop@bastion ocp]$ oc describe nodes infra1 Name: infra1.ocp.ken.lab Roles: infra,worker Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=infra1.ocp.ken.lab kubernetes.io/os=linux node-role.kubernetes.io/infra= node-role.kubernetes.io/worker= node.openshift.io/os_id=rhcos Annotations: machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable machineconfiguration.openshift.io/currentConfig: rendered-infra-c36859c83539d92df71208a425c004df machineconfiguration.openshift.io/desiredConfig: rendered-infra-c36859c83539d92df71208a425c004df machineconfiguration.openshift.io/desiredDrain: uncordon-rendered-infra-c36859c83539d92df71208a425c004df machineconfiguration.openshift.io/lastAppliedDrain: uncordon-rendered-infra-c36859c83539d92df71208a425c004df machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state: Done volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Thu, 13 Oct 2022 11:26:50 +0800 Taints: node-role.kubernetes.io/infra:NoSchedule Unschedulable: false Lease: HolderIdentity: infra1.ocp.ken.lab AcquireTime: \u0026lt;unset\u0026gt; RenewTime: Thu, 13 Oct 2022 16:23:54 +0800 刪除infra node的 worker label\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [devop@bastion ocp]$ oc edit nodes infra1.ocp.ken.lab node/infra1.ocp.ken.lab edited labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux kubernetes.io/arch: amd64 kubernetes.io/hostname: infra2.ocp.ken.lab kubernetes.io/os: linux node-role.kubernetes.io/infra: \u0026#34;\u0026#34; node-role.kubernetes.io/worker: \u0026#34;\u0026#34; \u0026gt;\u0026gt;\u0026gt;\u0026gt; 刪除此行 node.openshift.io/os_id: rhcos [devop@bastion ocp]$ oc get nodes NAME STATUS ROLES AGE VERSION infra1.ocp.ken.lab Ready infra 22h v1.24.0+3882f8f infra2.ocp.ken.lab Ready infra 22h v1.24.0+3882f8f infra3.ocp.ken.lab Ready infra 22h v1.24.0+3882f8f master1.ocp.ken.lab Ready master 23h v1.24.0+3882f8f master2.ocp.ken.lab Ready master 23h v1.24.0+3882f8f master3.ocp.ken.lab Ready master 23h v1.24.0+3882f8f worker1.ocp.ken.lab Ready worker 22h v1.24.0+3882f8f worker2.ocp.ken.lab Ready worker 22h v1.24.0+3882f8f Operator移動到Infra Node Router 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 方法一 [devop@bastion ocp]$ oc get ingresscontroller default -n openshift-ingress-operator -o yaml \u0026gt; router.yaml [devop@bastion ocp]$ vim router.yaml [devop@bastion ocp]$ cat router.yaml apiVersion: operator.openshift.io/v1 kind: IngressController metadata: creationTimestamp: \u0026#34;2022-10-13T02:54:05Z\u0026#34; finalizers: - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller generation: 1 name: default namespace: openshift-ingress-operator resourceVersion: \u0026#34;26400\u0026#34; uid: 1d5c3f18-0f53-4140-be74-35df5edf4c8b spec: clientTLS: clientCA: name: \u0026#34;\u0026#34; clientCertificatePolicy: \u0026#34;\u0026#34; httpCompression: {} httpEmptyRequestsPolicy: Respond httpErrorCodePages: name: \u0026#34;\u0026#34; replicas: 2 tuningOptions: {} unsupportedConfigOverrides: null nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; status: availableReplicas: 2 conditions: - lastTransitionTime: \u0026#34;2022-10-13T02:54:13Z\u0026#34; reason: Valid status: \u0026#34;True\u0026#34; type: Admitted - lastTransitionTime: \u0026#34;2022-10-13T03:12:03Z\u0026#34; status: \u0026#34;True\u0026#34; type: PodsScheduled - lastTransitionTime: \u0026#34;2022-10-13T03:11:54Z\u0026#34; message: The deployment has Available status condition set to True reason: DeploymentAvailable status: \u0026#34;True\u0026#34; type: DeploymentAvailable - lastTransitionTime: \u0026#34;2022-10-13T03:11:54Z\u0026#34; message: Minimum replicas requirement is met reason: DeploymentMinimumReplicasMet status: \u0026#34;True\u0026#34; type: DeploymentReplicasMinAvailable - lastTransitionTime: \u0026#34;2022-10-13T03:12:36Z\u0026#34; message: All replicas are available reason: DeploymentReplicasAvailable status: \u0026#34;True\u0026#34; type: DeploymentReplicasAllAvailable - lastTransitionTime: \u0026#34;2022-10-13T02:54:13Z\u0026#34; message: The configured endpoint publishing strategy does not include a managed load balancer reason: EndpointPublishingStrategyExcludesManagedLoadBalancer status: \u0026#34;False\u0026#34; type: LoadBalancerManaged - lastTransitionTime: \u0026#34;2022-10-13T02:54:13Z\u0026#34; message: No DNS zones are defined in the cluster dns config. reason: NoDNSZones status: \u0026#34;False\u0026#34; type: DNSManaged - lastTransitionTime: \u0026#34;2022-10-13T03:11:54Z\u0026#34; status: \u0026#34;True\u0026#34; type: Available - lastTransitionTime: \u0026#34;2022-10-13T02:54:13Z\u0026#34; status: \u0026#34;False\u0026#34; type: Progressing - lastTransitionTime: \u0026#34;2022-10-13T03:12:03Z\u0026#34; status: \u0026#34;False\u0026#34; type: Degraded - lastTransitionTime: \u0026#34;2022-10-13T02:54:13Z\u0026#34; message: IngressController is upgradeable. reason: Upgradeable status: \u0026#34;True\u0026#34; type: Upgradeable - lastTransitionTime: \u0026#34;2022-10-13T03:12:02Z\u0026#34; message: Canary route checks for the default ingress controller are successful reason: CanaryChecksSucceeding status: \u0026#34;True\u0026#34; type: CanaryChecksSucceeding domain: apps.ocp.ken.lab endpointPublishingStrategy: hostNetwork: httpPort: 80 httpsPort: 443 protocol: TCP statsPort: 1936 type: HostNetwork observedGeneration: 1 selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default tlsProfile: ciphers: - ECDHE-ECDSA-AES128-GCM-SHA256 - ECDHE-RSA-AES128-GCM-SHA256 - ECDHE-ECDSA-AES256-GCM-SHA384 - ECDHE-RSA-AES256-GCM-SHA384 - ECDHE-ECDSA-CHACHA20-POLY1305 - ECDHE-RSA-CHACHA20-POLY1305 - DHE-RSA-AES128-GCM-SHA256 - DHE-RSA-AES256-GCM-SHA384 - TLS_AES_128_GCM_SHA256 - TLS_AES_256_GCM_SHA384 - TLS_CHACHA20_POLY1305_SHA256 minTLSVersion: VersionTLS12 [devop@bastion ocp]$ oc apply -f router.yaml -n openshift-ingress-operator Warning: resource ingresscontrollers/default is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by oc apply. oc apply should only be used on resources created declaratively by either oc create --save-config or oc apply. The missing annotation will be patched automatically. ingresscontroller.operator.openshift.io/default configured 方法二 （官方建議） [devop@bastion ocp]$ oc edit ingresscontroller -n openshift-ingress-operator ####加入下方 matchLabels spec: nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; #### 確認ingress-operator已分配到Infra Node [devop@bastion ocp]$ oc get pod -n openshift-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-5c67b4db8b-75pjr 1/1 Running 0 5h44m 192.168.33.26 worker1.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-5c67b4db8b-glwcq 0/1 Terminating 0 5h44m 192.168.33.27 worker2.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-7c4cc569cc-85sbm 0/1 Pending 0 28s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [devop@bastion ocp]$ oc get pod -n openshift-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-7c4cc569cc-86ggk 1/1 Running 0 18m 192.168.33.32 infra2.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-7c4cc569cc-97mlv 1/1 Running 0 49m 192.168.33.31 infra1.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Registry 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 [devop@bastion ocp]$ oc get configs.imageregistry.operator.openshift.io/cluster -o yaml [devop@bastion ocp]$ oc edit configs.imageregistry.operator.openshift.io/cluster spec: logLevel: Normal managementState: Removed observedConfig: null operatorLogLevel: Normal proxy: {} replicas: 1 requests: read: maxWaitInQueue: 0s write: maxWaitInQueue: 0s rolloutStrategy: RollingUpdate storage: {} unsupportedConfigOverrides: null nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; [devop@bastion ocp]$ oc get pods -o wide -n openshift-image-registry NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cluster-image-registry-operator-598c5664bd-qz8cs 1/1 Running 0 23h 10.128.0.34 master1.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; image-pruner-27761760-jh4fz 0/1 Completed 0 101m 10.131.2.8 infra3.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-ca-cdt8j 1/1 Running 0 22h 192.168.33.23 master1.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-ca-dpzzb 1/1 Running 1 22h 192.168.33.32 infra2.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-ca-hfv8c 1/1 Running 1 21h 192.168.33.33 infra3.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-ca-hjptk 1/1 Running 1 22h 192.168.33.31 infra1.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-ca-k2qhs 1/1 Running 0 22h 192.168.33.24 master2.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-ca-lgkg8 1/1 Running 0 22h 192.168.33.27 worker2.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-ca-wqpn8 1/1 Running 0 22h 192.168.33.25 master3.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-ca-xt8dg 1/1 Running 0 22h 192.168.33.26 worker1.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Monitoring Include: Prometheus, Thanos Querier, and Alertmanager 先建立ConfigMap cluster-monitor-config.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 [devop@bastion ocp]$ oc get pod -n openshift-monitoring -o wide [devop@bastion ocp]$ vim cluster-monitoring-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; openshiftStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; thanosQuerier: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; --- 建立此ConfigMap [devop@bastion ocp]$ oc create -f cluster-monitoring-config.yaml configmap/cluster-monitoring-config created --- 查看pod 是否都移轉至infra node，若未移轉則delete該pod，使其重新部署 node-exporter、cluster-monitoring-operator不用刪除 [devop@bastion ocp]$ watch oc get pod -n openshift-monitoring -o wide [devop@bastion ocp]$ oc get pod -n openshift-monitoring -o wide |grep worker [devop@bastion ocp]$ oc get pod -n openshift-monitoring -o wide |grep master cluster-monitoring-operator-574c5f5b7b-sg6nh 2/2 Running 0 23h 10.128.0.33 master1.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-exporter-hr22c 2/2 Running 0 22h 192.168.33.23 master1.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-exporter-jnqkc 2/2 Running 0 22h 192.168.33.25 master3.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; node-exporter-lzk6n 2/2 Running 0 22h 192.168.33.24 master2.ocp.ken.lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 問題排除 調整 OperatorHub Blank 畫面空白 在install-config.yaml 裡面就要加入 marketplace的參數，sample是用來起template、image stream的operator install-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [devop@bastion ocp]$ vim install-config.yaml [devop@bastion ocp]$ cat install-config.yaml apiVersion: v1 baseDomain: ken.lab compute: - hyperthreading: Enabled name: worker replicas: 2 controlPlane: hyperthreading: Enabled name: master replicas: 3 metadata: name: ocp networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: none: {} fips: false pullSecret: \u0026#39;{\u0026#34;auths\u0026#34;:{\u0026#34;cloud.openshift.com\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;b3BlbnNoaWZ0LXJlbGVhc2UtZGV2K29jbV9hY2Nlc3NfMzEwNjVjNmVmMTk5NDZmNWEwNmUzZmI4MTVmNzk2OTE6NEJRVk1FVk9CTDRZQVRBT0pONEhQVkIyUExFQTdKNDE3WExFU1IzVDk3NU1YUTJXWE9FSjBNTVk0MzFQQk9RUA==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;},\u0026#34;quay.io\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;b3BlbnNoaWZ0LXJlbGVhc2UtZGV2K29jbV9hY2Nlc3NfMzEwNjVjNmVmMTk5NDZmNWEwNmUzZmI4MTVmNzk2OTE6NEJRVk1FVk9CTDRZQVRBT0pONEhQVkIyUExFQTdKNDE3WExFU1IzVDk3NU1YUTJXWE9FSjBNTVk0MzFQQk9RUA==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;},\u0026#34;registry.connect.redhat.com\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;fHVoYy1wb29sLTc5MzgwY2YyLTUxNTEtNGRiNi04YjA3LTRiNzQ4NDcxNWE2NDpleUpoYkdjaU9pSlNVelV4TWlKOS5leUp6ZFdJaU9pSmxOak5rWVdWbU1tSXlaRFEwTVdKaVlUTXpObVUwTkdGbVl6WmtPVEUwWVNKOS5RMlp6Q1JaWWltVDIxM2dpYU9CRWtNWEVkYUtjbnc4WHN1bTY3VEVOdjEyRjBjQUgyNWNZMVdWTjZNUEY2R21CNjh3TmF4TnoxRHVpVnBtY0ZKM2JZTW0yNVVzZGJxeFc4cEd1dGszSzF5NlNUWk9VejlkU0V4UGg4aUxha05qSEQ4Tml6emNmM1lUQjRNUXdhYTVFSG1PMXVTUERUZFRBTXFYS3FFa2gwRXpHOUpKZTlUMnM4bVJXWkVIbndoZW84cktncVZuV1VCeGhqTldFNElfWG1JZnBPRl91akpFNHdXNzRJWjd5UmdoUGExQVVwS3BsY0NJQ2ZGdGtaS29WUTFwNTBzUzNabkxWYjd6ZHdXR21WcXVsNUFoZUNiSHNoNEtpSjFnYXRSbkwtMG92THZoNGdseUZqd0c0Z3lMMEdyS1ItbU5WMFZYc01mVzJDNXNVaXBzVmhKcmhmY01vQjRNSDRDbjZKcjJXS2plbEdqSVdHQzBUV1BBNlZQZDBmc0lyLTRCY2RBdUFmcm5SaWpGM3d0Mng3RFdXQ1RScFVPbUVMY3U5bDR6Wm5uZkVSejNMd01rNmhCWTFyYXV1TGhCWXJraFlzQURqWEl3bGxPTkwyc01GMUU2OVNIVG1vV0tvNU5fdHdoWE5fX1BtT1ozMzc5ZVNDVGdnTFNZd3hyR1VwS3FXRDFva3JWZlRIWmNlcTk4WHQ2QjFtZHIxcmxLV2RiRmhsRDQ3Q1pldEhrNW1xQUlianp1UzlONENJWllhd0lIZE5LWVotN2tRNFQyUFg4QWdadHFNdHdjY2c0bWx3ME40c09LbWVZN2Q2bGw0bXpXV201bVo0aktTeHdpYmJ3STEwOFcxWVFlSlQ4QW9WZFlkc09YWkdWc3ZxZVVZOU9qbzR2RQ==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;},\u0026#34;registry.redhat.io\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;fHVoYy1wb29sLTc5MzgwY2YyLTUxNTEtNGRiNi04YjA3LTRiNzQ4NDcxNWE2NDpleUpoYkdjaU9pSlNVelV4TWlKOS5leUp6ZFdJaU9pSmxOak5rWVdWbU1tSXlaRFEwTVdKaVlUTXpObVUwTkdGbVl6WmtPVEUwWVNKOS5RMlp6Q1JaWWltVDIxM2dpYU9CRWtNWEVkYUtjbnc4WHN1bTY3VEVOdjEyRjBjQUgyNWNZMVdWTjZNUEY2R21CNjh3TmF4TnoxRHVpVnBtY0ZKM2JZTW0yNVVzZGJxeFc4cEd1dGszSzF5NlNUWk9VejlkU0V4UGg4aUxha05qSEQ4Tml6emNmM1lUQjRNUXdhYTVFSG1PMXVTUERUZFRBTXFYS3FFa2gwRXpHOUpKZTlUMnM4bVJXWkVIbndoZW84cktncVZuV1VCeGhqTldFNElfWG1JZnBPRl91akpFNHdXNzRJWjd5UmdoUGExQVVwS3BsY0NJQ2ZGdGtaS29WUTFwNTBzUzNabkxWYjd6ZHdXR21WcXVsNUFoZUNiSHNoNEtpSjFnYXRSbkwtMG92THZoNGdseUZqd0c0Z3lMMEdyS1ItbU5WMFZYc01mVzJDNXNVaXBzVmhKcmhmY01vQjRNSDRDbjZKcjJXS2plbEdqSVdHQzBUV1BBNlZQZDBmc0lyLTRCY2RBdUFmcm5SaWpGM3d0Mng3RFdXQ1RScFVPbUVMY3U5bDR6Wm5uZkVSejNMd01rNmhCWTFyYXV1TGhCWXJraFlzQURqWEl3bGxPTkwyc01GMUU2OVNIVG1vV0tvNU5fdHdoWE5fX1BtT1ozMzc5ZVNDVGdnTFNZd3hyR1VwS3FXRDFva3JWZlRIWmNlcTk4WHQ2QjFtZHIxcmxLV2RiRmhsRDQ3Q1pldEhrNW1xQUlianp1UzlONENJWllhd0lIZE5LWVotN2tRNFQyUFg4QWdadHFNdHdjY2c0bWx3ME40c09LbWVZN2Q2bGw0bXpXV201bVo0aktTeHdpYmJ3STEwOFcxWVFlSlQ4QW9WZFlkc09YWkdWc3ZxZVVZOU9qbzR2RQ==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ken.wang@infotech.com.tw\u0026#34;}}}\u0026#39; sshKey: \u0026#39;ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIN1Z2jXmGnPWGAVrtltN9A2q5D8QtJlRvhvdB2QXoZG devop@bastion.ocp.ken.lab\u0026#39; capabilities: baselineCapabilitySet: None additionalEnabledCapabilities: - marketplace ","date":"2023-09-23T08:18:45+08:00","permalink":"https://wangken0129.github.io/p/ocp-4.11.7_on_bare-metal/","title":"OCP-4.11.7_on_Bare-metal"},{"content":"Install Redhat Quay \u0026amp; Clair 安裝單節點的Quay跟Clair在同一台VM上面執行，需先裝好Clair再安裝Quay。\n參考文件 1.原廠文件：https://access.redhat.com/documentation/en-us/red_hat_quay/2.9/html/manage_red_hat_quay/clair-initial-setup\n2.原廠文件：https://access.redhat.com/documentation/zh-tw/red_hat_quay/3.7/html/manage_red_hat_quay/clair-intro2\n3.quay\u0026amp;clair: https://examples.openshift.pub/quay/ https://www.modb.pro/db/109809 \u0026ndash;\u0026gt; 本篇參考 https://docs.projectquay.io/deploy_quay.html\n架構 (單節點) Clair 說明 OS_User root、devop / redhat OS RHEL8.6 OS_IP 192.168.33.34 CPU、RAM 4vCPU、16GB Ram、100GB Disk HostName quay.ocp.ken.lab WorkDir /home/devop/clair Postgresdb clairuser / welcome1 / clairdb Clair v3.5.0 Quay v3.7.3 clairpod -p 5433:5432 -p 8081:8081 -p 8089:8089 Quay 說明 OS_User devop OS RHEL8.5 OS_IP 192.168.3.28 WorkDir /home/devop Postgresdb quayuser / welcome1 quay admin quayadmin / welcome1 redis password welcome1 quay-pod port 80:8080、3306:3306、5432:5432 Quay Version v3.7.3 Wordpresspod 8989:80、3307:3306 安裝步驟 OS 設定 Users 1 2 [root@quay ~]# useradd devop [root@quay ~]# passwd devop Disk 1 2 3 4 5 6 7 8 9 10 11 12 [root@quay ~]# df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 7.8G 0 7.8G 0% /dev tmpfs 7.8G 0 7.8G 0% /dev/shm tmpfs 7.8G 8.7M 7.8G 1% /run tmpfs 7.8G 0 7.8G 0% /sys/fs/cgroup /dev/mapper/rhel-root 61G 2.8G 59G 5% / /dev/sda2 1014M 207M 808M 21% /boot /dev/mapper/rhel-home 30G 245M 30G 1% /home /dev/sda1 599M 5.9M 594M 1% /boot/efi tmpfs 1.6G 0 1.6G 0% /run/user/0 tmpfs 1.6G 0 1.6G 0% /run/user/1000 NetWork 1 2 3 4 5 6 7 8 9 10 [root@quay ~]# nmcli c s NAME UUID TYPE DEVICE ens192 7e9a88db-859f-4580-bfc9-9fbe9c5985c6 ethernet ens192 [root@quay ~]# nmcli c mod ens192 ipv4.method manual ipv4.address 192.168.33.34/24 ipv4.gateway 192.168.33.251 ipv4.dns 168.95.1.1 [root@quay ~]# nmcli c u ens192 [root@quay ~]# cat /etc/resolv.conf # Generated by NetworkManager search ocp.ken.lab nameserver 192.168.33.21 nameserver 168.95.1.1 sudo 1 2 [root@quay ~]# cat /etc/sudoers.d/devop devop ALL=(ALL) NOPASSWD: ALL subsciption 1 2 3 4 5 6 root@quay ~]# subscription-manager register Registering to: subscription.rhsm.redhat.com:443/subscription Username: ken.wang@infotech.com.tw Password: The system has been registered with ID: 77693b7f-9bbe-412d-9e15-210c55e799ef The registered system name is: quay.ocp.ken.lab yum 1 2 3 4 5 [root@quay ~]# yum repolist Updating Subscription Management repositories. repo id repo name rhel-8-for-x86_64-appstream-rpms Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs) rhel-8-for-x86_64-baseos-rpms Red Hat Enterprise Linux 8 for x86_64 - BaseOS (RPMs) Install Podman 1 2 3 [root@quay ~]# yum install podman -y [root@quay ~]# podman -v podman version 4.1.1 環境設定 建立 Clair_pod\n1 2 3 [devop@uat-quay ~]$ cat create_clairpod.sh podman pod create --name=clairpod -p 5433:5432 -p 8081:8081 -p 8089:8089 [devop@uat-quay ~]$ ./create_clairpod.sh 建立 Quay_pod\n1 2 3 4 [devop@quay quay]$ cat createpod.sh podman pod create --name=quaypod -p 80:8080 -p 443:8443 -p 6379:6379 -p 5432:5432 [devop@quay quay]$ ./createpod.sh f3fcbd51ffa7454c4e8825806a07252a9072ed7d26ad092e6bd21eb992033948 設定防火牆、SELinux\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [devop@quay quay]$ sudo systemctl disable firewalld --now [devop@quay quay]$ sudo su - [root@quay ~]# setenforce 0 [root@quay ~]# vim /etc/selinux/config [root@quay ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled # SELINUXTYPE= can take one of these three values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE=targeted 調整podman port binding權限\n1 2 3 sudo su - echo \u0026#34;net.ipv4.ip_unprivileged_port_start = 80\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf [root@quay ~]# reboot podman 登入 registry.redhat.io\n1 2 3 4 [devop@quay clair]$ podman login registry.redhat.io Username: ken.wang@infotech.com.tw Password: Login Succeeded! podman 登入 安裝quay時使用\n1 2 3 [devop@quay ~]$ cat podmanlogin.sh #!/bin/bash podman login -u=\u0026#34;redhat+quay\u0026#34; -p=\u0026#34;O81WSHRSJR14UAZBK54GQHJS0P1V4CLWAJV1X2C4SD7KO59CQ9N3RE12612XU1HR\u0026#34; quay.io 產生CA\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 [devop@quay certs]$ cat gencert.sh #!/bin/bash # Create certificate given a specific url name if [ $# -ne 1 ] then echo \u0026#34;No arguments supplied or too many arguments\u0026#34; echo \u0026#34;Usage: $0 domain_name\u0026#34; exit 1 fi MYNAME=$1 echo \u0026#34;Generating a private key...for $MYNAME\u0026#34; openssl genrsa -out $MYNAME.key 4096 echo \u0026#34;Generating a CSR...for $MYNAME\u0026#34; openssl req -new -key $MYNAME.key \\ -out $MYNAME.csr \\ -subj \u0026#34;/C=TW/ST=TW/L=Taipei/O=KEN/OU=LAB/CN=$MYNAME\u0026#34; echo \u0026#34;Generating a certificate...for $MYNAME\u0026#34; openssl x509 -req -days 366 -in \\ $MYNAME.csr -signkey \\ $MYNAME.key \\ -out $MYNAME.crt -extensions v3_req -extfile quay.cnf --- [devop@quay certs]$ cat quay.cnf [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = quay.ocp.ken.lab DNS.2 = quay --- [devop@quay ~]$ ./gencert.sh quay.ocp.ken.lab Generating a private key...for quay.ocp.ken.lab Generating RSA private key, 2048 bit long modulus (2 primes) .....+++++ .................................+++++ e is 65537 (0x010001) Generating a CSR...for quay.ocp.ken.lab Generating a certificate...for quay.ocp.ken.lab Signature ok subject=C = TW, ST = TW, L = Taipei, O = KEN, OU = LAB, CN = quay.ocp.ken.lab Getting Private key --- [devop@quay certs]$ ll total 20 -rwxrwxr-x 1 devop devop 600 Oct 17 17:13 gencert.sh -rw-rw-r-- 1 devop devop 279 Oct 17 17:12 quay.cnf -rw-rw-r-- 1 devop devop 1988 Oct 17 17:14 quay.ocp.ken.lab.crt -rw-rw-r-- 1 devop devop 1691 Oct 17 17:14 quay.ocp.ken.lab.csr -rw------- 1 devop devop 3243 Oct 17 17:14 quay.ocp.ken.lab.key --- [devop@quay certs]$ sudo cp quay.ocp.ken.lab.crt /etc/pki/ca-trust/source/anchors/ [devop@quay certs]$ sudo update-ca-trust [devop@quay certs]$ 安裝DB Postgresql 建立Clair_pgsql目錄\n1 2 [devop@uat-quay ~]$ mkdir -p /home/devop/var/lib/pgsql_clair/ [devop@uat-quay ~]$ chmod 777 /home/devop/var/lib/pgsql_clair/ 建立Quay_pgsql目錄\n1 2 [devop@uat-quay ~]$ mkdir -p /home/devop/var/lib/pgsql [devop@uat-quay ~]$ chmod 777 /home/devop/var/lib/pgsql 啟動Clair_pgsql\n1 2 3 4 5 6 7 8 9 10 [devop@uat-quay clair]$ cat start_pgsql_clair.sh #!/bin/bash podman run -d --pod=clairpod --name=clair-pgsql \\ -e POSTGRESQL_ADMIN_PASSWORD=welcome1 \\ -e POSTGRESQL_USER=clairuser \\ -e POSTGRESQL_PASSWORD=welcome1 \\ -e POSTGRESQL_DATABASE=clairdb \\ -v /home/devop/var/lib/pgsql_clair:/var/lib/pgsql/data:Z \\ registry.redhat.io/rhel8/postgresql-10:1 [devop@uat-quay clair]$ ./start_pgsql_clair.sh 建立Clairdb Extension\n1 2 3 4 5 [devop@uat-quay clair]$ cat test_pgsql_clair.sh #!/bin/bash podman exec -it clair-pgsql /bin/bash -c \u0026#39;echo \u0026#34;CREATE EXTENSION IF NOT EXISTS \\\u0026#34;uuid-ossp\\\u0026#34;\u0026#34; | psql -d clairdb -U postgres\u0026#39; [devop@uat-quay clair]$ ./test_pgsql_clair.sh CREATE EXTENSION 啟動Quay_pgsql\n1 2 3 4 5 6 7 8 9 [devop@quay quay]$ cat start_pgsql.sh #!/bin/bash podman run -d --pod=quaypod --name=quay-pgsql \\ -e POSTGRESQL_ADMIN_PASSWORD=welcome1 \\ -e POSTGRESQL_USER=quayuser \\ -e POSTGRESQL_PASSWORD=welcome1 \\ -e POSTGRESQL_DATABASE=quaydb \\ -v /home/devop/var/lib/pgsql:/var/lib/pgsql/data:Z \\ registry.redhat.io/rhel8/postgresql-10:1 建立 quaydb Extension\n1 [devop@quay quay]$ podman exec -it quay-pgsql /bin/bash -c \u0026#39;echo \u0026#34;CREATE EXTENSION IF NOT EXISTS pg_trgm\u0026#34; | psql -d quaydb -U postgres\u0026#39; 安裝Redis 建立redis資料夾\n1 2 mkdir -p /home/devop/var/lib/redis chmod 777 /home/devop/var/lib/redis 安裝redis\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [devop@uat-quay ~]$ cat start_redis.sh #!/bin/bash podman run -d --pod=quaypod --name=quay-redis \\ -v /home/devop/var/lib/redis:/var/lib/redis/data:Z \\ -e REDIS_PASSWORD=welcome1 \\ registry.redhat.io/rhel8/redis-5:1 [devop@quay quay]$ ./start_redis.sh Trying to pull registry.redhat.io/rhel8/redis-5:1... Getting image source signatures Checking if image destination supports signatures Copying blob 8b33c6fa797f done Copying blob f70d60810c69 done Copying blob 545277d80005 done Copying blob 399eb2b7226e done Copying config c9df129d49 done Writing manifest to image destination Storing signatures c2576cc7542ec7637d0fa044c53adc1d2123ffdc99f851901f891084301d579c 確認container 狀態\n1 podman ps 配置Quay Config podman 登入quay.io\n1 2 3 4 5 6 [devop@quay ~]$ cat podmanlogin.sh #!/bin/bash podman login -u=\u0026#34;redhat+quay\u0026#34; -p=\u0026#34;O81WSHRSJR14UAZBK54GQHJS0P1V4CLWAJV1X2C4SD7KO59CQ9N3RE12612XU1HR\u0026#34; quay.io --- [devop@quay ~]$ ./podmanlogin.sh Login Succeeded! 安裝quay-config\n1 2 3 4 [devop@uat-quay ~]$ cat start_quay_config.sh #!/bin/bash podman run --rm -it --pod=quaypod --name=quay-config \\ registry.redhat.io/quay/quay-rhel8:v3.7.3 config welcome1 連線網頁 http://quay.ocp.ken.lab quayconfig welcome1\n左下角共有五項需設定 Hostname、DB、Redis，上傳憑證\n設定Server Hostname \u0026amp; TLS : quay.ocp.ken.lab 設定DB 資訊\n1 2 3 4 Database Server: quay.ocp.ken.lab:5432 Username:\tquayuser Password:\twelcome1 Database Name: quaydb 設定redis Hostname\n1 2 3 Redis Hostname: quay.ocp.ken.lab Redis Port: 6379 Redis password: welcome1 Enable Security Scanner ##### Enable Security Scanner \u0026amp; Generate PSK PSK: MTg4MDJnaTQ3OWRjYw== 保存設定並下載設定檔，並關閉quay-config\n解壓縮Config 下載後解壓縮\n1 2 3 4 5 [devop@quay quay]$ tar xvf quay-config.tar.gz extra_ca_certs/ config.yaml ssl.cert ssl.key 設定 Clair並啟動 建立Clair目錄\n1 2 [devop@uat-quay ~]$ mkdir -p /home/devop/mnt/clair/config [devop@uat-quay ~]$ chmod 777 /home/devop/mnt/clair/config 配置clair的config.yaml \u0026raquo; 要在quay_config階段時Enable Security Scanner 並Generate PSK\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [devop@uat-quay ~]$ cat prekey.txt MTg4MDJnaTQ3OWRjYw== [devop@uat-quay clair]$ vim config.yaml http_listen_addr: :8081 introspection_addr: :8089 log_level: info indexer: connstring: host=quay.ocp.ken.lab port=5433 dbname=clairdb user=clairuser password=welcome1 sslmode=disable scanlock_retry: 10 layer_scan_concurrency: 5 migrations: true matcher: connstring: host=quay.ocp.ken.lab port=5433 dbname=clairdb user=clairuser password=welcome1 sslmode=disable max_conn_pool: 100 run: \u0026#34;\u0026#34; migrations: true indexer_addr: clair-indexer notifier: connstring: host=quay.ocp.ken.lab port=5433 dbname=clairdb user=clairuser password=welcome1 sslmode=disable delivery_interval: 1m poll_interval: 5m migrations: true auth: psk: key: \u0026#34;MTg4MDJnaTQ3OWRjYw==\u0026#34; iss: [\u0026#34;quay\u0026#34;] --- [devop@uat-quay clair]$ cp config /home/devop/mnt/clair/config/config.yaml 啟動Clair ( 先停止Quay )\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [devop@uat-quay clair]$ podman login registry.redhat.io Username: ken.wang@infotech.com.tw Password: Login Succeeded! [devop@quay clair]$ cat start_clair.sh #!/bin/bash podman run -d --pod clairpod --name clair -e CLAIR_CONF=/clair/config.yaml \\ -e CLAIR_MODE=combo -e SSL_CERT_DIR=/etc/pki/tls/certs \\ -v /home/devop/mnt/clair/config:/clair:Z \\ -v /home/devop/mnt/quay/certs/quay.ocp.ken.lab.crt:/etc/pki/tls/certs/quay.ocp.ken.lab:Z \\ registry.redhat.io/quay/clair-rhel8:v3.5.0 [devop@quay clair]$ ./start_clair.sh Trying to pull registry.redhat.io/quay/clair-rhel8:v3.5.0... Getting image source signatures Checking if image destination supports signatures Copying blob d821702c1e09 done Copying blob 13897c84ca57 done Copying blob 64607cc74f9c done Copying config 10e4c3931b done Writing manifest to image destination Storing signatures 4d428098362e17ae606dad5412756f880ab40420af4f7a376170242064963154 確認container狀態\n1 2 3 4 5 6 7 [devop@quay clair]$ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES eb65b55e9875 localhost/podman-pause:4.1.1-1657551413 About an hour ago Up About an hour ago 0.0.0.0:5433-\u0026gt;5432/tcp, 0.0.0.0:8081-\u0026gt;8081/tcp, 0.0.0.0:8089-\u0026gt;8089/tcp 535faabcd216-infra 4b465bef2476 registry.redhat.io/rhel8/postgresql-10:1 run-postgresql About an hour ago Up About an hour ago 0.0.0.0:5433-\u0026gt;5432/tcp, 0.0.0.0:8081-\u0026gt;8081/tcp, 0.0.0.0:8089-\u0026gt;8089/tcp clair-pgsql 154a0b6a0803 localhost/podman-pause:4.1.1-1657551413 About an hour ago Up About an hour ago 0.0.0.0:80-\u0026gt;8080/tcp, 0.0.0.0:443-\u0026gt;8443/tcp, 0.0.0.0:5432-\u0026gt;5432/tcp, 0.0.0.0:6379-\u0026gt;6379/tcp da7ca63d2ac6-infra 518b5db0591a registry.redhat.io/rhel8/postgresql-10:1 run-postgresql About an hour ago Up About an hour ago 0.0.0.0:80-\u0026gt;8080/tcp, 0.0.0.0:443-\u0026gt;8443/tcp, 0.0.0.0:5432-\u0026gt;5432/tcp, 0.0.0.0:6379-\u0026gt;6379/tcp quay-pgsql c2576cc7542e registry.redhat.io/rhel8/redis-5:1 run-redis 46 minutes ago Up 46 minutes ago 0.0.0.0:80-\u0026gt;8080/tcp, 0.0.0.0:443-\u0026gt;8443/tcp, 0.0.0.0:5432-\u0026gt;5432/tcp, 0.0.0.0:6379-\u0026gt;6379/tcp quay-redis 確認更新完成即可啟動Quay (約3分鐘)\n1 2 3 4 [devop@uat-quay clair]$ podman logs --tail 10 clair {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;libvuln/New\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;libvuln/updateDriver/RunUpdaters\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;internal/updater/Executor\u0026#34;,\u0026#34;run_id\u0026#34;:1879968118,\u0026#34;updater\u0026#34;:\u0026#34;photon-updater-photon3\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;photon/Updater.Parse\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-09-14T05:21:38Z\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-09-14T05:21:38Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;starting parse\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;libvuln/New\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;libvuln/updateDriver/RunUpdaters\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;internal/updater/Executor\u0026#34;,\u0026#34;run_id\u0026#34;:1879968118,\u0026#34;updater\u0026#34;:\u0026#34;RHEL8-advanced-virtualization\u0026#34;,\u0026#34;ref\u0026#34;:\u0026#34;0f18ac9b-223f-460d-96ed-08ea268e278e\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-09-14T05:22:38Z\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-09-14T05:22:38Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;successful update\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;libvuln/New\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;libvuln/updateDriver/RunUpdaters\u0026#34;,\u0026#34;component\u0026#34;:\u0026#34;internal/updater/Executor\u0026#34;,\u0026#34;run_id\u0026#34;:1879968118,\u0026#34;updater\u0026#34;:\u0026#34;photon-updater-photon1\u0026#34;,\u0026#34;ref\u0026#34;:\u0026#34;b1ea50fa-b89c-4f94-a182-a65a9284125a\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-09-14T05:22:48Z\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-09-14T05:22:48Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;successful update\u0026#34;} 啟動Quay 啟動Quay\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ##建立quay資料夾 並將config 、 CA 放到 [devop@quay quay]$ mkdir -p /home/devop/mnt/quay/storage [devop@quay quay]$ mkdir -p /home/devop/mnt/quay/storage [devop@quay quay]$ mkdir -P /home/devop/mnt/quay/certs/ [devop@quay quay]$ chmod 777 /home/devop/mnt/quay/storage [devop@quay quay]$ chmod 777 /home/devop/mnt/quay/config [devop@quay quay]$ chmod 777 /home/devop/mnt/quay/certs [devop@quay quay]$ cp config.yaml /home/devop/mnt/quay/config/ [devop@quay certs]$ cp quay.ocp.ken.lab.* /home/devop/mnt/quay/certs [devop@quay config]$ ll total 16 -rwxrwxr-x 1 devop devop 2365 Jan 1 1970 config.yaml drwxrwxr-x 2 devop devop 6 Jan 1 1970 extra_ca_certs -rw-r--r-- 1 devop devop 3312 Oct 14 16:10 quay-config.tar.gz -rwxrwxr-x 1 devop devop 1204 Jan 1 1970 ssl.cert -rwxrwxr-x 1 devop devop 1679 Jan 1 1970 ssl.key [devop@quay config]$ cd [devop@uat-quay ~]$ cat start_quay.sh #!/bin/bash podman run -d --pod=quaypod --name=quay \\ -v /home/devop/mnt/quay/config:/conf/stack:Z \\ -v /home/devop/mnt/quay/storage:/datastorage:Z \\ registry.redhat.io/quay/quay-rhel8:v3.7.3 [devop@uat-quay ~]$ ./start_quay.sh 5f1915f2038f4167fd6e37c4233903dbad7de00acd6e753957ed9ff66b2d8ba2 [devop@quay quay]$ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES eb65b55e9875 localhost/podman-pause:4.1.1-1657551413 About an hour ago Up About an hour ago 0.0.0.0:5433-\u0026gt;5432/tcp, 0.0.0.0:8081-\u0026gt;8081/tcp, 0.0.0.0:8089-\u0026gt;8089/tcp 535faabcd216-infra 4b465bef2476 registry.redhat.io/rhel8/postgresql-10:1 run-postgresql About an hour ago Up About an hour ago 0.0.0.0:5433-\u0026gt;5432/tcp, 0.0.0.0:8081-\u0026gt;8081/tcp, 0.0.0.0:8089-\u0026gt;8089/tcp clair-pgsql 154a0b6a0803 localhost/podman-pause:4.1.1-1657551413 About an hour ago Up About an hour ago 0.0.0.0:80-\u0026gt;8080/tcp, 0.0.0.0:443-\u0026gt;8443/tcp, 0.0.0.0:5432-\u0026gt;5432/tcp, 0.0.0.0:6379-\u0026gt;6379/tcp da7ca63d2ac6-infra 518b5db0591a registry.redhat.io/rhel8/postgresql-10:1 run-postgresql About an hour ago Up About an hour ago 0.0.0.0:80-\u0026gt;8080/tcp, 0.0.0.0:443-\u0026gt;8443/tcp, 0.0.0.0:5432-\u0026gt;5432/tcp, 0.0.0.0:6379-\u0026gt;6379/tcp quay-pgsql c2576cc7542e registry.redhat.io/rhel8/redis-5:1 run-redis About an hour ago Up About an hour ago 0.0.0.0:80-\u0026gt;8080/tcp, 0.0.0.0:443-\u0026gt;8443/tcp, 0.0.0.0:5432-\u0026gt;5432/tcp, 0.0.0.0:6379-\u0026gt;6379/tcp quay-redis 3c9b87b803d3 registry.redhat.io/quay/clair-rhel8:v3.5.0 12 minutes ago Up 12 minutes ago 0.0.0.0:5433-\u0026gt;5432/tcp, 0.0.0.0:8081-\u0026gt;8081/tcp, 0.0.0.0:8089-\u0026gt;8089/tcp clair 開啟瀏覽器連線 https://quay.ocp.ken.lab quayadmin/welcome1 確認憑證 建立管理員帳號、密碼，Create Account\n1 2 3 Username: quayadmin E-mail: quayadmin@email.com Password: welcome1 登入確認 測試Clair 測試pull \u0026amp; push (憑證可透過 \u0026ndash;tls-verify=false解決 or 將憑證放到podman 設定檔路徑) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [devop@quay ~]$ openssl x509 -in certs/quay.ocp.ken.lab.crt -out certs/quay.ocp.ken.lab.cert [devop@quay ~]$ podman login quay.ocp.ken.lab --cert-dir=/home/devop/certs/ Username: quayadmin Password: Login Succeeded! ----- [devop@quay ~]$ sudo mkdir /etc/containers/certs.d/quay.ocp.ken.lab [devop@quay ~]$ sudo cp /home/devop/certs/quay.ocp.ken.lab.crt /etc/containers/certs.d/quay.ocp.ken.lab/ ＃＃＃重開terminal ----- [devop@quay ~]$ podman pull docker.io/library/ubuntu:20.04 Trying to pull docker.io/library/ubuntu:20.04... Getting image source signatures Copying blob fb0b3276a519 done Copying config 817578334b done Writing manifest to image destination Storing signatures 817578334b4dd5e2b0654610e895e08e1bea996c58119bb9c7e3bd9e74dd8936 [devop@quay ~]$ podman tag docker.io/library/ubuntu:20.04 quay.ocp.ken.lab/quayadmin/ubuntu:20.04 [devop@quay ~]$ podman push quay.ocp.ken.lab/quayadmin/ubuntu:20.04 --cert-dir=/home/devop/certs/ Getting image source signatures Copying blob cdca8156a203 done Copying config 817578334b done Writing manifest to image destination Storing signatures 查看Security Scan\n","date":"2023-09-23T08:18:17+08:00","permalink":"https://wangken0129.github.io/p/redhat_quayclair/","title":"Redhat_Quay\u0026Clair"},{"content":"Rancher RKE2 Cluster Rancher RKE2 安裝Lab\n參考資料 https://docs.rke2.io/#how-is-this-different-from-rke-or-k3s\nRKE2 vs RKE vs K3S RKE2 combines the best-of-both-worlds from the 1.x version of RKE (hereafter referred to as RKE1) and K3s.\nFrom K3s, it inherits the usability, ease-of-operations, and deployment model.\nFrom RKE1, it inherits close alignment with upstream Kubernetes. In places K3s has diverged from upstream Kubernetes in order to optimize for edge deployments, but RKE1 and RKE2 can stay closely aligned with upstream.\nImportantly, RKE2 does not rely on Docker as RKE1 does. RKE1 leveraged Docker for deploying and managing the control plane components as well as the container runtime for Kubernetes. RKE2 launches control plane components as static pods, managed by the kubelet. The embedded container runtime is containerd.\n架構 devop/suse\nDownStream K8S\n安裝作業系統 (No Swap) 關閉防火牆\n註冊作業系統 1 2 3 yast2 email: ken.wang@infotech.com.tw Registry Code: B8203D65240BDA3A 安裝RMS https://kb.vmware.com/s/article/57122\n安裝DNS Server 1 2 3 devop@rms:~\u0026gt; sudo zypper install -t pattern dhcp_dns_server [sudo] root 的密碼： devop@rms:~\u0026gt; sudo yast2 安裝Ngnix 安裝nginx\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 devop@rms:~\u0026gt; sudo zypper install nginx [sudo] root 的密碼： 正在重新整理服務 \u0026#39;Basesystem_Module_15_SP4_x86_64\u0026#39;。 正在重新整理服務 \u0026#39;Desktop_Applications_Module_15_SP4_x86_64\u0026#39;。 正在重新整理服務 \u0026#39;SUSE_Linux_Enterprise_Server_15_SP4_x86_64\u0026#39;。 正在重新整理服務 \u0026#39;Server_Applications_Module_15_SP4_x86_64\u0026#39;。 正在載入套件庫資料... 正在讀取已安裝的套件... 正在解決套件相依性... 將會安裝下列 1 個新的套件： nginx 1 要安裝的新套件. 全部下載大小：702.9 KiB。已快取：0 B。 完成操作後，將使用額外的 2.3 MiB。 要繼續嗎？ [y/n/v/...? 顯示所有選項] (y): y 正在取出 套件 nginx-1.21.5-150400.1.8.x86_64 (1/1), 702.9 KiB (已解開 2.3 MiB) 正在取回︰ nginx-1.21.5-150400.1.8.x86_64.rpm ......................................[完成] 正在檢查檔案衝突： .................................................................[完成] /usr/sbin/useradd -r -c User for nginx -d /var/lib/nginx -U nginx -s /usr/sbin/nologin (1/1) 正在安裝：nginx-1.21.5-150400.1.8.x86_64 .....................................[完成] 設定ngnix\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 devop@rms:/etc/nginx\u0026gt; sudo cat /etc/nginx/nginx.conf worker_processes 4; worker_rlimit_nofile 40000; load_module lib64/nginx/modules/ngx_stream_module.so; events { worker_connections 8192; } #http { # server { # listen 80; # return 301 https://$host$request_uri; # } #} stream { upstream rancher_servers_http { least_conn; server 192.168.33.41:80 max_fails=3 fail_timeout=5s; server 192.168.33.42:80 max_fails=3 fail_timeout=5s; server 192.168.33.43:80 max_fails=3 fail_timeout=5s; } #後端web server server { listen 80; proxy_pass rancher_servers_http; } upstream rancher_servers_https { least_conn; server 192.168.33.41:443 max_fails=3 fail_timeout=5s; server 192.168.33.42:443 max_fails=3 fail_timeout=5s; server 192.168.33.43:443 max_fails=3 fail_timeout=5s; } server { listen 443; proxy_pass rancher_servers_https; } upstream rancher_servers_6443 { least_conn; server 192.168.33.41:6443 max_fails=3 fail_timeout=5s; server 192.168.33.42:6443 max_fails=3 fail_timeout=5s; server 192.168.33.43:6443 max_fails=3 fail_timeout=5s; } server { listen 6443; proxy_pass rancher_servers_6443; } upstream rancher_servers_9345 { least_conn; server 192.168.33.41:9345 max_fails=3 fail_timeout=5s; server 192.168.33.42:9345 max_fails=3 fail_timeout=5s; server 192.168.33.43:9345 max_fails=3 fail_timeout=5s; } server { listen 9345; proxy_pass rancher_servers_9345; } log_format basic \u0026#39;$remote_addr [$time_local] \u0026#39; \u0026#39;$protocol $status $bytes_sent $bytes_received \u0026#39; \u0026#39;$session_time \u0026#34;$upstream_addr\u0026#34; \u0026#39; \u0026#39;\u0026#34;$upstream_bytes_sent\u0026#34; \u0026#34;$upstream_bytes_received\u0026#34; \u0026#34;$upstream_connect_time\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log basic; error_log /var/log/nginx/error.log; } 啟動服務\n1 2 3 4 5 6 devop@rms:/etc/nginx\u0026gt; sudo nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful devop@rms:/etc/nginx\u0026gt; sudo systemctl enable nginx --now Created symlink /etc/systemd/system/multi-user.target.wants/nginx.service → /usr/lib/systemd/system/nginx.service. 調整防火牆設定 1 2 3 4 5 6 7 8 9 10 devop@rms:/etc/nginx\u0026gt; sudo firewall-cmd --add-port=80/tcp --permanent success devop@rms:/etc/nginx\u0026gt; sudo firewall-cmd --add-port=443/tcp --permanent success devop@rms:/etc/nginx\u0026gt; sudo firewall-cmd --add-port=6443/tcp --permanent success devop@rms:/etc/nginx\u0026gt; sudo firewall-cmd --add-port=9345/tcp --permanent success devop@rms:/etc/nginx\u0026gt; sudo firewall-cmd --reload success 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Inbound Rules for RKE2 Server Nodes Protocol\tPort\tSource\tDescription TCP\t9345\tRKE2 agent nodes\tKubernetes API TCP\t6443\tRKE2 agent nodes\tKubernetes API UDP\t8472\tRKE2 server and agent nodes\tRequired only for Flannel VXLAN TCP\t10250\tRKE2 server and agent nodes\tkubelet TCP\t2379\tRKE2 server nodes\tetcd client port TCP\t2380\tRKE2 server nodes\tetcd peer port TCP\t30000-32767\tRKE2 server and agent nodes\tNodePort port range UDP\t8472\tRKE2 server and agent nodes\tCilium CNI VXLAN TCP\t4240\tRKE2 server and agent nodes\tCilium CNI health checks ICMP\t8/0\tRKE2 server and agent nodes\tCilium CNI health checks TCP\t179\tRKE2 server and agent nodes\tCalico CNI with BGP UDP\t4789\tRKE2 server and agent nodes\tCalico CNI with VXLAN TCP\t5473\tRKE2 server and agent nodes\tCalico CNI with Typha TCP\t9098\tRKE2 server and agent nodes\tCalico Typha health checks TCP\t9099\tRKE2 server and agent nodes\tCalico health checks TCP\t5473\tRKE2 server and agent nodes\tCalico CNI with Typha UDP\t8472\tRKE2 server and agent nodes\tCanal CNI with VXLAN TCP\t9099\tRKE2 server and agent nodes\tCanal CNI health checks UDP\t51820\tRKE2 server and agent nodes\tCanal CNI with WireGuard IPv4 UDP\t51821\tRKE2 server and agent nodes\tCanal CNI with WireGuard IPv6/dual-stack 用ansible來執行開啟port (針對各個nodes)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 wangken@wangken-MAC aps-rancher % cat inventory [rke2-cluster] 192.168.2.[71:73] ansible_ssh_pass=P@55w.rd [downstream] 192.168.2.[74:78] ansible_ssh_pass=P@55w.rd wangken@wangken-MAC aps-rancher % cat ansible.cfg [defaults] inventory = /Users/wangken/ansible/infoserver/aps-rancher/inventory remote_user = devop sudo_user = root host_key_checking = False [privilege_escalation] become = true become_user = root become_ask_pass = true wangken@wangken-MAC aps-rancher % cat firewall.yml --- - hosts: rke2-cluster tasks: - name: firewalld: port: \u0026#34;{{ item }}\u0026#34; permanent: yes immediate: yes state: enabled with_items: - 9345/tcp - 6443/tcp - 8472/udp - 10250/tcp - 2379-2380/tcp - 30000-32767/tcp - 8472/udp - 4240/tcp - 179/tcp - 4789/udp - 5473/tcp - 9098/tcp - 9099/tcp - 8472/udp - 51820/udp - 51821/udp wangken@wangken-MAC aps-rancher % ansible-playbook firewall.yml TASK [firewalld] *************************************************************** changed: [192.168.2.73] =\u0026gt; (item=9345/tcp) changed: [192.168.2.72] =\u0026gt; (item=9345/tcp) changed: [192.168.2.71] =\u0026gt; (item=9345/tcp) changed: [192.168.2.72] =\u0026gt; (item=6443/tcp) changed: [192.168.2.73] =\u0026gt; (item=6443/tcp) changed: [192.168.2.71] =\u0026gt; (item=6443/tcp) changed: [192.168.2.73] =\u0026gt; (item=8472/udp) changed: [192.168.2.72] =\u0026gt; (item=8472/udp) changed: [192.168.2.71] =\u0026gt; (item=8472/udp) changed: [192.168.2.73] =\u0026gt; (item=10250/tcp) changed: [192.168.2.71] =\u0026gt; (item=10250/tcp) changed: [192.168.2.72] =\u0026gt; (item=10250/tcp) changed: [192.168.2.72] =\u0026gt; (item=2379-2380/tcp) changed: [192.168.2.73] =\u0026gt; (item=2379-2380/tcp) changed: [192.168.2.71] =\u0026gt; (item=2379-2380/tcp) changed: [192.168.2.71] =\u0026gt; (item=30000-32767/tcp) changed: [192.168.2.73] =\u0026gt; (item=30000-32767/tcp) changed: [192.168.2.72] =\u0026gt; (item=30000-32767/tcp) 安裝NFS Server https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html\n建立NFS Server 目錄\n1 2 3 4 5 6 devop@rms:/\u0026gt; pwd / devop@rms:/\u0026gt; sudo mkdir nfs devop@rms:/\u0026gt; sudo chmod 777 nfs devop@rms:/\u0026gt; ll drwxrwxrwx 1 root root 0 11月 8 14:33 nfs 安裝NFS Server套件 (SUSE OS)\n1 2 3 4 5 6 devop@rms:/home\u0026gt; sudo zypper install yast2-nfs-server 下列 1 個推薦的套件已自動被選取： nfs-kernel-server 將會安裝下列 2 個新的套件： nfs-kernel-server yast2-nfs-server 設定nfs-server path\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 devop@rms:/home\u0026gt; sudo vim /etc/exports devop@rms:/home\u0026gt; cat /etc/exports # See the exports(5) manpage for a description of the syntax of this file. # This file contains a list of all directories that are to be exported to # other computers via NFS (Network File System). # This file used by rpc.nfsd and rpc.mountd. See their manpages for details # on how make changes in this file effective. /nfs 192.168.33.0/255.255.255.0(rw,sync,no_root_squash,insecure) ---- # 參數 * rw：read-write，可讀寫的權限； * ro：read-only，唯讀的權限； * sync：資料同步寫入到記憶體與硬碟當中； * async：資料會先暫存於記憶體當中，而非直接寫入硬碟。 * no_root_squash： 登入 NFS 主機使用分享目錄的使用者如果是 root，對於分享的目錄具有 root 的權限。 極不安全，不建議使用。 * root_squash： 登入 NFS 主機使用分享目錄的使用者如果是 root，權限將被壓縮成匿名使用者， 通常他的 UID 與 GID 都會變成 nobody(nfsnobody) 那個系統帳號的身份； * all_squash： 不論登入 NFS 的使用者身份為何，都會被壓縮成為匿名使用者，通常是 nobody(nfsnobody) 。 * anonuid： anon 意指 anonymous (匿名者)，自訂匿名使用者的 UID。 * anongid：自訂匿名使用者的是變成 GID。 啟動nfs server、調整防火牆\n1 2 3 4 5 6 7 8 9 10 11 12 devop@rms:/home\u0026gt; sudo systemctl enable nfs-server --now Created symlink /etc/systemd/system/multi-user.target.wants/nfs-server.service → /usr/lib/systemd/system/nfs-server.service. devop@rms:~/nfs\u0026gt; sudo firewall-cmd --add-service=nfs --permanent [sudo] root 的密碼： success devop@rms:~/nfs\u0026gt; sudo firewall-cmd --reload success devop@rms:~/nfs\u0026gt; sudo showmount -e Export list for rms: /nfs 192.168.33.0/255.255.255.0 安裝RKE2-1 安裝OS 調整DNS 指向192.168.33.40 關閉防火牆 安裝RKE2 下載install.sh\n1 2 devop@rke2-1:~\u0026gt; curl -sfL https://get.rke2.io --output install.sh devop@rke2-1:~\u0026gt; chmod +x install.sh 新增設定檔\n1 2 3 4 5 6 7 8 9 10 11 12 13 devop@rke2-1:~\u0026gt; sudo mkdir -p /etc/rancher/rke2/ [sudo] root 的密碼： devop@rke2-1:~\u0026gt; sudo vim /etc/rancher/rke2/config.yaml node-name: - \u0026#34;rke2-1\u0026#34; token: my-shared-secret tls-san: - rms.rancher.ken.lab devop@rke2-1:~\u0026gt; rke2 etcd-snapshot save \\ --name pre-upgrade-snapshot 安裝rke2 \u0026ndash; version 1.23.9\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 devop@rke2-1:~\u0026gt; sudo INSTALL_RKE2_CHANNEL=v1.23.9+rke2r1 ./install.sh [WARN] /usr/local is read-only or a mount point; installing to /opt/rke2 [INFO] finding release for channel v1.23.9+rke2r1 [INFO] using v1.23.9+rke2r1 as release [INFO] downloading checksums at https://github.com/rancher/rke2/releases/download/v1.23.9+rke2r1/sha256sum-amd64.txt [INFO] downloading tarball at https://github.com/rancher/rke2/releases/download/v1.23.9+rke2r1/rke2.linux-amd64.tar.gz [INFO] verifying tarball [INFO] unpacking tarball file to /opt/rke2 [INFO] updating tarball contents to reflect install path [INFO] moving systemd units to /etc/systemd/system [INFO] install complete; you may want to run: export PATH=$PATH:/opt/rke2/bin -- devop@rke2-1:~\u0026gt; export PATH=$PATH:/opt/rke2/bin devop@rke2-1:~\u0026gt; sudo systemctl enable rke2-server --now Created symlink /etc/systemd/system/multi-user.target.wants/rke2-server.service → /etc/systemd/system/rke2-server.service. 調整kubectl 、rke2 指令及kubeconfig \u0026raquo; 可把 rke2、kubectl、rke2.yaml scp到 rms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 devop@rke2-1:~\u0026gt; mkdir .kube devop@rke2-1:~\u0026gt; sudo cp /etc/rancher/rke2/rke2.yaml .kube/config devop@rke2-1:~\u0026gt; sudo chown devop .kube/config devop@rke2-1:~\u0026gt; sudo cp /var/lib/rancher/rke2/bin/kubectl /usr/local/bin/ devop@rke2-1:~\u0026gt; cp /opt/rke2/bin/rke2 /usr/local/bin/ ---- devop@rke2-1:~\u0026gt; sudo scp /etc/rancher/rke2/rke2.yaml devop@192.168.33.40:/home/devop/.kube/config The authenticity of host \u0026#39;192.168.33.40 (192.168.33.40)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:NS1PCnKGGGiOsCVPSdoRDhX5jq0yhqhtEIr3pMNDSzw. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;192.168.33.40\u0026#39; (ECDSA) to the list of known hosts. Password: rke2.yaml 100% 2969 2.6MB/s 00:00 devop@rke2-1:~\u0026gt; sudo scp /var/lib/rancher/rke2/bin/kubectl root@192.168.33.40:/usr/local/bin Password: kubectl 100% 47MB 220.3MB/s 00:00 確認pod status (切到 rms來操作)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 devop@rms:~\u0026gt; kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cloud-controller-manager-rke2-1 1/1 Running 0 5m46s kube-system etcd-rke2-1 1/1 Running 0 5m30s kube-system helm-install-rke2-canal-9pg26 0/1 Completed 0 5m35s kube-system helm-install-rke2-coredns-d4j4q 0/1 Completed 0 5m35s kube-system helm-install-rke2-ingress-nginx-gz7db 0/1 Completed 0 5m35s kube-system helm-install-rke2-metrics-server-fx65q 0/1 Completed 0 5m35s kube-system kube-apiserver-rke2-1 1/1 Running 0 5m21s kube-system kube-controller-manager-rke2-1 1/1 Running 0 5m47s kube-system kube-proxy-rke2-1 1/1 Running 0 5m43s kube-system kube-scheduler-rke2-1 1/1 Running 0 5m47s kube-system rke2-canal-qkrdv 2/2 Running 0 5m19s kube-system rke2-coredns-rke2-coredns-545d64676-2q6x9 1/1 Running 0 5m20s kube-system rke2-coredns-rke2-coredns-autoscaler-5dd676f5c7-5bwdl 1/1 Running 0 5m20s kube-system rke2-ingress-nginx-controller-gxp6v 1/1 Running 0 4m42s kube-system rke2-metrics-server-6564db4569-98ghm 1/1 Running 0 4m53s devop@rms:~\u0026gt; 安裝Helm\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 devop@rms:~\u0026gt; wget https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz --2022-10-25 16:55:33-- https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz Resolving get.helm.sh (get.helm.sh)... 152.199.39.108, 2606:2800:247:1cb7:261b:1f9c:2074:3c Connecting to get.helm.sh (get.helm.sh)|152.199.39.108|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 13633605 (13M) [application/x-tar] Saving to: ‘helm-v3.8.2-linux-amd64.tar.gz’ helm-v3.8.2-linux-amd64.ta 100%[=======================================\u0026gt;] 13.00M 34.4MB/s in 0.4s 2022-10-25 16:55:35 (34.4 MB/s) - ‘helm-v3.8.2-linux-amd64.tar.gz’ saved [13633605/13633605] devop@rms:~\u0026gt; tar zxvf helm-v3.8.2-linux-amd64.tar.gz linux-amd64/ linux-amd64/helm linux-amd64/LICENSE linux-amd64/README.md devop@rms:~\u0026gt; sudo cp linux-amd64/helm /usr/local/bin/ [sudo] root 的密碼： devop@rms:~\u0026gt; helm version version.BuildInfo{Version:\u0026#34;v3.8.2\u0026#34;, GitCommit:\u0026#34;6e3701edea09e5d55a8ca2aae03a68917630e91b\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.17.5\u0026#34;} install rancher and cert-manager Add helm repo \u0026amp; Install Cert-manager\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 devop@rms:~\u0026gt; helm repo add rancher-stable https://releases.rancher.com/server-charts/stable \u0026#34;rancher-stable\u0026#34; has been added to your repositories devop@rms:~\u0026gt; kubectl create namespace cattle-system namespace/cattle-system created devop@rms:~\u0026gt; kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.7.1/cert-manager.crds.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created devop@rms:~\u0026gt; helm repo add jetstack https://charts.jetstack.io \u0026#34;jetstack\u0026#34; has been added to your repositories devop@rms:~\u0026gt; helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;jetstack\u0026#34; chart repository ...Successfully got an update from the \u0026#34;rancher-stable\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ devop@rms:~\u0026gt; helm install cert-manager jetstack/cert-manager \\ \u0026gt; --namespace cert-manager \\ \u0026gt; --create-namespace \\ \u0026gt; --version v1.7.1 NAME: cert-manager LAST DEPLOYED: Tue Oct 25 17:01:20 2022 NAMESPACE: cert-manager STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: cert-manager v1.7.1 has been deployed successfully! In order to begin issuing certificates, you will need to set up a ClusterIssuer or Issuer resource (for example, by creating a \u0026#39;letsencrypt-staging\u0026#39; issuer). More information on the different types of issuers and how to configure them can be found in our documentation: https://cert-manager.io/docs/configuration/ For information on how to configure cert-manager to automatically provision Certificates for Ingress resources, take a look at the `ingress-shim` documentation: https://cert-manager.io/docs/usage/ingress/ devop@rms:~\u0026gt; kubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-76d44b459c-xnr7q 1/1 Running 0 75s cert-manager-cainjector-9b679cc6-8lk2m 1/1 Running 0 75s cert-manager-webhook-57c994b6b9-hssn2 1/1 Running 0 75s 安裝Rancher\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 devop@rms:~\u0026gt; helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=rms.rancher.ken.lab --version 2.6.6 NAME: rancher LAST DEPLOYED: Tue Oct 25 17:05:13 2022 NAMESPACE: cattle-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Rancher Server has been installed. NOTE: Rancher may take several minutes to fully initialize. Please standby while Certificates are being issued, Containers are started and the Ingress rule comes up. Check out our docs at https://rancher.com/docs/ If you provided your own bootstrap password during installation, browse to https://rms.rancher.ken.lab to get started. If this is the first time you installed Rancher, get started by running this command and clicking the URL it generates: ``` echo https://rms.rancher.ken.lab/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}\u0026#39;) ``` To get just the bootstrap password on its own, run: ``` kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}{{ \u0026#34;\\n\u0026#34; }}\u0026#39; ``` Happy Containering! ----- 密碼： devop@rms:~\u0026gt; kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}{{ \u0026#34;\\n\u0026#34; }}\u0026#39; mc85v6z7s6tnfcrwpnd67pg4phlnnch64gwsd5gwkflsmt8bjqckgg 瀏覽器連線 https://rms.rancher.ken.lab\n登入後修改預設密碼 admin / rancheradmin 登入測試 安裝RKE2-2 安裝OS 調整DNS 指向192.168.33.40 關閉防火牆 安裝RKE2 下載install.sh\n1 2 devop@rke2-2:~\u0026gt; curl -sfL https://get.rke2.io --output install.sh devop@rke2-2:~\u0026gt; chmod +x install.sh 新增設定檔\n1 2 3 4 5 6 7 8 9 devop@rke2-2:~\u0026gt; sudo mkdir -p /etc/rancher/rke2/ [sudo] root 的密碼： devop@rke2-2:~\u0026gt; sudo vim /etc/rancher/rke2/config.yaml server: https://rms.rancher.ken.lab:9345 node-name: - \u0026#34;rke2-2\u0026#34; token: my-shared-secret tls-san: - rms.rancher.ken.lab 安裝rke2\n1 devop@rke2-2:~\u0026gt; sudo INSTALL_RKE2_CHANNEL=v1.23.9+rke2r1 ./install.sh 啟動服務\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 devop@rke2-2:~\u0026gt; export PATH=$PATH:/opt/rke2/bin devop@rke2-2:~\u0026gt; sudo systemctl enable rke2-server --now Created symlink /etc/systemd/system/multi-user.target.wants/rke2-server.service → /etc/systemd/system/rke2-server.service. devop@rke2-2:~\u0026gt; systemctl status rke2-server.service ● rke2-server.service - Rancher Kubernetes Engine v2 (server) Loaded: loaded (/etc/systemd/system/rke2-server.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2022-10-25 17:53:05 CST; 17min ago Docs: https://github.com/rancher/rke2#readme Process: 5602 ExecStartPre=/bin/sh -xc ! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service (code=exited, status=0/SUCCESS) Process: 5604 ExecStartPre=/sbin/modprobe br_netfilter (code=exited, status=0/SUCCESS) Process: 5605 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 5606 (rke2) Tasks: 148 切換到RMS 確認nodes狀態\n1 2 3 4 devop@rms:~\u0026gt; kubectl get nodes NAME STATUS ROLES AGE VERSION rke2-1 Ready control-plane,etcd,master 81m v1.23.9+rke2r1 rke2-2 Ready control-plane,etcd,master 17m v1.23.9+rke2r1 安裝RKE 2-3 安裝OS 調整DNS 指向192.168.33.40 關閉防火牆 安裝RKE2 下載install.sh\n1 2 devop@rke2-3:~\u0026gt; curl -sfL https://get.rke2.io --output install.sh devop@rke2-3:~\u0026gt; chmod +x install.sh 新增設定檔\n1 2 3 4 5 6 7 8 9 devop@rke2-3:~\u0026gt; sudo mkdir -p /etc/rancher/rke2/ [sudo] root 的密碼： devop@rke2-3:~\u0026gt; sudo vim /etc/rancher/rke2/config.yaml server: https://rms.rancher.ken.lab:9345 node-name: - \u0026#34;rke2-3\u0026#34; token: my-shared-secret tls-san: - rms.rancher.ken.lab 安裝rke2\n1 2 3 4 5 6 7 8 9 10 11 devop@rke2-3:~\u0026gt; sudo INSTALL_RKE2_CHANNEL=v1.23.9+rke2r1 ./install.sh [WARN] /usr/local is read-only or a mount point; installing to /opt/rke2 [INFO] finding release for channel v1.23.9+rke2r1 [INFO] using v1.23.9+rke2r1 as release [INFO] downloading checksums at https://github.com/rancher/rke2/releases/download/v1.23.9+rke2r1/sha256sum-amd64.txt [INFO] downloading tarball at https://github.com/rancher/rke2/releases/download/v1.23.9+rke2r1/rke2.linux-amd64.tar.gz [INFO] verifying tarball [INFO] unpacking tarball file to /opt/rke2 [INFO] updating tarball contents to reflect install path [INFO] moving systemd units to /etc/systemd/system [INFO] install complete; you may want to run: export PATH=$PATH:/opt/rke2/bin 啟動服務\n1 2 devop@rke2-3:~\u0026gt; export PATH=$PATH:/opt/rke2/bin devop@rke2-3:~\u0026gt; sudo systemctl enable rke2-server --now 切換到RMS 確認Nodes\n1 2 3 4 5 devop@rms:~\u0026gt; kubectl get nodes NAME STATUS ROLES AGE VERSION rke2-1 Ready control-plane,etcd,master 91m v1.23.9+rke2r1 rke2-2 Ready control-plane,etcd,master 27m v1.23.9+rke2r1 rke2-3 Ready control-plane,etcd,master 58s v1.23.9+rke2r1 新增Down Stream K8s 架構 建議用v1.24以上 作業系統與安裝Dokcer 要關閉防火牆、安裝Docker (用RKE2 不用安裝Docker)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 需先註冊系統或是設定repo devop@master01:~\u0026gt; sudo SUSEConnect -p sle-module-containers/15.4/x86_64 Registering system to SUSE Customer Center Updating system details on https://scc.suse.com ... Activating sle-module-containers 15.4 x86_64 ... -\u0026gt; Adding service to system ... -\u0026gt; Installing release package ... Successfully registered system --- devop@master01:~\u0026gt; sudo zypper -n install docker 正在重新整理服務 \u0026#39;Basesystem_Module_15_SP4_x86_64\u0026#39;。 正在重新整理服務 \u0026#39;Containers_Module_15_SP4_x86_64\u0026#39;。 正在重新整理服務 \u0026#39;SUSE_Linux_Enterprise_Server_15_SP4_x86_64\u0026#39;。 正在重新整理服務 \u0026#39;Server_Applications_Module_15_SP4_x86_64\u0026#39;。 正在載入套件庫資料... 正在讀取已安裝的套件... 正在解決套件相依性... 下列 1 個推薦的套件已自動被選取： git-core 將會安裝下列 7 個新的套件： catatonit containerd docker docker-bash-completion git-core libsha1detectcoll1 runc 7 要安裝的新套件. 全部下載大小：52.2 MiB。已快取：0 B。 完成操作後，將使用額外的 242.1 MiB。 要繼續嗎？ [y/n/v/...? 顯示所有選項] (y): y 正在取出 套件 libsha1detectcoll1-1.0.3-2.18.x86_64 (1/7), 23.2 KiB (已解開 45.8 KiB) 正在取回︰ libsha1detectcoll1-1.0.3-2.18.x86_64.rpm ...........................................[完成] 正在取出 套件 catatonit-0.1.5-3.3.2.x86_64 (2/7), 257.2 KiB (已解開 696.5 KiB) 正在取回︰ catatonit-0.1.5-3.3.2.x86_64.rpm ...................................................[完成] 正在取出 套件 runc-1.1.4-150000.33.4.x86_64 (3/7), 2.6 MiB (已解開 9.1 MiB) 正在取回︰ runc-1.1.4-150000.33.4.x86_64.rpm ....................................[完成 (288.0 KiB/s)] 正在取出 套件 containerd-1.6.6-150000.73.2.x86_64 (4/7), 17.7 MiB (已解開 74.2 MiB) 正在取回︰ containerd-1.6.6-150000.73.2.x86_64.rpm ................................[完成 (3.6 MiB/s)] 正在取出 套件 git-core-2.35.3-150300.10.15.1.x86_64 (5/7), 4.8 MiB (已解開 26.6 MiB) 正在取回︰ git-core-2.35.3-150300.10.15.1.x86_64.rpm ..........................................[完成] 正在取出 套件 docker-20.10.17_ce-150000.166.1.x86_64 (6/7), 26.6 MiB (已解開 131.4 MiB) 正在取回︰ docker-20.10.17_ce-150000.166.1.x86_64.rpm .............................[完成 (7.9 MiB/s)] 正在取出 套件 docker-bash-completion-20.10.17_ce-150000.166.1.noarch (7/7), 121.3 KiB (已解開 113.6 KiB) 正在取回︰ docker-bash-completion-20.10.17_ce-150000.166.1.noarch.rpm .........................[完成] 正在檢查檔案衝突： ............................................................................[完成] (1/7) 正在安裝：libsha1detectcoll1-1.0.3-2.18.x86_64 ..........................................[完成] (2/7) 正在安裝：catatonit-0.1.5-3.3.2.x86_64 ..................................................[完成] (3/7) 正在安裝：runc-1.1.4-150000.33.4.x86_64 .................................................[完成] (4/7) 正在安裝：containerd-1.6.6-150000.73.2.x86_64 ...........................................[完成] (5/7) 正在安裝：git-core-2.35.3-150300.10.15.1.x86_64 .........................................[完成] Updating /etc/sysconfig/docker ... (6/7) 正在安裝：docker-20.10.17_ce-150000.166.1.x86_64 ........................................[完成] (7/7) 正在安裝：docker-bash-completion-20.10.17_ce-150000.166.1.noarch ........................[完成] --- 啟動服務 devop@master01:~\u0026gt; sudo systemctl enable docker --now Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service. 從Rancher管理介面新增Cluster 點選Create 選擇Custom 要把RKE2的選項打勾 才可以選擇RKE2 建立名稱: DownStreamK8S ，其他Default即可 針對各Node選擇不同的指令-Master 直接貼在Master01、Master02、Ｍaster03上 針對各Node選擇不同的指令-Master 直接貼在Woker01、Worker02 查看Log 查看Cluster狀態 安裝後的設定 Authentication LDAP 點到Users \u0026amp; Authentication 設定參數並Enable 指定特定Group可存取Rancher 登入測試 調整群組權限 Assign Global Roles 加入ranchers的群組為Administrator 並Save 確認權限 Storage設定 在Local Cluster的infra project建立rancher-nfs-storage的namespace 建立namespace Storage Class的yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 devop@rms:~/config\u0026gt; cat storageclass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: # 設定為default StorageClass，如果建立pvc時不指定StorageClass，則會使用當前的StorageClass storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; name: default-nfs-storage namespace: rancher-nfs-storage provisioner: provisioner-nfs # or choose another name, must match deployment\u0026#39;s env PROVISIONER_NAME\u0026#39; parameters: # 刪除PVC時不會保留資料 archiveOnDelete: \u0026#34;false\u0026#34; # 回收策略：刪除 reclaimPolicy: Delete #允許pvc建立後擴容 allowVolumeExpansion: true Deployment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 devop@rms:~/config\u0026gt; cat deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: rancher-nfs-storage spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: provisioner-nfs - name: NFS_SERVER value: 192.168.33.40 - name: NFS_PATH value: /nfs volumes: - name: nfs-client-root nfs: server: 192.168.33.40 path: /nfs rbac.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: rancher-nfs-storage --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;persistentvolumes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;persistentvolumeclaims\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;storage.k8s.io\u0026#34;] resources: [\u0026#34;storageclasses\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;events\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services\u0026#34;, \u0026#34;endpoints\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;,\u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;] resources: [\u0026#34;podsecuritypolicies\u0026#34;] resourceNames: [\u0026#34;nfs-client-provisioner\u0026#34;] verbs: [\u0026#34;use\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: rancher-nfs-storage roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: rancher-nfs-storage rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;endpoints\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: rancher-nfs-storage subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: rancher-nfs-storage roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io NFS Server先設定好可連線的主機\n1 2 3 4 5 6 範例： [root@bastion auth]# cat /etc/exports /data/nfsshare\t*(rw,sync,no_wdelay,no_root_squash,insecure) --- devop@rms:~/config\u0026gt; cat /etc/exports /nfs 192.168.33.0/255.255.255.0(rw,sync,no_root_squash,insecure) 將rbac.yaml 、storage class.yaml、deployment.yaml部署到rancher-nfs-storage的NS上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 devop@rms:~/config\u0026gt; kubectl create -f rbac.yaml -f storageclass.yaml -f deployment.yaml serviceaccount/nfs-client-provisioner created clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created storageclass.storage.k8s.io/default-nfs-storage created deployment.apps/nfs-client-provisioner created --- devop@rms:~/nfs\u0026gt; kubectl get all -n rancher-nfs-storage NAME READY STATUS RESTARTS AGE pod/nfs-client-provisioner-6b8787f55b-r4m4t 1/1 Running 0 5m19s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nfs-client-provisioner 1/1 1 1 5m19s NAME DESIRED CURRENT READY AGE replicaset.apps/nfs-client-provisioner-6b8787f55b 1 1 1 5m19s 查看WebUI，確認deployment、StorageClass 新增測試Claim 確認狀態為bound\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 devop@rms:~/nfs\u0026gt; cat test-claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim annotations: #storageclass 名稱 volume.beta.kubernetes.io/storage-class: \u0026#34;default-nfs-storage\u0026#34; spec: #訪問模式 accessModes: - ReadWriteMany resources: requests: #請求資料大小 storage: 1024Mi --- devop@rms:~/nfs\u0026gt; kubectl create -f test-claim.yaml persistentvolumeclaim/test-claim created devop@rms:~/nfs\u0026gt; kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-f058cf4a-cc7b-4e44-97c3-aa1bfc654b65 1Gi RWX Delete Bound default/test-claim default-nfs-storage 33m NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/test-claim Bound pvc-f058cf4a-cc7b-4e44-97c3-aa1bfc654b65 1Gi RWX default-nfs-storage 37m 新增測試pod 確認可寫入\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 devop@tpeinfoapsrms01:~/nfs\u0026gt; cat test-pod.yaml kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: busybox:latest command: - \u0026#34;/bin/sh\u0026#34; args: - \u0026#34;-c\u0026#34; - \u0026#34;touch /mnt/SUCCESS \u0026amp;\u0026amp; exit 0 || exit 1\u0026#34; volumeMounts: - name: nfs-pvc mountPath: \u0026#34;/mnt\u0026#34; restartPolicy: \u0026#34;Never\u0026#34; volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-claim --- devop@rms:~/nfs\u0026gt; kubectl create -f test-pod.yaml pod/test-pod created devop@rms:~/nfs\u0026gt; kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-f058cf4a-cc7b-4e44-97c3-aa1bfc654b65 1Gi RWX Delete Bound default/test-claim default-nfs-storage 35m NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/test-claim Bound pvc-f058cf4a-cc7b-4e44-97c3-aa1bfc654b65 1Gi RWX default-nfs-storage 38m --- WebUI 刪除測試用的pv,pvc\n1 2 3 devop@rms:~/nfs\u0026gt; kubectl delete -f test-pod.yaml -f test-claim.yaml pod \u0026#34;test-pod\u0026#34; deleted persistentvolumeclaim \u0026#34;test-claim\u0026#34; deleted 同樣設定也放到DownStream K8s ，建立時也可透過WEB UI來將yaml檔放到Cluster 安裝Logging 點到local的cluster 建立project (infra) 點選Apps \u0026ndash;\u0026gt; Charts \u0026ndash;\u0026gt; logging Default 開始安裝 重整網頁後即會出現 後續可再針對後端的logging 機制 安裝Monitoring https://www.ibm.com/docs/en/cloud-private/3.2.0?topic=console-namespace-is-stuck-in-terminating-state\n點到local的cluster\n建立project (infra) 點選Apps \u0026ndash;\u0026gt; Charts \u0026ndash;\u0026gt; monitoring 點選Monitoring \u0026ndash; \u0026gt; Install (安裝到Infra Project) Admin API Enable、PV Grafana Enable 點選Install後重新整理WEB UI，即會出現Monitoring\n點選Grafana確認資料 查詢Grafana預設帳號密碼 會放在cattle-monitoring-system的 Secret裡 (帳密為admin/prom-operator) 安裝Neuvector https://www.twblogs.net/a/6225a6dfc1f9c8b6ea0c84d2 https://github.com/IBM/Runtime-Container-Security-for-Kube/blob/main/neuvector.yaml https://www.readfog.com/a/1685538272501141504 https://open-docs.neuvector.com/\nRancher Cluster (備份.還原.升級) https://docs.ranchermanager.rancher.io/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster/upgrades\nRancher Backup (備份還原APP) 在Apps \u0026ndash;\u0026gt; Charts 搜尋rancher backups 點選install 設定儲存位置 (Storage Class)\n點選install 重整Web UI，出現Rancher Backups 點選Rancher Backups \u0026ndash;\u0026gt; Create 設定名稱、參數等，點選Create 確認備份完成 測試先將monitoring刪除後備份 （手動刪除cattle-monitoring-system、相關CRD）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 devop@rms:~\u0026gt; kubectl delete ns cattle-monitoring-system --- 卡在terminating 可執行下方操作 devop@rms:~\u0026gt; kubectl get ns cattle-monitoring-system -o json \u0026gt; tmp.json devop@rms:~\u0026gt; vim tmp.json --- 刪除 \u0026#34;finalizers\u0026#34;: [ \u0026#34;kubernetes\u0026#34; ] devop@rms:~\u0026gt; kubectl proxy --- 開另一個terminal devop@rms:~\u0026gt; curl -k -H \u0026#34;Content-Type: application/json\u0026#34; -X PUT --data-binary @tmp.json http://127.0.0.1:8001/api/v1/namespaces/cattle-monitoring-system/finalize --- 確認cattle-monitoring-system 刪除 devop@rms:~\u0026gt; kubectl get ns cattle-monitoring-system Error from server (NotFound): namespaces \u0026#34;cattle-monitoring-system\u0026#34; not found --- 刪除相關CRD devop@rms:~\u0026gt; kubectl delete crd prometheusrules.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io \u0026#34;prometheusrules.monitoring.coreos.com\u0026#34; deleted devop@rms:~\u0026gt; kubectl delete crd prometheuses.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io \u0026#34;prometheuses.monitoring.coreos.com\u0026#34; deleted devop@rms:~\u0026gt; kubectl delete crd alertmanagerconfigs.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io \u0026#34;alertmanagerconfigs.monitoring.coreos.com\u0026#34; deleted devop@rms:~\u0026gt; kubectl delete crd alertmanagers.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io \u0026#34;alertmanagers.monitoring.coreos.com\u0026#34; deleted devop@rms:~\u0026gt; kubectl delete crd podmonitors.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io \u0026#34;podmonitors.monitoring.coreos.com\u0026#34; deleted devop@rms:~\u0026gt; kubectl delete crd probes.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io \u0026#34;probes.monitoring.coreos.com\u0026#34; deleted devop@rms:~\u0026gt; kubectl delete crd servicemonitors.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io \u0026#34;servicemonitors.monitoring.coreos.com\u0026#34; deleted devop@rms:~\u0026gt; kubectl delete crd thanosrulers.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io \u0026#34;thanosrulers.monitoring.coreos.com\u0026#34; deleted 確認備份完成後將Monitoring安裝回去 確認安裝完成 確認備份檔案\n還原備份檔案\u0026ndash;\u0026gt;點選restore \u0026ndash;\u0026gt;Create 即會開始執行restore\n1 2 3 4 5 Result: The backup file is created and updated to the target storage location. The resources are restored in this order: 1. Custom Resource Definitions (CRDs) 2. Cluster-scoped resources 3. Namespaced resources 確認 status，Rancher Cluster服務會中斷 (約3分鐘)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 devop@rms:~\u0026gt; kubectl get po -n cattle-system NAME READY STATUS RESTARTS AGE rancher-6c8f689f57-bgj4j 0/1 Terminating 0 30s rancher-6c8f689f57-jkmpq 0/1 Terminating 0 30s rancher-6c8f689f57-tj8tw 0/1 Terminating 0 30s rancher-webhook-d6f8b6cb-xzjxc 1/1 Running 0 18h devop@rms:~\u0026gt; kubectl logs rancher-backup-6b9fc54cc6-nzt6m -n cattle-resources-system INFO[2022/11/09 02:32:31] Marking resource rolebindings.rbac.authorization.k8s.io#v1/fleet-local/request-6hktw for deletion INFO[2022/11/09 02:32:31] Marking resource rolebindings.rbac.authorization.k8s.io#v1/fleet-local/request-d9wpm for deletion INFO[2022/11/09 02:32:31] Marking resource rolebindings.rbac.authorization.k8s.io#v1/fleet-local/request-hskp9 for deletion INFO[2022/11/09 02:32:31] Will retry pruning resources by removing finalizers in 10s INFO[2022/11/09 02:32:41] Retrying pruning resources by removing finalizers INFO[2022/11/09 02:32:41] Processing controllerRef apps/v1/deployments/rancher INFO[2022/11/09 02:32:41] Scaling up controllerRef apps/v1/deployments/rancher to 3 INFO[2022/11/09 02:32:41] Done restoring 確認Rancher 服務啟動 cattle-monitoring-system \u0026ndash; terminating 、相關crd還存在需手動刪除\n(若為自定義的Application應會全部清空，此處使用helm chart安裝的有留存著)\n1 2 3 4 5 6 7 8 9 10 11 12 devop@rms:~\u0026gt; kubectl get po -n cattle-system NAME READY STATUS RESTARTS AGE rancher-6c8f689f57-8cn46 1/1 Running 0 71s rancher-6c8f689f57-kcqxc 1/1 Running 0 71s rancher-6c8f689f57-w5hsz 1/1 Running 0 71s rancher-webhook-d6f8b6cb-xzjxc 1/1 Running 0 18h devop@rms:~\u0026gt; kubectl get ns NAME STATUS AGE cattle-monitoring-system Terminating 9m11s devop@rms:~\u0026gt; kubectl get crd devop@rms:~\u0026gt; kubectl delete crd probes.monitoring.coreos.com ... 備份Rancher Cluster etcd https://docs.rke2.io/backup_restore/\nansible設定\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 wangken@wangken-MAC rke2-lab % cat ansible.cfg [defaults] inventory = /Users/wangken/ansible/infoserver/rke2-lab/inventory remote_user = devop sudo_user = root host_key_checking = False [privilege_escalation] become = true become_user = root become_ask_pass = true -- wangken@wangken-MAC rke2-lab % cat inventory [rke2-cluster] 192.168.33.[41:43] ansible_ssh_pass=suse [downstreamk8s-master] 192.168.33.[44:46] ansible_ssh_pass=suse [downstreamk8s-worker] 192.168.33.[47:48] ansible_ssh_pass=suse [downstreamk8s:children] downstreamk8s-master downstreamk8s-worker wangken@wangken-MAC rke2-lab % ansible rke2-cluster -m ping 確認各nodes皆有rke2指令 /opt/rke2/bin/rke2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 wangken@wangken-MAC rke2-lab % ansible rke2-cluster -m shell -a \u0026#39;rke2 --help\u0026#39; BECOME password: 192.168.33.41 | FAILED | rc=127 \u0026gt;\u0026gt; /bin/sh: rke2：命令找不到non-zero return code 192.168.33.42 | FAILED | rc=127 \u0026gt;\u0026gt; /bin/sh: rke2：命令找不到non-zero return code 192.168.33.43 | FAILED | rc=127 \u0026gt;\u0026gt; /bin/sh: rke2：命令找不到non-zero return code --- copy rke2 到/usr/local/bin wangken@wangken-MAC rke2-lab % ansible rke2-cluster -m shell -a \u0026#39;cp /opt/rke2/bin/rke2 /usr/local/bin/\u0026#39; BECOME password: --- wangken@wangken-MAC rke2-lab % ansible rke2-cluster -m shell -a \u0026#39;rke2 --help\u0026#39; BECOME password: 192.168.33.41 | CHANGED | rc=0 \u0026gt;\u0026gt; 192.168.33.42 | CHANGED | rc=0 \u0026gt;\u0026gt; 192.168.33.43 | CHANGED | rc=0 \u0026gt;\u0026gt; backup etcd while running\n1 2 3 4 5 6 7 8 9 10 11 Snapshots are enabled by default. The snapshot directory defaults to /var/lib/rancher/rke2/server/db/snapshots wangken@wangken-MAC rke2-lab % ansible rke2-cluster -m shell -a \u0026#39;rke2 etcd-snapshot save --name pre-upgrade-snapshot-v2.6.6\u0026#39; BECOME password: 192.168.33.41 | CHANGED | rc=0 \u0026gt;\u0026gt; 192.168.33.42 | CHANGED | rc=0 \u0026gt;\u0026gt; 192.168.33.43 | CHANGED | rc=0 \u0026gt;\u0026gt; wangken@wangken-MAC rke2-lab % ansible rke2-cluster -m shell -a \u0026#39;ls -l /var/lib/rancher/rke2/server/db/snapshots\u0026#39; BECOME password: 利用下篇方法升級rancher cluster的version v2.6.6 to v2.6.9\n升級rancher cluster的k8s version\n升級Rancher Cluster 確認Rancher Cluster\n1 2 3 4 5 6 7 devop@rms:~/nfs\u0026gt; helm get values rancher -n cattle-system USER-SUPPLIED VALUES: hostname: rms.rancher.ken.lab --- devop@rms:~\u0026gt; helm get values rancher -n cattle-system -o yaml \u0026gt; values.yaml devop@rms:~\u0026gt; cat values.yaml hostname: rms.rancher.ken.lab 確認rancher helm chart\n1 2 3 4 5 devop@rms:~\u0026gt; helm repo list NAME URL rancher-stable\thttps://releases.rancher.com/server-charts/stable jetstack https://charts.jetstack.io neuvector https://neuvector.github.io/neuvector-helm/ update helm chart\n1 2 3 4 5 6 devop@rms:~\u0026gt; helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;rancher-stable\u0026#34; chart repository ...Successfully got an update from the \u0026#34;neuvector\u0026#34; chart repository ...Successfully got an update from the \u0026#34;jetstack\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ 確認目前rancher 最新版本\n1 2 3 devop@rms:~\u0026gt; helm search repo rancher-stable NAME CHART VERSION\tAPP VERSION\tDESCRIPTION rancher-stable/rancher\t2.6.9 v2.6.9 Install Rancher Server to manage Kubernetes clu... 確認更新項目\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 devop@rms:~\u0026gt; helm fetch rancher-stable/rancher --version=2.6.9 --- devop@rms:~\u0026gt; tar xvf rancher-2.6.9.tgz rancher/Chart.yaml rancher/values.yaml rancher/templates/NOTES.txt rancher/templates/_helpers.tpl rancher/templates/clusterRoleBinding.yaml rancher/templates/deployment.yaml rancher/templates/ingress.yaml rancher/templates/issuer-letsEncrypt.yaml rancher/templates/issuer-rancher.yaml rancher/templates/post-delete-hook-cluster-role-binding.yaml rancher/templates/post-delete-hook-cluster-role.yaml rancher/templates/post-delete-hook-config-map.yaml rancher/templates/post-delete-hook-job.yaml rancher/templates/post-delete-hook-psp.yaml rancher/templates/post-delete-hook-service-account.yaml rancher/templates/pvc.yaml rancher/templates/secret.yaml rancher/templates/service.yaml rancher/templates/serviceAccount.yaml rancher/.helmignore rancher/README.md rancher/scripts/post-delete-hook.sh ---- devop@rms:~\u0026gt; tree rancher rancher ├── Chart.yaml ├── README.md ├── scripts │ └── post-delete-hook.sh ├── templates │ ├── clusterRoleBinding.yaml │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── ingress.yaml │ ├── issuer-letsEncrypt.yaml │ ├── issuer-rancher.yaml │ ├── NOTES.txt │ ├── post-delete-hook-cluster-role-binding.yaml │ ├── post-delete-hook-cluster-role.yaml │ ├── post-delete-hook-config-map.yaml │ ├── post-delete-hook-job.yaml │ ├── post-delete-hook-psp.yaml │ ├── post-delete-hook-service-account.yaml │ ├── pvc.yaml │ ├── secret.yaml │ ├── serviceAccount.yaml │ └── service.yaml └── values.yaml 2 directories, 21 files 執行更新\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 devop@rms:~\u0026gt; helm upgrade rancher rancher-stable/rancher --namespace cattle-system -f values.yaml --version=2.6.9 --- Release \u0026#34;rancher\u0026#34; has been upgraded. Happy Helming! NAME: rancher LAST DEPLOYED: Tue Nov 8 15:52:00 2022 NAMESPACE: cattle-system STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: Rancher Server has been installed. NOTE: Rancher may take several minutes to fully initialize. Please standby while Certificates are being issued, Containers are started and the Ingress rule comes up. Check out our docs at https://rancher.com/docs/ If you provided your own bootstrap password during installation, browse to https://rms.rancher.ken.lab to get started. If this is the first time you installed Rancher, get started by running this command and clicking the URL it generates: ``` echo https://rms.rancher.ken.lab/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}\u0026#39;) ``` To get just the bootstrap password on its own, run: ``` kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}{{ \u0026#34;\\n\u0026#34; }}\u0026#39; ``` Happy Containering! 確認status\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 devop@rms:~\u0026gt; kubectl get pod -n cattle-system NAME READY STATUS RESTARTS AGE helm-operation-trgz9 0/2 Completed 0 17m rancher-6c8f689f57-47hfd 0/1 ContainerCreating 0 43s rancher-6c8f689f57-g666k 0/1 ContainerCreating 0 43s rancher-7fd65d9cd6-m9gt7 1/1 Running 2 (7d1h ago) 13d rancher-7fd65d9cd6-vjcpk 1/1 Running 2 (7d1h ago) 13d rancher-webhook-5b65595df9-9xsrc 1/1 Running 2 (7d1h ago) 13d devop@rms:~\u0026gt; kubectl get pod -n cattle-system NAME READY STATUS RESTARTS AGE helm-operation-ffdn6 0/2 Completed 0 13m helm-operation-kjqgc 0/2 Completed 0 14m helm-operation-tfbhr 0/2 Completed 0 13m helm-operation-trgz9 0/2 Completed 0 33m rancher-6c8f689f57-47hfd 1/1 Running 0 16m rancher-6c8f689f57-g666k 1/1 Running 0 16m rancher-6c8f689f57-nkfkb 1/1 Running 0 15m rancher-webhook-d6f8b6cb-xzjxc 1/1 Running 0 13m devop@rms:~\u0026gt; helm history rancher -n cattle-system REVISION\tUPDATED STATUS CHART APP VERSION\tDESCRIPTION 1 Tue Oct 25 17:05:13 2022\tsuperseded\trancher-2.6.6\tv2.6.6 Install complete 2 Tue Nov 8 15:52:00 2022\tsuperseded\trancher-2.6.9\tv2.6.9 Upgrade complete 3 Wed Nov 9 10:55:25 2022\tsuperseded\trancher-2.6.6\tv2.6.6 Rollback to 1 4 Wed Nov 9 12:04:06 2022\tdeployed rancher-2.6.9\tv2.6.9 Upgrade complete 從Web UI確認版本 v2.6.9 升級k8s version，針對升級後的Rancher Cluster點選edit config\nv1.23.9 to v1.24.4 點選save，查看升級狀態 rolling upgrade ，期間Web UI不受影響\nnodes輪流升級 (RAM的使用率會飆高) 升級完成 Roll Backup Rancher (Helm) https://docs.ranchermanager.rancher.io/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster/rollbacks\n雖官方文件有寫到，但原廠不建議這麼做 ，只針對rancher 版本回復\n查看rancher安裝過的版本\n1 2 3 4 devop@rms:~\u0026gt; helm history rancher -n cattle-system REVISION\tUPDATED STATUS CHART APP VERSION\tDESCRIPTION 1 Tue Oct 25 17:05:13 2022\tsuperseded\trancher-2.6.6\tv2.6.6 Install complete 2 Tue Nov 8 15:52:00 2022\tdeployed rancher-2.6.9\tv2.6.9 Upgrade complete 利用helm 還原rancher版本\n1 2 devop@rms:~\u0026gt; helm rollback rancher 1 -n cattle-system Rollback was a success! Happy Helming! 確認status\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 devop@rms:~\u0026gt; watch kubectl get po -n cattle-system NAME READY STATUS RESTARTS AGE rancher-6c8f689f57-kcqxc 1/1 Running 0 23m rancher-6c8f689f57-w5hsz 1/1 Running 0 23m rancher-7fd65d9cd6-hsbhg 0/1 ContainerCreating 0 24s rancher-7fd65d9cd6-zrdm8 0/1 Running 0 24s rancher-webhook-d6f8b6cb-xzjxc 1/1 Running 0 19h devop@rms:~\u0026gt; helm history rancher -n cattle-system REVISION\tUPDATED STATUS CHART APP VERSION\tDESCRIPTION 1 Tue Oct 25 17:05:13 2022\tsuperseded\trancher-2.6.6\tv2.6.6 Install complete 2 Tue Nov 8 15:52:00 2022\tsuperseded\trancher-2.6.9\tv2.6.9 Upgrade complete 3 Wed Nov 9 10:55:25 2022\tdeployed rancher-2.6.6\tv2.6.6 Rollback to 1 devop@rms:~\u0026gt; kubectl get po -n cattle-system NAME READY STATUS RESTARTS AGE rancher-7fd65d9cd6-hsbhg 1/1 Running 0 34m rancher-7fd65d9cd6-qz9xb 1/1 Running 0 33m rancher-7fd65d9cd6-zrdm8 1/1 Running 0 34m rancher-webhook-d6f8b6cb-xzjxc 1/1 Running 0 19h devop@rms:~\u0026gt; kubectl describe po rancher-7fd65d9cd6-hsbhg -n cattle-system Containers: rancher: Container ID: containerd://13a9d5362e57776b8cfb54f63e872cd81b6ec38623716d7d3b3006f57acc9f89 Image: rancher/rancher:v2.6.6 重整Web UI確認版本已為v2.6.6 還原Rancher Cluster etcd 1 2 3 4 5 6 順序： 1. 停止所有node的rke2-server service 2. 在第一個節點 執行restore指令 3. 啟動第一個節點的 rke2-server service 4. 刪除剩下兩個節點的db目錄 /var/lib/rancher/rke2/server/db 5. 啟動剩下兩個節點的 rke2-server service 先確認先前做snapshot的檔案都在\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 wangken@wangken-MAC rke2-lab % ansible rke2-cluster -m shell -a \u0026#39;ls -l /var/lib/rancher/rke2/server/db/snapshots\u0026#39; BECOME password: 192.168.33.43 | CHANGED | rc=0 \u0026gt;\u0026gt; 總用量 270504 -rw------- 1 root root 41029664 11月 7 12:00 etcd-snapshot-rke2-3-1667793600 -rw------- 1 root root 41029664 11月 8 00:00 etcd-snapshot-rke2-3-1667836800 -rw------- 1 root root 41029664 11月 8 12:00 etcd-snapshot-rke2-3-1667880000 -rw------- 1 root root 41029664 11月 9 00:00 etcd-snapshot-rke2-3-1667923200 -rw------- 1 root root 56426528 11月 9 12:00 etcd-snapshot-rke2-3-1667966400 -rw------- 1 root root 56426528 11月 9 11:58 pre-upgrade-snapshot-v2.6.6-rke2-3-1667966290 192.168.33.42 | CHANGED | rc=0 \u0026gt;\u0026gt; 總用量 270512 -rw------- 1 root root 40488992 11月 7 12:00 etcd-snapshot-rke2-2-1667793600 -rw------- 1 root root 40488992 11月 8 00:00 etcd-snapshot-rke2-2-1667836800 -rw------- 1 root root 40488992 11月 8 12:00 etcd-snapshot-rke2-2-1667880000 -rw------- 1 root root 40488992 11月 9 00:00 etcd-snapshot-rke2-2-1667923200 -rw------- 1 root root 57511968 11月 9 12:00 etcd-snapshot-rke2-2-1667966400 -rw------- 1 root root 57511968 11月 9 11:58 pre-upgrade-snapshot-v2.6.6-rke2-2-1667966291 192.168.33.41 | CHANGED | rc=0 \u0026gt;\u0026gt; 總用量 269656 -rw------- 1 root root 41537568 11月 7 12:00 etcd-snapshot-rke2-1-1667793600 -rw------- 1 root root 41537568 11月 8 00:00 etcd-snapshot-rke2-1-1667836800 -rw------- 1 root root 41537568 11月 8 12:00 etcd-snapshot-rke2-1-1667880000 -rw------- 1 root root 41537568 11月 9 00:00 etcd-snapshot-rke2-1-1667923200 -rw------- 1 root root 54976544 11月 9 12:00 etcd-snapshot-rke2-1-1667966400 -rw------- 1 root root 54976544 11月 9 11:58 pre-upgrade-snapshot-v2.6.6-rke2-1-1667966289 確認目錄etcd\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 wangken@wangken-MAC rke2-lab % ansible rke2-cluster -m shell -a \u0026#39;ls -l /var/lib/rancher/rke2/server/db/\u0026#39; BECOME password: 192.168.33.41 | CHANGED | rc=0 \u0026gt;\u0026gt; 總用量 0 drwx------ 1 root root 32 11月 9 12:20 etcd drwx------ 1 root root 400 11月 9 12:00 snapshots 192.168.33.43 | CHANGED | rc=0 \u0026gt;\u0026gt; 總用量 0 drwx------ 1 root root 32 11月 9 12:18 etcd drwx------ 1 root root 400 11月 9 12:00 snapshots 192.168.33.42 | CHANGED | rc=0 \u0026gt;\u0026gt; 總用量 0 drwx------ 1 root root 32 11月 9 12:17 etcd drwx------ 1 root root 400 11月 9 12:00 snapshots 停用rke2-server service\n1 2 3 4 5 6 7 8 wangken@wangken-MAC rke2-lab % ansible rke2-cluster -m shell -a \u0026#39;systemctl stop rke2-server\u0026#39; BECOME password: 192.168.33.41 | CHANGED | rc=0 \u0026gt;\u0026gt; 192.168.33.43 | CHANGED | rc=0 \u0026gt;\u0026gt; 192.168.33.42 | CHANGED | rc=0 \u0026gt;\u0026gt; 在rke2-1執行 restore指令並啟動rke2-server service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 rke2 server \\ --cluster-reset \\ --cluster-reset-restore-path=\u0026lt;PATH-TO-SNAPSHOT\u0026gt; rke2-1:/var/lib/rancher/rke2/server/db/snapshots # pwd /var/lib/rancher/rke2/server/db/snapshots rke2-1:/var/lib/rancher/rke2/server/db/snapshots # ll total 269656 -rw------- 1 root root 41537568 Nov 7 12:00 etcd-snapshot-rke2-1-1667793600 -rw------- 1 root root 41537568 Nov 8 00:00 etcd-snapshot-rke2-1-1667836800 -rw------- 1 root root 41537568 Nov 8 12:00 etcd-snapshot-rke2-1-1667880000 -rw------- 1 root root 41537568 Nov 9 00:00 etcd-snapshot-rke2-1-1667923200 -rw------- 1 root root 54976544 Nov 9 12:00 etcd-snapshot-rke2-1-1667966400 -rw------- 1 root root 54976544 Nov 9 11:58 pre-upgrade-snapshot-v2.6.6-rke2-1-1667966289 rke2-1:/var/lib/rancher/rke2/server/db/snapshots # rke2 server --cluster-reset --cluster-reset-restore-path=/var/lib/rancher/rke2/server/db/snapshots/pre-upgrade-snapshot-v2.6.6-rke2-1-1667966289 INFO[0032] Defragmenting etcd database INFO[0032] Reconciling bootstrap data between datastore and disk INFO[0032] Cluster reset: backing up certificates directory to /var/lib/rancher/rke2/server/tls-1667973877 INFO[0033] Defragmenting etcd database INFO[0033] etcd data store connection OK INFO[0033] Waiting for API server to become available INFO[0033] ETCD server is now running INFO[0033] rke2 is up and running WARN[0033] bootstrap key already exists INFO[0033] Reconciling etcd snapshot data in rke2-etcd-snapshots ConfigMap INFO[0035] Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6444/v1-rke2/readyz: 500 Internal Server Error INFO[0037] Managed etcd cluster membership has been reset, restart without --cluster-reset flag now. Backup and delete ${datadir}/server/db on each peer etcd server and rejoin the nodes rke2-1:/var/lib/rancher/rke2/server/db # ll total 0 drwx------ 1 root root 32 Nov 9 14:05 etcd drwx------ 1 root root 32 Nov 9 12:20 etcd-old-1667973863 drwx------ 1 root root 426 Nov 9 14:04 snapshots rke2-1:/var/lib/rancher/rke2/server/db/snapshots # systemctl start rke2-server 刪除剩下兩個節點的 /var/lib/rancher/rke2/server/db\n1 2 3 4 5 6 7 devop@rke2-2:~\u0026gt; sudo cp -r /var/lib/rancher/rke2/server/db /var/lib/rancher/rke2/db_old devop@rke2-2:~\u0026gt; sudo rm -rf /var/lib/rancher/rke2/server/db devop@rke2-3:~\u0026gt; sudo cp -r /var/lib/rancher/rke2/server/db /var/lib/rancher/rke2/db_old [sudo] root 的密碼： devop@rke2-3:~\u0026gt; sudo rm -rf /var/lib/rancher/rke2/server/db devop@rke2-3:~\u0026gt; sudo systemctl start rke2-server 切換到rms確認node狀態\n1 2 3 4 5 devop@rms:~\u0026gt; kubectl get nodes NAME STATUS ROLES AGE VERSION rke2-1 Ready control-plane,etcd,master 14d v1.24.4+rke2r1 rke2-2 Ready control-plane,etcd,master 14d v1.24.4+rke2r1 rke2-3 Ready control-plane,etcd,master 14d v1.24.4+rke2r1 Web UI 版本回到v2.6.6 /var/lib/rancher/rke2/server/db/ 檔案路徑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 wangken@wangken-MAC rke2-lab % ansible rke2-cluster -m shell -a \u0026#39;ls -l /var/lib/rancher/rke2/server/db/\u0026#39; BECOME password: 192.168.33.42 | CHANGED | rc=0 \u0026gt;\u0026gt; 總用量 0 drwx------ 1 root root 32 11月 9 14:16 etcd drwx------ 1 root root 0 11月 9 14:17 snapshots 192.168.33.43 | CHANGED | rc=0 \u0026gt;\u0026gt; 總用量 0 drwx------ 1 root root 32 11月 9 14:15 etcd drwx------ 1 root root 0 11月 9 14:15 snapshots 192.168.33.41 | CHANGED | rc=0 \u0026gt;\u0026gt; 總用量 0 drwx------ 1 root root 32 11月 9 14:05 etcd drwx------ 1 root root 32 11月 9 12:20 etcd-old-1667973863 drwx------ 1 root root 426 11月 9 14:04 snapshots 確認pod status 卡在Terminating\n1 2 3 4 5 6 7 8 devop@rms:~\u0026gt; kubectl get po -o wide -n cattle-system NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rancher-7fd65d9cd6-fjvxr 1/1 Running 0 16m 10.42.0.127 rke2-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; rancher-7fd65d9cd6-hsbhg 0/1 Terminating 0 3h33m \u0026lt;none\u0026gt; rke2-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; rancher-7fd65d9cd6-qz9xb 1/1 Terminating 0 3h33m 10.42.1.58 rke2-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; rancher-7fd65d9cd6-zk2f9 1/1 Running 0 16m 10.42.0.123 rke2-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; rancher-7fd65d9cd6-zrdm8 1/1 Running 0 3h33m 10.42.0.117 rke2-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; rancher-webhook-d6f8b6cb-j2whv 1/1 Running 0 16m 10.42.0.126 rke2-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 強制刪除pod\n1 2 3 4 5 6 7 8 9 10 11 12 13 devop@rms:~\u0026gt; kubectl delete pod --grace-period=0 --force -n cattle-system rancher-7fd65d9cd6-qz9xb warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \u0026#34;rancher-7fd65d9cd6-qz9xb\u0026#34; force deleted devop@rms:~\u0026gt; kubectl delete pod --grace-period=0 --force -n cattle-system rancher-7fd65d9cd6-hsbhg warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \u0026#34;rancher-7fd65d9cd6-hsbhg\u0026#34; force deleted devop@rms:~\u0026gt; kubectl get po -o wide -n cattle-system NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES rancher-7fd65d9cd6-5rglz 0/1 ContainerCreating 0 12m \u0026lt;none\u0026gt; rke2-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; rancher-7fd65d9cd6-fjvxr 1/1 Running 0 29m 10.42.0.127 rke2-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; rancher-7fd65d9cd6-zk2f9 1/1 Running 0 30m 10.42.0.123 rke2-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; rancher-webhook-d6f8b6cb-j2whv 1/1 Running 0 29m 10.42.0.126 rke2-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 重建rke2-2、rke2-3\n1 2 3 sudo systemctl stop rke2-server sudo rke2 server --cluster-reset sudo systemctl start rke2-server 備份Downstream Cluster 調整備份排程,目前每五小時備份一次 /var/lib/rancher/rke2/server/db/snapshots\n還原Downstream Cluster 在Cluster Management可針對Cluster做還原\n選擇要還原的時間點、還原的版本及etcd\n","date":"2023-09-23T07:42:08+08:00","permalink":"https://wangken0129.github.io/p/rancher_rke2_lab/","title":"Rancher_RKE2_Lab"},{"content":"RKE2 and Rancher install 0. Install DNS 1 sudo zypper in -t pattern dhcp_dns_server 1. login 1 2 3 4 5 6 7 8 sam@sam:~\u0026gt; ssh rancher@192.168.122.41 Password: Last failed login: Wed Sep 21 08:33:47 CST 2022 from 192.168.122.1 on ssh:notty There was 1 failed login attempt since the last successful login. Last login: Wed Sep 21 08:31:58 2022 rancher@rms1:~\u0026gt; curl -sfL https://get.rke2.io --output install.sh rancher@rms1:~\u0026gt; chmod +x install.sh 2. config rke2 basic parameters 1 2 3 4 5 6 7 rancher@rms1:~\u0026gt; sudo mkdir -p /etc/rancher/rke2/ [sudo] root 的密碼： rancher@rms1:~\u0026gt; sudo vim /etc/rancher/rke2/config.yaml rancher@rms1:~\u0026gt; cat /etc/rancher/rke2/config.yaml node-name: - \u0026#34;rms1\u0026#34; token: my-shared-secret 3. install rke2 with 1.23.9 1 2 3 4 5 6 7 8 9 10 11 12 rancher@rms1:~\u0026gt; sudo INSTALL_RKE2_CHANNEL=v1.23.9+rke2r1 ./install.sh [WARN] /usr/local is read-only or a mount point; installing to /opt/rke2 [INFO] finding release for channel v1.23.9+rke2r1 [INFO] using v1.23.9+rke2r1 as release [INFO] downloading checksums at https://github.com/rancher/rke2/releases/download/v1.23.9+rke2r1/sha256sum-amd64.txt [INFO] downloading tarball at https://github.com/rancher/rke2/releases/download/v1.23.9+rke2r1/rke2.linux-amd64.tar.gz [INFO] verifying tarball [INFO] unpacking tarball file to /opt/rke2 [INFO] updating tarball contents to reflect install path [INFO] moving systemd units to /etc/systemd/system [INFO] install complete; you may want to run: export PATH=$PATH:/opt/rke2/bin rancher@rms1:~\u0026gt; export PATH=$PATH:/opt/rke2/bin 4. enable rke2 and setup kubeconfig 1 2 3 4 5 6 7 8 rancher@rms1:~\u0026gt; sudo systemctl enable rke2-server Created symlink /etc/systemd/system/multi-user.target.wants/rke2-server.service → /etc/systemd/system/rke2-server.service. rancher@rms1:~\u0026gt; sudo systemctl start rke2-server rancher@rms1:~\u0026gt; mkdir .kube rancher@rms1:~\u0026gt; sudo cp /etc/rancher/rke2/rke2.yaml .kube/config [sudo] root 的密碼： rancher@rms1:~\u0026gt; sudo chown rancher .kube/config rancher@rms1:~\u0026gt; sudo cp /var/lib/rancher/rke2/bin/kubectl /usr/local/bin/ 5. check pod status 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 rancher@rms1:~\u0026gt; kubectl get po No resources found in default namespace. rancher@rms1:~\u0026gt; kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cloud-controller-manager-rms1 1/1 Running 0 15m kube-system etcd-rms1 1/1 Running 0 15m kube-system helm-install-rke2-canal-6bpd4 0/1 Completed 0 15m kube-system helm-install-rke2-coredns-mjflj 0/1 Completed 0 15m kube-system helm-install-rke2-ingress-nginx-76r2c 0/1 Completed 0 15m kube-system helm-install-rke2-metrics-server-wkc4k 0/1 Completed 0 15m kube-system kube-apiserver-rms1 1/1 Running 0 15m kube-system kube-controller-manager-rms1 1/1 Running 0 15m kube-system kube-proxy-rms1 1/1 Running 0 15m kube-system kube-scheduler-rms1 1/1 Running 0 15m kube-system rke2-canal-8x56p 2/2 Running 0 15m kube-system rke2-coredns-rke2-coredns-545d64676-zlnk9 1/1 Running 0 15m kube-system rke2-coredns-rke2-coredns-autoscaler-5dd676f5c7-zrdbb 1/1 Running 0 15m kube-system rke2-ingress-nginx-controller-xhxr6 1/1 Running 0 14m kube-system rke2-metrics-server-6564db4569-542hx 1/1 Running 0 14m 6. install helm3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 rancher@rms1:~\u0026gt; wget https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz --2022-09-21 09:06:57-- https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz Resolving get.helm.sh (get.helm.sh)... 152.199.39.108, 2606:2800:247:1cb7:261b:1f9c:2074:3c Connecting to get.helm.sh (get.helm.sh)|152.199.39.108|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 13633605 (13M) [application/x-tar] Saving to: ‘helm-v3.8.2-linux-amd64.tar.gz’ helm-v3.8.2-linux-amd64.tar.gz 100%[=============================================================\u0026gt;] 13.00M 5.95MB/s in 2.2s 2022-09-21 09:07:00 (5.95 MB/s) - ‘helm-v3.8.2-linux-amd64.tar.gz’ saved [13633605/13633605] rancher@rms1:~\u0026gt; tar zxvf helm-v3.8.2-linux-amd64.tar.gz linux-amd64/ linux-amd64/helm linux-amd64/LICENSE linux-amd64/README.md rancher@rms1:~\u0026gt; ls bin helm-v3.8.2-linux-amd64.tar.gz install.sh linux-amd64 public_html rancher@rms1:~\u0026gt; sudo cp linux-amd64/helm /usr/local/bin/ [sudo] root 的密碼： rancher@rms1:~\u0026gt; helm --help The Kubernetes package manager Common actions for Helm: - helm search: search for charts - helm pull: download a chart to your local directory to view - helm install: upload the chart to Kubernetes - helm list: list releases of charts ... ... ... 7. install rancher and cert-manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 rancher@rms1:~\u0026gt; helm repo add rancher-stable https://releases.rancher.com/server-charts/stable \u0026#34;rancher-stable\u0026#34; has been added to your repositories rancher@rms1:~\u0026gt; kubectl create namespace cattle-system namespace/cattle-system created rancher@rms1:~\u0026gt; kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.7.1/cert-manager.crds.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created rancher@rms1:~\u0026gt; helm repo add jetstack https://charts.jetstack.io \u0026#34;jetstack\u0026#34; has been added to your repositories rancher@rms1:~\u0026gt; helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;rancher-stable\u0026#34; chart repository ...Successfully got an update from the \u0026#34;jetstack\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ rancher@rms1:~\u0026gt; helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.7.1 NAME: cert-manager LAST DEPLOYED: Wed Sep 21 09:11:15 2022 NAMESPACE: cert-manager STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: cert-manager v1.7.1 has been deployed successfully! In order to begin issuing certificates, you will need to set up a ClusterIssuer or Issuer resource (for example, by creating a \u0026#39;letsencrypt-staging\u0026#39; issuer). More information on the different types of issuers and how to configure them can be found in our documentation: https://cert-manager.io/docs/configuration/ For information on how to configure cert-manager to automatically provision Certificates for Ingress resources, take a look at the `ingress-shim` documentation: https://cert-manager.io/docs/usage/ingress/ rancher@rms1:~\u0026gt; kubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-76d44b459c-zhpp2 1/1 Running 0 32s cert-manager-cainjector-9b679cc6-6tzd8 1/1 Running 0 32s cert-manager-webhook-57c994b6b9-4dfvs 1/1 Running 0 32s rancher@rms1:~\u0026gt; helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=rancher.example.com --version 2.6.6 NAME: rancher LAST DEPLOYED: Wed Sep 21 09:14:06 2022 NAMESPACE: cattle-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Rancher Server has been installed. NOTE: Rancher may take several minutes to fully initialize. Please standby while Certificates are being issued, Containers are started and the Ingress rule comes up. Check out our docs at https://rancher.com/docs/ If you provided your own bootstrap password during installation, browse to https://rancher.example.com to get started. If this is the first time you installed Rancher, get started by running this command and clicking the URL it generates: echo https://rancher.example.com/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}\u0026#39;) To get just the bootstrap password on its own, run: kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}{{ \u0026#34;\\n\u0026#34; }}\u0026#39; Happy Containering! 8. check rancher status 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 rancher@rms1:~\u0026gt; kubectl -n cattle-system get po NAME READY STATUS RESTARTS AGE rancher-7fd65d9cd6-8krrq 0/1 ContainerCreating 0 16s rancher-7fd65d9cd6-h28fw 0/1 ContainerCreating 0 16s rancher-7fd65d9cd6-k9hrr 0/1 ContainerCreating 0 16s rancher@rms1:~\u0026gt; watch kubectl -n cattle-system get po rancher@rms1:~\u0026gt; kubectl -n cattle-system rollout status deploy/rancher Waiting for deployment \u0026#34;rancher\u0026#34; rollout to finish: 0 of 3 updated replicas are available... Waiting for deployment spec update to be observed... Waiting for deployment \u0026#34;rancher\u0026#34; rollout to finish: 0 of 3 updated replicas are available... Waiting for deployment \u0026#34;rancher\u0026#34; rollout to finish: 1 of 3 updated replicas are available... Waiting for deployment \u0026#34;rancher\u0026#34; rollout to finish: 2 of 3 updated replicas are available... deployment \u0026#34;rancher\u0026#34; successfully rolled out rancher@rms1:~\u0026gt; kubectl -n cattle-system get po NAME READY STATUS RESTARTS AGE rancher-7fd65d9cd6-8krrq 1/1 Running 1 (51s ago) 3m11s rancher-7fd65d9cd6-h28fw 1/1 Running 0 3m11s rancher-7fd65d9cd6-k9hrr 1/1 Running 1 (51s ago) 3m11s notice must use DNS for rancher. downstream must resolve rancher portal from dns. good luck. ","date":"2023-09-23T07:39:03+08:00","permalink":"https://wangken0129.github.io/p/guide_rke2rancher_install/","title":"Guide_RKE2\u0026Rancher_install"},{"content":"NeuVector 0. install RKE2 0.1. 下載安裝腳本 1 2 rancher@rms1:~\u0026gt; curl -sfL https://get.rke2.io --output install.sh rancher@rms1:~\u0026gt; chmod +x install.sh 0.2. 設定RKE2基本餐數 1 2 3 4 5 6 7 rancher@rms1:~\u0026gt; sudo mkdir -p /etc/rancher/rke2/ [sudo] root 的密碼： rancher@rms1:~\u0026gt; sudo vim /etc/rancher/rke2/config.yaml rancher@rms1:~\u0026gt; cat /etc/rancher/rke2/config.yaml node-name: - \u0026#34;rms1\u0026#34; token: my-shared-secret 0.3. 安裝rke2 1 2 3 4 5 6 7 8 9 10 11 12 rancher@rms1:~\u0026gt; sudo ./install.sh [WARN] /usr/local is read-only or a mount point; installing to /opt/rke2 [INFO] finding release for channel v1.24.6+rke2r1 [INFO] using v1.24.6+rke2r1 as release [INFO] downloading checksums at https://github.com/rancher/rke2/releases/download/v1.24.6+rke2r1/sha256sum-amd64.txt [INFO] downloading tarball at https://github.com/rancher/rke2/releases/download/v1.24.6+rke2r1/rke2.linux-amd64.tar.gz [INFO] verifying tarball [INFO] unpacking tarball file to /opt/rke2 [INFO] updating tarball contents to reflect install path [INFO] moving systemd units to /etc/systemd/system [INFO] install complete; you may want to run: export PATH=$PATH:/opt/rke2/bin rancher@rms1:~\u0026gt; export PATH=$PATH:/opt/rke2/bin 0.4. enable rke2 and setup kubeconfig 1 2 3 4 5 6 7 8 rancher@rms1:~\u0026gt; sudo systemctl enable rke2-server Created symlink /etc/systemd/system/multi-user.target.wants/rke2-server.service → /etc/systemd/system/rke2-server.service. rancher@rms1:~\u0026gt; sudo systemctl start rke2-server rancher@rms1:~\u0026gt; mkdir .kube rancher@rms1:~\u0026gt; sudo cp /etc/rancher/rke2/rke2.yaml .kube/config [sudo] root 的密碼： rancher@rms1:~\u0026gt; sudo chown rancher .kube/config rancher@rms1:~\u0026gt; sudo cp /var/lib/rancher/rke2/bin/kubectl /usr/local/bin/ 0.5. check pod status 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 rancher@rms1:~\u0026gt; kubectl get po No resources found in default namespace. rancher@rms1:~\u0026gt; kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cloud-controller-manager-rms1 1/1 Running 0 15m kube-system etcd-rms1 1/1 Running 0 15m kube-system helm-install-rke2-canal-6bpd4 0/1 Completed 0 15m kube-system helm-install-rke2-coredns-mjflj 0/1 Completed 0 15m kube-system helm-install-rke2-ingress-nginx-76r2c 0/1 Completed 0 15m kube-system helm-install-rke2-metrics-server-wkc4k 0/1 Completed 0 15m kube-system kube-apiserver-rms1 1/1 Running 0 15m kube-system kube-controller-manager-rms1 1/1 Running 0 15m kube-system kube-proxy-rms1 1/1 Running 0 15m kube-system kube-scheduler-rms1 1/1 Running 0 15m kube-system rke2-canal-8x56p 2/2 Running 0 15m kube-system rke2-coredns-rke2-coredns-545d64676-zlnk9 1/1 Running 0 15m kube-system rke2-coredns-rke2-coredns-autoscaler-5dd676f5c7-zrdbb 1/1 Running 0 15m kube-system rke2-ingress-nginx-controller-xhxr6 1/1 Running 0 14m kube-system rke2-metrics-server-6564db4569-542hx 1/1 Running 0 14m 1. install helm3 1.1. check kubernetes version 1 2 3 rancher@rms1:~\u0026gt; kubectl get no NAME STATUS ROLES AGE VERSION rms1 Ready control-plane,etcd,master 37h v1.24.6+rke2r1 1.2. 下載與安裝helm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 rancher@rms1:~\u0026gt; wget https://get.helm.sh/helm-v3.9.4-linux-amd64.tar.gz --2022-10-16 12:34:56-- https://get.helm.sh/helm-v3.9.4-linux-amd64.tar.gz Resolving get.helm.sh (get.helm.sh)... 152.199.39.108, 2606:2800:247:1cb7:261b:1f9c:2074:3c Connecting to get.helm.sh (get.helm.sh)|152.199.39.108|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 14026634 (13M) [application/x-tar] Saving to: ‘helm-v3.9.4-linux-amd64.tar.gz’ helm-v3.9.4-linux-amd64.tar 100%[========================================\u0026gt;] 13.38M 7.86MB/s in 1.7s 2022-10-16 12:34:59 (7.86 MB/s) - ‘helm-v3.9.4-linux-amd64.tar.gz’ saved [14026634/14026634] rancher@rms1:~\u0026gt; tar zxvf helm-v3.9.4-linux-amd64.tar.gz linux-amd64/ linux-amd64/helm linux-amd64/LICENSE linux-amd64/README.md rancher@rms1:~\u0026gt; sudo cp linux-amd64/helm /usr/local/bin/ [sudo] password for root: rancher@rms1:~\u0026gt; helm --help The Kubernetes package manager Common actions for Helm: - helm search: search for charts - helm pull: download a chart to your local directory to view - helm install: upload the chart to Kubernetes - helm list: list releases of charts ... ... ... 2. install NeuVector 2.1. 加入neuvector helm chart 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 rancher@rms1:~\u0026gt; helm repo add neuvector https://neuvector.github.io/neuvector-helm/ \u0026#34;neuvector\u0026#34; has been added to your repositories rancher@rms1:~\u0026gt; helm search repo neuvector/core NAME CHART VERSION\tAPP VERSION\tDESCRIPTION neuvector/core\t2.2.3 5.0.3 Helm chart for NeuVector\u0026#39;s core services rancher@rms1:~\u0026gt; helm search repo neuvector/core -l NAME CHART VERSION\tAPP VERSION\tDESCRIPTION neuvector/core\t2.2.3 5.0.3 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t2.2.2 5.0.2 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t2.2.1 5.0.1 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t2.2.0 5.0.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.9.2 4.4.4-s2 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.9.1 4.4.4 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.9.0 4.4.4 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.8.9 4.4.3 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.8.8 4.4.2 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.8.7 4.4.1 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.8.6 4.4.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.8.5 4.3.2 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.8.4 4.3.2 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.8.3 4.3.2 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.8.2 4.3.1 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.8.0 4.3.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.7.7 4.2.2 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.7.6 4.2.2 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.7.5 4.2.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.7.2 4.2.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.7.1 4.2.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.7.0 4.0.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.6.9 4.0.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.6.8 4.0.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.6.7 4.0.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.6.6 4.0.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.6.5 4.0.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.6.4 4.0.0 Helm chart for NeuVector\u0026#39;s core services neuvector/core\t1.6.1 4.0.0 NeuVector Full Lifecycle Container Security Pla... rancher@rms1:~\u0026gt; helm show values neuvector/core --version 2.2.3 \u0026gt; neuvector-values.yaml 2.2. 調整yaml - part-1(replicas for test) 1 2 3 4 5 6 7 8 9 10 11 controller: ... replicas: 1 ... ... ... scanner: enabled: true replicas: 1 ... ... 2.3. 調整yaml - part-2(CRI for k3s) 1 2 k3s: enabled: true 2.4. 建立NeuVector NameSpace and create neuvector 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 rancher@rms1:~\u0026gt; kubectl create ns neuvector namespace/neuvector created rancher@rms1:~\u0026gt; helm install neuvector neuvector/core --version 2.2.3 --namespace neuvector --values neuvector-values.yaml NAME: neuvector LAST DEPLOYED: Sun Oct 16 13:10:22 2022 NAMESPACE: neuvector STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Get the NeuVector URL by running these commands: NODE_PORT=$(kubectl get --namespace neuvector -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services neuvector-service-webui) NODE_IP=$(kubectl get nodes --namespace neuvector -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo https://$NODE_IP:$NODE_PORT rancher@rms1:~\u0026gt; kubectl -n neuvector get po NAME READY STATUS RESTARTS AGE neuvector-controller-pod-6cf58c7894-hvvlw 1/1 Running 0 42s neuvector-enforcer-pod-rpzqg 1/1 Running 0 42s neuvector-manager-pod-8488cb586c-87q9n 1/1 Running 0 42s neuvector-scanner-pod-7d6bc8f775-pfwkk 1/1 Running 0 42s rancher@rms1:~\u0026gt; kubectl -n neuvector get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE neuvector-service-webui NodePort 10.43.189.243 \u0026lt;none\u0026gt; 8443:30805/TCP 66s neuvector-svc-admission-webhook ClusterIP 10.43.103.17 \u0026lt;none\u0026gt; 443/TCP 67s neuvector-svc-controller ClusterIP None \u0026lt;none\u0026gt; 18300/TCP,18301/TCP,18301/UDP 67s neuvector-svc-crd-webhook ClusterIP 10.43.185.195 \u0026lt;none\u0026gt; 443/TCP 66s 3. 其他 3.1. image registry Name: Nginx Registry: https://registry.hub.docker.com/ Filter: nginx:stable 3.2. Response Rule Category: Security Event Group: nv.default Criteria: name:Process.Profile.Violation Action: Quarantine Status: Enabled 3.3. CLI reference 3.3.1. nginx install recon-ng 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 rancher@rms1:~\u0026gt; kubectl create deploy web --image=nginx --port=80 --replicas=1 deployment.apps/web created rancher@rms1:~\u0026gt; kubectl get po NAME READY STATUS RESTARTS AGE web-cff6559d7-jddwt 1/1 Running 0 8s rancher@rms1:~\u0026gt; kubectl exec -it web-cff6559d7-jddwt -- /bin/bash root@web-cff6559d7-jddwt:/# apt update Get:1 http://deb.debian.org/debian bullseye InRelease [116 kB] Get:2 http://deb.debian.org/debian-security bullseye-security InRelease [48.4 kB] Get:3 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB] Get:4 http://deb.debian.org/debian bullseye/main amd64 Packages [8184 kB] Get:5 http://deb.debian.org/debian-security bullseye-security/main amd64 Packages [189 kB] Get:6 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [6340 B] Fetched 8588 kB in 8s (1050 kB/s) Reading package lists... Done Building dependency tree... Done Reading state information... Done 1 package can be upgraded. Run \u0026#39;apt list --upgradable\u0026#39; to see it. root@web-cff6559d7-jddwt:/# apt install recon-ng Reading package lists... Done Building dependency tree... Done Reading state information... Done The following additional packages will be installed: javascript-common libgpm2 libjs-jquery libjs-skeleton libjs-sphinxdoc libjs-underscore libmpdec3 ... ... ... 0 upgraded, 69 newly installed, 0 to remove and 1 not upgraded. Need to get 13.0 MB of archives. After this operation, 55.7 MB of additional disk space will be used. Do you want to continue? [Y/n] y ... ... ... Setting up recon-ng (5.1.1-3) ... Processing triggers for libc-bin (2.31-13+deb11u4) ... root@web-cff6559d7-jddwt:/# recon-ng [*] Version check disabled. _/_/_/ _/_/_/_/ _/_/_/ _/_/_/ _/ _/ _/ _/ _/_/_/ _/ _/ _/ _/ _/ _/ _/_/ _/ _/_/ _/ _/ _/_/_/ _/_/_/ _/ _/ _/ _/ _/ _/ _/_/_/_/ _/ _/ _/ _/ _/_/_/ _/ _/ _/ _/ _/ _/ _/ _/_/ _/ _/_/ _/ _/ _/ _/ _/_/_/_/ _/_/_/ _/_/_/ _/ _/ _/ _/ _/_/_/ /\\ / \\\\ /\\ Sponsored by... /\\ /\\/ \\\\V \\/\\ / \\\\/ // \\\\\\\\\\ \\\\ \\/\\ // // BLACK HILLS \\/ \\\\ www.blackhillsinfosec.com ____ ____ ____ ____ _____ _ ____ ____ ____ |____] | ___/ |____| | | | |____ |____ | | | \\_ | | |____ | | ____| |____ |____ www.practisec.com [recon-ng v5.1.1, Tim Tomes (@lanmaster53)] [*] No modules enabled/installed. [recon-ng][default] \u0026gt; 3.3.2. nginx bash 1 2 3 4 rancher@rms1:~\u0026gt; kubectl get po NAME READY STATUS RESTARTS AGE web-cff6559d7-jddwt 1/1 Running 0 5m4s rancher@rms1:~\u0026gt; kubectl exec -it web-cff6559d7-jddwt -- /bin/bash ","date":"2023-09-23T07:38:37+08:00","permalink":"https://wangken0129.github.io/p/guide_neuvector/","title":"Guide_NeuVector"},{"content":"rke2 offline install 1. 下載1.24.6離線安裝所需image 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 rancher@rms1:~\u0026gt; sudo mkdir /root/rke2-artifacts \u0026amp;\u0026amp; cd /root/rke2-artifacts/ mkdir: cannot create directory ‘/root/rke2-artifacts’: File exists rancher@rms1:~\u0026gt; sudo su rms1:/home/rancher # cd /root/rke2-artifacts/ rms1:~/rke2-artifacts # ll total 0 rms1:~/rke2-artifacts # curl -OLs https://github.com/rancher/rke2/releases/download/v1.24.6%2Brke2r1/rke2-images.linux-amd64.tar.zst rms1:~/rke2-artifacts # curl -OLs https://github.com/rancher/rke2/releases/download/v1.24.6%2Brke2r1/rke2.linux-amd64.tar.gz rms1:~/rke2-artifacts # curl -OLs https://github.com/rancher/rke2/releases/download/v1.24.6%2Brke2r1/sha256sum-amd64.txt rms1:~/rke2-artifacts # curl -sfL https://get.rke2.io --output install.sh rms1:~/rke2-artifacts # ll total 823052 -rw-r--r-- 1 root root 21438 Oct 11 20:19 install.sh -rw-r--r-- 1 root root 794531974 Oct 11 20:17 rke2-images.linux-amd64.tar.zst -rw-r--r-- 1 root root 48238609 Oct 11 20:18 rke2.linux-amd64.tar.gz -rw-r--r-- 1 root root 3626 Oct 11 20:18 sha256sum-amd64.txt 2. 解壓縮與基本設定 1 2 3 4 5 6 7 8 9 10 11 12 13 14 rms1:~/rke2-artifacts # INSTALL_RKE2_ARTIFACT_PATH=/root/rke2-artifacts sh install.sh [WARN] /usr/local is read-only or a mount point; installing to /opt/rke2 [INFO] staging local checksums from /root/rke2-artifacts/sha256sum-amd64.txt [INFO] staging zst airgap image tarball from /root/rke2-artifacts/rke2-images.linux-amd64.tar.zst [INFO] staging tarball from /root/rke2-artifacts/rke2.linux-amd64.tar.gz [INFO] verifying airgap tarball grep: /tmp/rke2-install.XRvs61dJ7e/rke2-images.checksums: No such file or directory [INFO] installing airgap tarball to /var/lib/rancher/rke2/agent/images [INFO] verifying tarball [INFO] unpacking tarball file to /opt/rke2 [INFO] updating tarball contents to reflect install path [INFO] moving systemd units to /etc/systemd/system [INFO] install complete; you may want to run: export PATH=$PATH:/opt/rke2/bin rms1:~/rke2-artifacts # export PATH=$PATH:/opt/rke2/bin 3. 叢集基礎組態 1 2 3 4 5 6 7 rms1:~/rke2-artifacts # mkdir -p /etc/rancher/rke2/ [sudo] root 的密碼： rms1:~/rke2-artifacts # vim /etc/rancher/rke2/config.yaml rms1:~/rke2-artifacts # cat /etc/rancher/rke2/config.yaml node-name: - \u0026#34;rms1\u0026#34; token: my-shared-secret 4. 啟用RKE2服務 1 2 rms1:~/rke2-artifacts # systemctl enable --now rke2-server Created symlink /etc/systemd/system/multi-user.target.wants/rke2-server.service → /etc/systemd/system/rke2-server.service. 5. 設定一般帳號使用kubectl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 rms1:~/rke2-artifacts # exit exit rancher@rms1:~\u0026gt; mkdir .kube rancher@rms1:~\u0026gt; sudo cp /etc/rancher/rke2/rke2.yaml .kube/config [sudo] password for root: rancher@rms1:~\u0026gt; sudo chown rancher .kube/config rancher@rms1:~\u0026gt; sudo cp /var/lib/rancher/rke2/bin/kubectl /usr/local/bin/ rancher@rms1:~\u0026gt; kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cloud-controller-manager-rms1 1/1 Running 0 3m5s kube-system etcd-rms1 1/1 Running 0 2m46s kube-system helm-install-rke2-canal-lpv22 0/1 Completed 0 3m kube-system helm-install-rke2-coredns-xndpj 0/1 Completed 0 3m kube-system helm-install-rke2-ingress-nginx-5r5sq 0/1 Completed 0 3m kube-system helm-install-rke2-metrics-server-wkz6c 0/1 Completed 0 3m kube-system kube-apiserver-rms1 1/1 Running 0 2m37s kube-system kube-controller-manager-rms1 1/1 Running 0 2m30s kube-system kube-proxy-rms1 1/1 Running 0 2m57s kube-system kube-scheduler-rms1 1/1 Running 0 2m37s kube-system rke2-canal-clqp4 2/2 Running 0 2m41s kube-system rke2-coredns-rke2-coredns-76cb76d66-xpnqg 1/1 Running 0 2m42s kube-system rke2-coredns-rke2-coredns-autoscaler-58867f8fc5-hzz2l 1/1 Running 0 2m42s kube-system rke2-ingress-nginx-controller-zfbvx 1/1 Running 0 93s kube-system rke2-metrics-server-6979d95f95-kmbcv 1/1 Running 0 109s 6. 加入其他節點 7. 安裝helm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 rancher@rms1:~\u0026gt; wget https://get.helm.sh/helm-v3.9.4-linux-amd64.tar.gz --2022-10-11 20:33:57-- https://get.helm.sh/helm-v3.9.4-linux-amd64.tar.gz Resolving get.helm.sh (get.helm.sh)... 152.199.39.108, 2606:2800:247:1cb7:261b:1f9c:2074:3c Connecting to get.helm.sh (get.helm.sh)|152.199.39.108|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 14026634 (13M) [application/x-tar] Saving to: ‘helm-v3.9.4-linux-amd64.tar.gz’ helm-v3.9.4-linux-amd64.tar.gz 100%[======================================================\u0026gt;] 13.38M 11.0MB/s in 1.2s 2022-10-11 20:34:00 (11.0 MB/s) - ‘helm-v3.9.4-linux-amd64.tar.gz’ saved [14026634/14026634] rancher@rms1:~\u0026gt; tar zxvf helm-v3.9.4-linux-amd64.tar.gz linux-amd64/ linux-amd64/helm linux-amd64/LICENSE linux-amd64/README.md rancher@rms1:~\u0026gt; sudo cp linux-amd64/helm /usr/local/bin/ 8. 啟用cert-manager(optional) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 rancher@rms1:~\u0026gt; kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.7.1/cert-manager.crds.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created rancher@rms1:~\u0026gt; helm repo add jetstack https://charts.jetstack.io \u0026#34;jetstack\u0026#34; has been added to your repositories rancher@rms1:~\u0026gt; helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;jetstack\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ rancher@rms1:~\u0026gt; helm install cert-manager jetstack/cert-manager \\ \u0026gt; --namespace cert-manager \\ \u0026gt; --create-namespace \\ \u0026gt; --version v1.7.1 NAME: cert-manager LAST DEPLOYED: Tue Oct 11 20:35:43 2022 NAMESPACE: cert-manager STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: cert-manager v1.7.1 has been deployed successfully! In order to begin issuing certificates, you will need to set up a ClusterIssuer or Issuer resource (for example, by creating a \u0026#39;letsencrypt-staging\u0026#39; issuer). More information on the different types of issuers and how to configure them can be found in our documentation: https://cert-manager.io/docs/configuration/ For information on how to configure cert-manager to automatically provision Certificates for Ingress resources, take a look at the `ingress-shim` documentation: https://cert-manager.io/docs/usage/ingress/ rancher@rms1:~\u0026gt; kubectl -n cert-manager get po NAME READY STATUS RESTARTS AGE cert-manager-646c67487-p9w77 1/1 Running 0 72s cert-manager-cainjector-7cb8669d6b-cwghz 1/1 Running 0 72s cert-manager-webhook-696c5db7ff-sjbw4 1/1 Running 0 72s 9. 啟用Rancher 2.6.8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 rancher@rms1:~\u0026gt; helm repo add rancher-stable https://releases.rancher.com/server-charts/stable \u0026#34;rancher-stable\u0026#34; has been added to your repositories rancher@rms1:~\u0026gt; kubectl create namespace cattle-system namespace/cattle-system created rancher@rms1:~\u0026gt; helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;jetstack\u0026#34; chart repository ...Successfully got an update from the \u0026#34;rancher-stable\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ rancher@rms1:~\u0026gt; helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=rancher.example.com NAME: rancher LAST DEPLOYED: Tue Oct 11 20:37:49 2022 NAMESPACE: cattle-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Rancher Server has been installed. NOTE: Rancher may take several minutes to fully initialize. Please standby while Certificates are being issued, Containers are started and the Ingress rule comes up. Check out our docs at https://rancher.com/docs/ If you provided your own bootstrap password during installation, browse to https://rancher.example.com to get started. If this is the first time you installed Rancher, get started by running this command and clicking the URL it generates: echo https://rancher.example.com/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}\u0026#39;) To get just the bootstrap password on its own, run: kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}{{ \u0026#34;\\n\u0026#34; }}\u0026#39; Happy Containering! rancher@rms1:~\u0026gt; kubectl -n cattle-system get po NAME READY STATUS RESTARTS AGE rancher-69595dc9c4-mrhwt 0/1 ContainerCreating 0 2m17s rancher-69595dc9c4-nhl5m 0/1 ContainerCreating 0 2m17s rancher-69595dc9c4-tswd8 0/1 ContainerCreating 0 2m17s rancher@rms1:~\u0026gt; kubectl -n cattle-system get po NAME READY STATUS RESTARTS AGE rancher-69595dc9c4-mrhwt 0/1 Running 0 2m27s rancher-69595dc9c4-nhl5m 0/1 Running 0 2m27s rancher-69595dc9c4-tswd8 0/1 ContainerCreating 0 2m27s rancher@rms1:~\u0026gt; kubectl -n cattle-system get po NAME READY STATUS RESTARTS AGE rancher-69595dc9c4-mrhwt 1/1 Running 2 (77s ago) 7m12s rancher-69595dc9c4-nhl5m 1/1 Running 1 (2m39s ago) 7m12s rancher-69595dc9c4-tswd8 1/1 Running 2 (77s ago) 7m12s ","date":"2023-09-23T07:02:38+08:00","permalink":"https://wangken0129.github.io/p/guide_rke2_offline_install/","title":"Guide_rke2_offline_install"},{"content":"Gigamon UCT on OCP https://docs.gigamon.com/pdfs/Content/Resources/PDF%20Library/GV-6000-Doc/Universal-Container-Tap-Guide-v60.pdf#page18\n安裝Gigamon UCT 在RedHat Openshift上，讓Gigamon可以監看Openshift的流量，\nUCT會透過daemon set的方式安裝在每個Node上面，所以需要修改Project的權限。\n安裝步驟 create new project , 並加ssc給此project\n1 2 3 oc new-project oc adm policy add-scc-to-user anyuid -z default oc adm policy add-scc-to-user privileged -n gigamon -z default Gigamon UTC-Controller,修改FM IP、Port (default 443)、External DNS、API等等 以及image版本,要注意第一個執行目錄會有所不同,可以的話用podman run起來看看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 [ocpadmin@Bastion gigamon]$ cat uct-controller.yaml # Copyright 2020 Gigamon Inc. ################################################################################################## # UCT service ################################################################################################## apiVersion: v1 kind: Service metadata: name: gigamon-uct-cntlr-service labels: app: uct-cntlr service: gigamon-uct-cntlr-service # change the namespace tp match your namespace namespace: gigamon spec: ports: - port: 8443 protocol: TCP name: uct-rest targetPort: 8443 - port: 42042 protocol: TCP name: uct-stats targetPort: 42042 selector: app: uct-cntlr --- apiVersion: apps/v1 kind: Deployment metadata: name: uct-cntlr-v1 # change the namespace tp match your namespace namespace: gigamon labels: app: uct-cntlr version: v1 spec: replicas: 1 selector: matchLabels: app: uct-cntlr version: v1 template: metadata: labels: app: uct-cntlr version: v1 spec: # this below serviceAccountName should match with the ServiceAccount # name under ClusterRoleBinding section.It is recommended to use # a non default name as customers may use name of their choice. # non default serviceAccountName should be created using the kubectl. # kubectl create serviceaccount \u0026lt;non default name\u0026gt; -n \u0026lt;namespace\u0026gt; serviceAccountName: default containers: - name: uct-cntlr image: gigamon/uct-controller:6.2.00-374893 # Usage: # /uct-cntlr # \u0026lt;FM IP\u0026gt; # \u0026lt;FM REST Svc Port\u0026gt; # \u0026lt;UCT-Cntlr REST SVC Port\u0026gt; # \u0026lt;mTLS Mode: 1(ON)|0(OFF)) # \u0026lt;Cert Path\u0026gt; # \u0026lt;Cert file\u0026gt; # \u0026lt;Pvt Key\u0026gt; # \u0026lt;CA-Root\u0026gt; command: - /uct-controller - \u0026#34;192.168.85.197\u0026#34; - \u0026#39;443\u0026#39; - \u0026#39;8443\u0026#39; - \u0026#39;0\u0026#39; - \u0026#34;/etc/gcbcerts\u0026#34; - \u0026#34;gcb-cert.pem\u0026#34; - \u0026#34;gcb-pvt-key.pem\u0026#34; - \u0026#34;gcb-ca-root-cert.pem\u0026#34; #command: [/giga_setup_init] imagePullPolicy: Always ports: - containerPort: 8443 - containerPort: 42042 env: # Service name.Shoudl match name specified in metadata section above. - name: UCT_CNTLR_SERVICE_NAME value: \u0026#34;gigamon-uct-cntlr-service\u0026#34; # External LB balancer IP, for controller (FM) to connect to uct-cntlr - name: UCT_CNTLR_EXT_IP_DNS value: \u0026#34;gigamon-uct.ocp.dynasafelab.local\u0026#34; # K8S cluster end-point (typically, master nodes with default port of 6443) # example: kubectl cluster-info cmd should give this value - name: K8S_CLUSTER_ENDPOINT value: \u0026#34;https://api.ocp.dynasafelab.local:6443\u0026#34; # This value is upto user to specify. Will accept any value - name: FM_FQDN value: \u0026#34;fm.dynasafelab.local\u0026#34; # Namespace of pod - gets from metadata - name: UCT_CNTLR_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace # This value is to enable inventory collection through yaml file. # true means enable, false or any other value means disable - name: UCT_CNTLR_INVENTORY_COLLECTION_ENABLE value: \u0026#34;false\u0026#34; # We are specifying HOME dir as /pod-data to work with SCC restrictions - name: HOME value: \u0026#34;/pod-data\u0026#34; # For No mTLS Case, Volume Mount is Optional. volumeMounts: - mountPath: /etc/gcbcerts name: pki-path - mountPath: /pod-data name: shared-data volumes: - emptyDir: {} name: shared-data - emptyDir: {} name: pki-path imagePullSecrets: - name: gig-dock-regkey --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pods-list rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;,\u0026#34;services\u0026#34;, \u0026#34;nodes\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;] verbs: [\u0026#34;list\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pods-list subjects: - kind: ServiceAccount name: default #change the below namespace as per the requirement namespace: gigamon roleRef: kind: ClusterRole name: pods-list apiGroup: rbac.authorization.k8s.io Gigamon UTC-Tap , 注意此為Daemonset 所以一定要加scc不然會過不了 我把原文件的SecurityContext刪除,改用scc 修改service的名稱 、image版本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 [ocpadmin@Bastion gigamon]$ cat uct-tap.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: gigamon-uct labels: app: gigamon-uct version: v1 spec: selector: matchLabels: app: gigamon-uct version: v1 template: metadata: labels: app: gigamon-uct version: v1 spec: restartPolicy: Always hostPID: true hostNetwork: true terminationGracePeriodSeconds: 30 securityContext: {} containers: - resources: limits: cpu: \u0026#39;1\u0026#39; memory: 512Mi requests: cpu: \u0026#39;1\u0026#39; memory: 512Mi name: gigamon-uct command: - /uctapp/uct - \u0026#39;1\u0026#39; - \u0026#39;1\u0026#39; - \u0026#39;3\u0026#39; - \u0026#39;0\u0026#39; - \u0026#39;55\u0026#39; env: - name: LD_LIBRARY_PATH value: /usr/lib64 - name: UCT_DEBUG_MODE value: \u0026#39;0x0A000004\u0026#39; - name: UCT_SERVICE_NAME value: UCT_HTTP_SERVICE - name: UCT_CNTLR_SVC_DNS value: gigamon-uct-cntlr-service - name: UCT_CNTLR_REST_SVC_PORT value: \u0026#39;8443\u0026#39; - name: UCT_WORKERNODE_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: UCT_POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: UCT_POD_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP - name: UCT_NOKIA_SCP_VTAP value: \u0026#39;0\u0026#39; ports: - hostPort: 9443 containerPort: 9443 protocol: TCP imagePullPolicy: IfNotPresent volumeMounts: - name: socket mountPath: /var/run/containerd/containerd.sock - name: shared-data mountPath: /pod-data mountPropagation: None - name: map mountPath: /sys/fs/bpf terminationMessagePolicy: File image: \u0026#39;gigamon/uct-tap:6.2.00-374893\u0026#39; volumes: - name: socket hostPath: path: /var/run/containerd/containerd.sock type: \u0026#39;\u0026#39; - name: map hostPath: path: /sys/fs/bpf type: \u0026#39;\u0026#39; - name: shared-data emptyDir: {} dnsPolicy: ClusterFirstWithHostNet UTC-Controller Routes , 此處因爲service endpoint走的是 8443, 所以加上edge的tls方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [ocpadmin@Bastion gigamon]$ cat cntrl-route.yml apiVersion: route.openshift.io/v1 kind: Route metadata: name: gigamon-uct-cntlr-service namespace: gigamon spec: host: gigamon-uct.apps.ocp.dynasafelab.local port: targetPort: 8443 to: kind: Service name: gigamon-uct-cntlr-service tls: termination: edge 最後oc apply -f 上述的yaml檔案\n1 2 3 [ocpadmin@Bastion gigamon]$ oc apply -f uct-controller.yaml [ocpadmin@Bastion gigamon]$ oc apply -f cntrl-route.yml [ocpadmin@Bastion gigamon]$ oc apply -f uct-tap.yaml ","date":"2023-09-23T03:50:05+08:00","permalink":"https://wangken0129.github.io/p/gigamon_on_ocp/","title":"Gigamon_on_OCP"},{"content":"Gigamon On Nutanix 參考連結 https://docs.gigamon.com/pdfs/Content/Resources/PDF%20Library/GV-6100-Doc/GigaVUE-Cloud-Suite-Nutanix-GigaVUE-V-Series-2-Guide-v61.pdf https://community.gigamon.com/gigamoncp/s/article/Deploying-Gigamon-Cloud-Suite-for-pervasive-visibility-on-Nutanix-underlay-using-native-service-chaining-6-1\nAHV Networking\nhttps://www.nutanixbible.com/5a-book-of-ahv-architecture.html https://hyperhci.com/2019/11/20/nutanix-ahv-networking-advanced-features/\nFortinet https://docs.fortinet.com/document/fortigate/6.4.0/new-features/932284/nutanix-service-chaining-6-4-5\n架構圖 利用Nutanix的Flow Service Chaining 將流量導入到Gigamon vSeries 由GigaVUE-FM來控管要出去的流量,再從vSeries把流量導到資安設備 or Wireshark\nPort Requrements 資訊 Gigamon FM https://172.16.101.160 admin / netfos123A!! Monitoring Mode 依照手冊建立FM 建立vSeries 1 2 需注意Management跟Data Lan都要使用DHCP 也就是IPAM的子網 tcpdump 建立ubuntu VM\n安裝\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 https://community.gigamon.com/gigamoncp/s/article/Deploying-Gigamon-Cloud-Suite-for-AWS-Behind-GWLB-to-Gain-Cross-Account-Visibility-6-1 Once this session is successful, login to an Ubuntu instance being used as tools VM. User can input the following commands: Install TCPDump: sudo snap install tcpdump Retrieve traffic being sent by the VSeries node: tcpdump -i eth0 port 4789 To capture traffic in a PCAP file instead of being displayed on terminal: sudo tcpdump -i eth0 port 4789 -w test.pcap (Optional) Download captured pcap to a local Linux workstation: scp test.pcap user-name@11.22.33.44:/Downloads The traffic being retrieve is VXLAN encapsulated. To de-capsulate the traffic: Add a VXLAN interface: sudo ip link add vxlan0 type vxlan id 112 group 239.1.1.1 dev eth0 dstport 4789 The ID needs to match the VXLAN ID provided in the tunnel Bring up the VXLAN port: sudo ip link set vxlan0 up Capture the decapsulated packets: sudo tcpdump -ni vxlan0 By following above steps 9 and 10, user can view the intended traffic from the source VMs as defined by the policies under the map. Wireshark 設定要監控的流量 在FM調整tunnel 的 VxLan Network Identifier及Destination L4 Port 為 4789 ","date":"2023-09-23T03:47:05+08:00","permalink":"https://wangken0129.github.io/p/gigamon_on_nutanix/","title":"Gigamon_on_Nutanix"},{"content":"Nivida On Nutanix AHV 主要測試NVIDIA vGPU是否可在Nutanix AHV上安裝及運作，\n要使用 vGPU 則需要先安裝License Server，VM上的vGPU才可正常使用，\nvGPU相關的License Server、Host Driver、GuestOS Driver都有相依性，安裝前要先去對應，\n以下方法已在客戶端驗證可安裝使用。\n參考資料 Nutanix vGPU 說明\u0026amp;troubleshooting https://portal.nutanix.com/page/documents/solutions/details?targetId=TN-2046-vGPU-on-Nutanix:TN-2046-vGPU-on-Nutanix Nvidia License Server Requirement https://docs.nvidia.com/license-system/latest/nvidia-license-system-faq/index.html Nvidia License Server User Guide https://docs.nvidia.com/license-system/latest/nvidia-license-system-user-guide/index.html#registering-dls-administrator-user AHV Install Nvidia Driver https://portal.nutanix.com/page/documents/details?targetId=NVIDIA-Grid-Host-Driver-For-AHV-Install-Guide:NVIDIA-Grid-Host-Driver-For-AHV-Install-Guide AHV Create VM with vGPU https://portal.nutanix.com/page/documents/details?targetId=Web-Console-Guide-Prism:wc-vm-create-acropolis-wc-t.html 預先準備 下載AHV Driver 要對應Guest OS Driver https://portal.nutanix.com/page/downloads?product=ahv\u0026bit=NVIDIA https://ui.licensing.nvidia.com/software 下載License Server，NLS License Server (DLS 新版就用qcow2或是ovf) https://ui.licensing.nvidia.com/software License Key、Nvida Account Compatibility Matrix:\nAOS Version AHV Versiob Nvidia Version AOS 6.5.3 LTS 20220304.420 15.2 Download driver below AOS 6.6.2.5 20220304.10057 15.0 Download driver below AOS 6.5.2.7 LTS 20220304.392 15.0 Download driver below AOS 6.6.2 20220304.10055 15.0 Download driver below AOS 6.5.2.6 LTS 20220304.385 15.0 Download driver below AOS 6.6.0.5 20220304.10019 13.3 Download driver below AOS 6.5.2 LTS 20220304.342 13.3 Download driver below AOS 6.6 20220304.10013 13.3 Download driver below AOS 6.5.1.8 LTS 20220304.336 13.3 Download driver below AOS 5.20.5 LTS 20201105.2312 13.3 Download driver below License Server 說明 A VM obtains a license over the network from an NVIDIA vGPU software license server.\nThe license is “checked out” or “borrowed” when the VM is booted, and returned when the\nVM is shut down. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Before proceeding, ensure that you have a platform suitable for hosting a DLS virtual appliance or containerized DLS software image. - The hosting platform must be a physical host running a supported hypervisor or container orchestration platform. - The minimum resource requirements for the VM in which the DLS virtual appliance or DLS container will run is as follows: - Number of vCPUs: 4 - RAM: 8 Gbytes - Disk Size: 10 Gbytes For additional guidelines, refer to [Sizing Guidelines for a DLS Appliance](https://docs.nvidia.com/license-system/latest/nvidia-license-system-user-guide/index.html#dls-virtual-appliance-sizing-guidelines). - The platform must have a fixed (unchanging) IP address. The IP address may be assigned dynamically by DHCP or statically configured, but must be constant. - The platform’s date and time must be set accurately. NTP is recommended. 執行步驟 建立Nvidia 帳號 並匯入Key 在License文件下方點選註冊或是登入 登入後即可看到License\n安裝License Server 上傳License Server的 qcow2 image\n開啟vm 設定 4 vpu 8gb ram\n預設使用者 dls_system 登入 ( nls 3.1.0 預設帳號改為dls_admin / welcome )\n設定固定IP\n1 $ /etc/adminscripts/set-static-ip-cli.sh 註冊DLS Admin User (Cluster架構只需建立一次) 連線https://dls-vm-ip 選擇First time setup，設定dls_admin密碼 複製secret，並用dls_admin登入\n1 2 79256b19-ffc6-4617-bec3-b62b0696064d 密碼: Nutanix/4u 登入後即可註冊在Nvidia 建立的Service Instance，以及上傳License File 建立sudo user: rsu_admin 用dls_admin / welcome or dls_system 登入License Server\n1 2 $ /etc/adminscripts/enable_sudo.sh 輸入密碼: Nutanix/4u 製作HA Cluster ( optional )，依照上述步驟到步驟4再建立一台License Server 並在Web 上點選Configure high avaliability並輸入另一台的ip 要先確認防火牆port有通 ，建議同網段\nCreate Cluster 登入另一台License Server確認 dls_admin 可以登入，資訊同步 註冊License Server 下載DLS Instance Token 上傳到Nvidia License Portal 點選SERVICE INSTANCES \u0026gt; Register DLS Instance\n上傳Token 在Nvidia Portal 建立License Server \u0026gt; CREATE SERVER 設定名稱、描述 選擇License 建立License Server 建立好後把SERVER INSTANCES 連結到License Server 連結好後即可下載License File的bin檔 再將bin檔上傳到地端的License Server 即可看到License 已可在地端使用 下載Client Config Token 以利日後GuestOS 使用\nAHV 安裝Nvidia Driver 確認AHV有抓到顯卡\n1 root@ahv# lspci | grep -i nvidia PE 上看到Hardware有無顯卡\n上傳lcm_nvidia_5.10.170-2.el7.nutanix.20220304.420-525.105.14.tar.gz 到CVM\nCVM 執行安裝Driver\n1 2 3 4 5 6 7 8 9 解壓縮 nutanix@cvm$ tar -xvzf lcm_nvidia_version.tar.gz builds/nvidia-builds/version/ builds/nvidia-builds/version/metadata.json builds/nvidia-builds/version/metadata.sign builds/nvidia-builds/version/nvidia-vgpu-version.rpm 執行安裝 nutanix@cvm$ install_host_package -r driver_package_location 確認VM可能會關機或是被移轉\n1 2 3 4 5 6 7 8 9 10 11 12 13 VMs using a GPU must be powered off if their parent host is affected by this install. If left running, these VMs will be automatically powered off when installation begins on their parent host, and powered back on after the driver install is completed. VMs using vGPU will be migrated if their parent host is affected by this install. However, some vGPU VMs might be automatically powered off due to lack of resource when installation begins on their parent host, and powered back on after the driver install is completed. Install now (yes/no) yes Note: If a host already has the same version vGPU driver installed, the install_host_package script skips putting that host in maintenance mode, and also skips the restart of that host. 這邊選擇no，在沒有GPU的node上面不用安裝\n1 2 3 4 2021-05-27 06:51:52,082Z INFO install_host_package:70 Waiting for download... 2021-05-27 06:51:55,269Z INFO install_host_package:192 No supported GPU hardware found for host 1.1.1.1 Continue installing on this node anyway? Install now (yes/no) no 確認安裝完畢\n1 2 3 4 5 nutanix@cvm$ hostssh \u0026#34;rpm -qa | grep -i nvidia\u0026#34; nutanix@cvm$ ncc health_checks hypervisor_checks gpu_driver_installed_check nutanix@cvm$ hostssh nvidia-smi 指派vGPU Profile給Guest VM Guest VM 安裝Driver 參考 grid-vgpu-user-guide.pdf\n安裝檔案\nWindows 安裝Driver，裝完會重開機\n指定License Server路徑\nLinux A-Series的顯卡要裝15.1以上的版本\nubuntu 安裝方法,裝完重開機\n1 ＄apt-get install ./nvidia-linux-grid-525_525.105.17_amd64.deb 裝完後設定/etc/nvidia/gridd.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 $vi /etc/nvidia/gridd.conf # 主要有三項要修改 ServerAddress= ServerPort= #新版預設是443 FeatureType= #如下方描述 Description: Set Feature to be enabled # Data type: integer # Possible values: # 0 =\u0026gt; for unlicensed state # 1 =\u0026gt; for NVIDIA vGPU (Auto Select) # 2 =\u0026gt; for NVIDIA RTX Virtual Workstation # 4 =\u0026gt; for NVIDIA Virtual Compute Server 把Client_Configuration_Token放到 /etc/nvidia/ClientConfigToken/ 重啟nvidia-gridd 服務,確認有抓到License\n1 2 $ service nvidia-gridd restart $ service nvidia-gridd status Troubleshoot 產Log report給Nvidia Support (enterprisesupport@nvidia.com)\n1 2 3 4 5 6 SSH to your AHV Host Run: nvidia-bug-report.sh This will generate the file named: nvidia-bug-report.log.gz It will be placed in folder from which you ran the command Here is the example: https://docs.nvidia.com/grid/11.0/grid-vgpu-user-guide/index.html#capture-config-data-for-bug-report 安裝時有遇到無法取得License：\n建議先檢查Client跟License Server的時區、時間是否正確。\nLicense Server設定網卡IP時會出現錯誤訊息：\n建議可用IPAM先配發DHCP，等服務起來再進去Web更改。\n","date":"2023-09-23T03:16:38+08:00","permalink":"https://wangken0129.github.io/p/nvidia_on_nutanix/","title":"Nvdia_on_Nutanix"},{"content":"Mine™ with Veeam Guide 3.0 Nutanix Mine 是Nutanix整合備份軟體讓Nutanix Cluster成為備份專用的Cluster，\n以下會測試安裝Nutanix Mine with Veeam，測試備份、還原、升級等。\nRefrence Document https://portal.nutanix.com/page/documents/details?targetId=Mine_veeam:Mine_veeam Video (Mine 2.0) https://www.youtube.com/watch?v=e13I-VXVaoo Download https://portal.nutanix.com/page/downloads?product=mine Veeam https://helpcenter.veeam.com/docs/backup/mine/architecture_overview.html?ver=120 Notice 安裝Mine Foundation VM時,時區要先調成UTC,安裝完後再調回需要的時區。\nMine Foundation需要一直保持開機狀態,才可以與AHV Cluster溝通。\nMine Cluster只支援AOS LTS版本,STS不支援。\n請勿修改或刪除部署後的以下6個VM以及Foundation For Mine。 Veeam-Win-Node1,Veeam-Win-Node2, Veeam-Win-Node3,Veeam-Lin-Node1, Veeam-Lin-Node2,Veeam-Lin-Node3\nAHV Cluster建議使用另一個Cluster Admin帳戶,用預設的admin可能會有warning。\n部署Mine Cluster目前只支援Windows 2019 Std.\n確認DNS設定。\n請勿刪除default Storage Container,Mine Cluster會使用到。\n如刪除會出現以下錯誤 部署Mine Cluster至少要四個Node。\n架構 Backup Flow VM數量:\nFoundation For Mine *1 (Linux) ,4 vCPUs and 4 GB memory Backup Server *1 (Windows) , 8 vCPU,16GB memory Backup Proxy *2 or 5 (Node\u0026gt;8) , 8 vCPU,16GB memory Backup Ropositories (Linux) 3 or *6 (Node \u0026gt;8) , 8 vCPU,128GB memory\n網路 Requirement Installation 安裝步驟 上架並連上網路。\n利用Foundation建立AHV Cluster (CVM建議32GB RAM)。\n建立Veeam的Cluster admin account。\n下載 NutanixFoundationForMineWithVeeam-v3.0.vmdk。\n上傳 NutanixFoundationForMineWithVeeam-v3.0.vmdk 至AHV Cluster。\n利用上傳的vmdk建立Mine Foundation VM並開機 (4 vCPU, 4GB RAM)\n如有DHCP 直接連線Web https://ip:8743 如沒有DHCP則開啟VM Console並調整IP ( account/password: veeam/veeam )\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $sudo nano /etc/network/interfaces ----------- auto eth0 iface eth0 inet dhcp ----------- ----------\u0026gt; ----------- auto eth0 iface eth0 inet static address yourIpAddress netmask yourSubnetMask gateway yourAddressAddress ----------- sudo service networking restart 登入web後點選setup Accept EULA Configure a virtual network for guest VM interfaces.\n輸入Prism Element Cluster的資訊\n上傳Windows 2019 Std的iso\n上傳Veeam的License\n輸入Veeam Server的資訊(用現存的或是部署新的, Windows: 8 vCPUs and 16 GB memory, Linux: 8 vCPUs and 128 GB memory)\n開始部署(1~2Hrs)\n輸入Windows License Key\n部署完成。\n安裝後的步驟 新增Volume Group,attach到backup server,並將VG,bakcup server加到protection domain 設定備份VM等Job Mine 安裝實作 Lab架構 資訊欄 AHV Cluster , Block SN: 13SM35330024 項目 SN IPMI MAC IPMI IP CVM IP AHV IP NodeA-AHV1 ZM137S025587 00:25:90:d3:c8:23 10.0.90.2 172.16.90.61 172.16.90.51 NodeB-AHV2 ZM139S125484 00:25:90:d8:74:17 10.0.90.3 172.16.90.62 172.16.90.52 NodeC-AHV3 ZM137S025585 00:25:90:d3:c8:20 10.0.90.4 172.16.90.63 172.16.90.53 NodeD-AHV4 ZM137S025604 00:25:90:d3:c7:f4 10.0.90.5 172.16.90.64 172.16.90.54 SSD、HDD per node Memory per node IPMI Account IPMI Passwd Account CVM,AHV Passwd 400GB SSD *2 1TB HDD *4 128GB(16GB *8) ADMIN nutanix Veeam Cluster\n項目 IP Account vCPU Memory Foundation-Mine 172.16.90.206:8743 veeam 4 4 Veeam-Win-Node1 172.16.90.207 administrator 8 16 Veeam-Win-Node2 172.16.90.208 administrator 8 16 Veeam-Win-Node3 172.16.90.209 administrator 8 16 Veeam-Lin-Node1 172.16.90.210 8 128 Veeam-Lin-Node2 172.16.90.211 8 128 Veeam-Lin-Node3 172.16.90.212 8 128 Helper Appliance 172.16.90.215 Backup Proxy 172.16.90.216 veeam 4 4 資源不足無法安裝\n加裝記憶體到196GB Per Node\nInstallation AHV Cluster Foundation四個Node,安裝成一個AHV Cluster,AOS版本建議是LTS的版本。 設定DNS、NTP等基本設定,以下不贅述。 Mine Foundation Install 下載Mine Foundation VM的vmdk (NutanixFoundationForMineWithVeeam-v3.0.vmdk) https://portal.nutanix.com/page/downloads?product=mine\n將下載後的vmdk上傳到Image Service 建立For Veeam使用的Cluster Admin 利用上傳的vmdk建立Mine Foundation VM並開機 (4 vCPU, 4GB RAM) 新增上傳好的硬碟\n新增網卡並save 設定Mine Foundation IP (預設是DHCP) VM開機後進入Console介面,登入帳密 veeam/veeam 執行以下指令修改ip\n1 2 3 4 5 6 7 8 9 10 11 12 $sudo vim /etc/network/interfaces ------原始----- auto eth0 iface eth0 inet dhcp ------更改後----- auto eth0 iface eth0 inet static address yourIpAddress netmask yourSubnetMask gateway yourAddressAddress ----------- $sudo service networking restart 瀏覽器連線至 Foundation_Mine的8743 port https://172.16.90.206:8743 預設帳密veeam / veeam ,登入後進行修改 登入測試 Mine Cluster Install 全新部署請選擇 Setup 點選接受EULA後下一步 輸入Prism Element的IP、帳密(前步驟建立的Veeam Account),點選下一步 登入後會進行檢查確認硬體是否合適\n上傳windows 2019的iso檔,可先上傳至PE 選擇新增DEPLOY NEW BACKUP\u0026amp;REPLICATION SERVER 上傳Veeam License File (可先申請試用版授權來使用) 網路設定,共會新增3台windows 、3台Linux VM 設定windows 認證,官方文件不建議使用windows AD驗證 review設定 開始進行安裝 (順序：搜集資訊\u0026gt;部署Linux\u0026gt;部署Windows\u0026gt;設定Scale Out Backup Repository) 登入PE可看到VM正在部署 透由PowerShell自動部署 自動建立Storage Container 自動建立Volume Group 部署完成,輸入windows授權啟用,可先略過 完成 點close後設定時區\u0026gt;settings 設定時區、NTP 重新登入PE Dashboard沒出現,可參考以下說明 https://www.veeam.com/kb4265\n1 2 3 4 5 6 7 1. 開啟Foundation_Mine的VM Console,並開啟ssh service 2. ssh 登入Foundation_mine VM 3. copy 指令並修改PE的IP、帳號密碼 4. 執行完成後重新登入PE會看到下方圖一 5. 關閉Foundation_mine VM的ssh service 6. ssh登入其中一台CVM並執行指令 allssh sudo cp -rp /home/nutanix/prism/webapps/console/nirvana/ /home/apache/www/console/nirvana/ 7.即可看到圖二的Dashboard 圖一\n圖二 更新完後LCM的選項會消失,但還是可以在Setting \u0026gt; Upgrade Software上點選出來 點選Launch Console會連線到Bakcup Server的VM (帳號密碼為Foundation Mine時所建立的) 新增需備份的Hypervisor平台 使用此方式建立的Mine Cluster會新增Nutanix AHV的Plug-in 新增Nutanix AHV Cluster (輸入PE的VIP)\n詳細設定可參考 https://helpcenter.veeam.com/archive/van/21/userguide/overview.html\n新增連線帳號密碼 此處會建立一個Helper Appliance VM在目的地的AHV Cluster,需要與Backup Server同網段 Helper Appliance建立完成 新增完成 建立Proxy 在備份目標上 , 備份會透過此Proxy來連線AHV Cluster (https://helpcenter.veeam.com/archive/van/21/userguide/add_ahv_proxy.html) 建立新的Backup Proxy Linux VM 設定Backup Proxy名稱、vCPU、Memory,點選Advance進行設定 依照同時進行任務的數量來決定資源, EX: 同時進行4個任務就需要4個vCPU Core、4GB Memory 設定Backup Proxy的網路 (預設使用DHCP) 設定帳號權限 設定Backup Repository的權限 建立完成,跳出DNS Warning,需要在DNS Server上新增兩筆紀錄,veeam-win-node1、NX1365-VeeamProxy的紀錄 並重新apply backup proxy的設定才會重新連線\nBackup Server 也需設定DNS尾碼,或是加一筆記錄至hosts內 同理在Linux Proxy Server也需設定 (/etc/hosts) Finish ADD Backup Job 新增一個Backup Job測試,用ken-jump VM來測試 會連線到 https://172.16.90.216:8100 (Backup Proxy的IP) Dashboard畫面,NTP跟時區需要手動再去設定,點右上方齒輪可進行設定 Backup Server\n可備份的Cluster\n新增一個backup job 選擇備份的VM 設定備份目的地,及還原點數量\u0026hellip; 設定排程 完成設定 查看備份進度 備份完成 Protech VMs可以看到從Veeam執行備份的VM以及NX1365 Cluster上原有的Snapshot Events查看執行紀錄 Proxy Server Dashboard畫面 PE上的Mine\u0026amp;Veeam Dashboard Restore Backup VM 目標：將備份的VM還原至AHV Cluster\n在Backup Proxy上的Protected VMs點選Restore 選擇需要的VM及還原點 還原模式選擇： 還原至原來的位置(原機還原) or 還原至新的位置(客製化設定) 因為是測試還原故選擇Restore to a new location,設定VM Name 選擇還原至哪個Storage Container 選擇網路,測試還原故將網路先斷線 還原的原因,可略過 還原Job總覽 點選Finish即會開始還原 目的地的AHV Cluster上會先建立Volume Group,還原完後自動刪除 備份完成 NX1365 AHV Cluster即可看到該VM,登入確認OK 在Backup Proxy上的還原只針對整台VM或是Disk的還原 如要針對檔案或是Application (AD,Exchange,SQL等),則要回到Backup Server上進行還原 實測Disk or Snapshots皆可進行檔案、Application還原,但Snapshots還原會在PE上建立VM及Volume Group故時間上會較慢 如下圖為針對檔案的還原 可選擇還原至原來位置or複製一份到別地方 以上為Nutanix Mine with Veeam的安裝及備份還原實作測試。\nUpgrading Mine 開啟Mine Foundation的UI介面 \u0026gt; Mine Platform 點選Settings\n點選check updates 來更新,目前為最新版本故無需更新 Backup Proxy Version: 2.1.396\n連線至Backup Proxy UI https://172.16.90.216:8100 點選右上角齒輪 \u0026gt; Appliance Settings \u0026gt; Updates \u0026gt; Check and view updates\n會出現可更新的項目 Security updates、DoNetCore updates 勾選全部並執行更新,勾選自動重新啟動 需先確認沒有backup jobs or restore jobs正在執行 更新完成後可查看更新紀錄 再次執行備份確認沒問題 AOS AOS如有更新則需要在Mine Foundation的UI介面點選Maintenance \u0026gt; Redeploy Mine Dashboard 來重新更新UI介面\nVeeam Server 目前版本: 11.0.0.837 更新版本: 12.0.0.1420 Repository版本: 11.0.0.839\n下載最新的Veeam Backup\u0026amp;Replication iso檔 https://www.veeam.com/backup-replication-download.html?ad=downloads\n開啟iso檔,用系統管理員身份執行Setup.exe 執行Upgrade (需先關閉Veeam的視窗) Accept License 確認更新版本,及自動更新遠端的元件 更新License (11 to 12) 選擇帳號 SQL 提示升級可能也會更新Datebase 升級前的check,Next 開始升級 安裝完畢,需重新啟動 重新登入Veeam Console後會出現可更新的遠端元件 更新遠端的元件,Linux Node Failed: Permission denied\nNX1365-VeeamProxy 更新中 Backup Server會直接呼叫Proxy Server的AHV Cluster進行快照、更新\n連線到Foundation Mine來進行更新,點選Update Linux Repository更新完畢 版本11.0.0.839 \u0026gt; 12.0.0.1420\nProxy Server 更新完成 登入Proxy Server https://172.16.90.216\n介面已更新 版本 v2.1.396 \u0026gt; v4.0.0.1899 回到PE上的Mine with Veeam Dashboard確認狀態\nProtected Instances 顯示N/A\n回到Foundation Mine上面下載Support Bundle\n下載後會是一個zip檔,解壓縮看到以下目錄\n點開NutanixMineDashboard\nError \u0026ldquo;Cannot get restore points from backup ken-jump, because it is encrypted or create by an enterprise plug-in\u0026rdquo;\n有可能是因為目前12版本太新,有相容性問題 除了Dashboard之外,其他功能顯示皆正常,也可正常備份還原。\n","date":"2023-09-23T03:11:15+08:00","permalink":"https://wangken0129.github.io/p/nutanix_mine/","title":"Nutanix_Mine"},{"content":"Karbon、Files 測試安裝Nutanix 內建的Kubernetes，並使用內建的Storage儲存空間(NFS、Volume)\nLinux主機安裝設定(測試用) (測試功能又不佔用ip才需要此Linux主機)\n安裝一台CentOS-Stream9 IP: 172.16.90.205 , 192.168.1.1 Host: bastion.netfos.ken.lab\n此VM作用為 Router, NAT, DHCP, DNS,Web等等\nPE Network設定,新增192.168.1.1的subnet並綁到vSwitch上 (啟動IPAM)\nupload iso to Prism Element\non Prism Central import iso\ncreate VM on Prism Central 安裝OS\n設定Router \u0026amp; NAT https://linux.vbird.org/linux_server/centos6/0230router.php\n啟動Linux Kernal 的 ip_forwarder功能\n1 2 3 4 5 6 7 8 [ken@bastion ~]$ sudo vim /etc/sysctl.conf net.ipv4.ip_forward = 1 [ken@bastion ~]$ sudo sysctl -p net.ipv4.ip_forward = 1 [ken@bastion ~]$ sudo cat /proc/sys/net/ipv4/ip_forward 1 NAT設定\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 [ken@bastion ~]$ sudo iptables -A INPUT -i ens4 -j ACCEPT [ken@bastion ~]$ sudo iptables -A INPUT -i lo -j ACCEPT [ken@bastion ~]$ sudo iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -j MASQUERADE [ken@bastion ~]$ sudo iptables -L Chain INPUT (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere Chain FORWARD (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination [ken@bastion ~]$ sudo iptables-save # Generated by iptables-save v1.8.8 (nf_tables) on Wed Jan 11 12:06:10 2023 *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] -A INPUT -i ens4 -j ACCEPT COMMIT # Completed on Wed Jan 11 12:06:10 2023 # Generated by iptables-save v1.8.8 (nf_tables) on Wed Jan 11 12:06:10 2023 *nat :PREROUTING ACCEPT [0:0] :INPUT ACCEPT [0:0] :OUTPUT ACCEPT [0:0] :POSTROUTING ACCEPT [0:0] -A POSTROUTING -s 192.168.1.0/24 -j MASQUERADE COMMIT # Completed on Wed Jan 11 12:06:10 2023 關閉防火牆 sudo systemctl disable firewalld --now DNAT設定 https://portal.nutanix.com/page/documents/details?targetId=Port-Reference:NKE_Airgap_port_auto_r.html https://portal.nutanix.com/page/documents/details?targetId=Port-Reference:Nutanix_Kubernetes_Engine_port_auto_r.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## For NKE Registry sudo iptables -t nat -A PREROUTING -i ens3 -p tcp --dport 22 \\ -j DNAT --to-destination 192.168.1.5:22 sudo iptables -t nat -A PREROUTING -i ens3 -p tcp --dport 5000 \\ -j DNAT --to-destination 192.168.1.5:5000 sudo iptables-save ## For NKE Loadbalnacer sudo iptables -t nat -A PREROUTING -i ens3 -p tcp --dport 443 \\ -j DNAT --to-destination 192.168.1.10:22 sudo iptables -t nat -A PREROUTING -i ens3 -p tcp --dport 2379 \\ -j DNAT --to-destination 192.168.1.10:2379 sudo iptables -t nat -A PREROUTING -i ens3 -p tcp --dport 30001 \\ -j DNAT --to-destination 192.168.1.10:30001 sudo iptables -t nat -A PREROUTING -i ens3 -p tcp --dport 30000:32767 \\ -j DNAT --to-destination 192.168.1.10:30000-32767 sudo iptables-save 拿一台測試VM確認可ping 通 安裝DNS Server (named) 安裝named\n1 2 [ken@bastion ~]$ sudo yum install bind bind-libs bind-chroot bind-utils [sudo] password for ken: 修改named 設定檔 /etc/named.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 [root@bastion data]# cat /etc/named.conf // // named.conf // // Provided by Red Hat bind package to configure the ISC BIND named(8) DNS // server as a caching only nameserver (as a localhost DNS resolver only). // // See /usr/share/doc/bind*/sample/ for example named configuration files. // options { listen-on port 53 { 192.168.1.1; }; //listen-on-v6 port 53 { ::1; }; directory \u0026#34;/var/named\u0026#34;; dump-file \u0026#34;/var/named/data/cache_dump.db\u0026#34;; statistics-file \u0026#34;/var/named/data/named_stats.txt\u0026#34;; memstatistics-file \u0026#34;/var/named/data/named_mem_stats.txt\u0026#34;; secroots-file\t\u0026#34;/var/named/data/named.secroots\u0026#34;; recursing-file\t\u0026#34;/var/named/data/named.recursing\u0026#34;; allow-query { any; }; ## 設定轉寄站 forwarders { 192.168.102.1; 192.168.102.2; }; /* - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion. - If you are building a RECURSIVE (caching) DNS server, you need to enable recursion. - If your recursive DNS server has a public IP address, you MUST enable access control to limit queries to your legitimate users. Failing to do so will cause your server to become part of large scale DNS amplification attacks. Implementing BCP38 within your network would greatly reduce such attack surface */ recursion yes; dnssec-validation no; dnssec-enable yes; managed-keys-directory \u0026#34;/var/named/dynamic\u0026#34;; geoip-directory \u0026#34;/usr/share/GeoIP\u0026#34;; pid-file \u0026#34;/run/named/named.pid\u0026#34;; session-keyfile \u0026#34;/run/named/session.key\u0026#34;; /* https://fedoraproject.org/wiki/Changes/CryptoPolicy */ include \u0026#34;/etc/crypto-policies/back-ends/bind.config\u0026#34;; }; logging { channel default_debug { file \u0026#34;data/named.run\u0026#34;; severity dynamic; }; }; zone \u0026#34;.\u0026#34; IN { type hint; file \u0026#34;named.ca\u0026#34;; }; include \u0026#34;/etc/named.rfc1912.zones\u0026#34;; include \u0026#34;/etc/named.root.key\u0026#34;; zone \u0026#34;netfos.ken.lab\u0026#34; IN { type master; file \u0026#34;ken.zone\u0026#34;; allow-update { none; }; allow-query { any; }; allow-transfer { none; }; }; zone \u0026#34;1.168.192.in-addr.arpa\u0026#34; IN { type master; file \u0026#34;ken.reverse\u0026#34;; allow-update { none; }; allow-query { any; }; allow-transfer { none; }; }; 修改正向解析 /var/named/ken.zone\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@bastion named]# cat ken.zone $TTL 1W @\tIN\tSOA\tbastion.netfos.ken.lab.\troot ( 2019070700\t; serial 3H\t; refresh (3 hours) 30M\t; retry (30 minutes) 2W\t; expiry (2 weeks) 1W )\t; minimum (1 week) IN\tNS\tbastion.netfos.ken.lab. ; bastion.netfos.ken.lab.\tIN\tA\t192.168.1.1 ; ;EOF 修改反解析 /var/named/ken.reverse\n1 2 3 4 5 6 7 8 9 10 11 [root@bastion named]# cat ken.reverse $TTL 10 @\tIN\tSOA\tbastion.netfos.ken.lab.\troot ( 2019070700\t; serial 3H\t; refresh (3 hours) 30M\t; retry (30 minutes) 2W\t; expiry (2 weeks) 1W )\t; minimum (1 week) IN\tNS\tbastion.netfos.ken.lab. ; ;EOF 調整防火牆\n1 2 [root@bastion ~]# firewall-cmd --add-service=dns --permanent [root@bastion ~]# firewall-cmd --reload 啟動服務\n1 [root@bastion ~]# systemctl enable named --now 驗證DNS\n1 2 3 4 5 6 7 [ken@bastion ~]$ dig +noall +answer @192.168.1.1 bastion.netfos.ken.lab bastion.netfos.ken.lab.\t604800\tIN\tA\t192.168.1.1 [ken@bastion ~]$ nslookup 168.95.1.1 1.1.95.168.in-addr.arpa\tname = dns.hinet.net. Authoritative answers can be found from: 安裝NFS Server install\n1 [ken@bastion ~]$ sudo yum install nfs-utils setting\n1 2 3 4 5 6 7 8 [ken@bastion ~]$ sudo mkdir /nfs01 [ken@bastion ~]$ sudo chmod 777 /nfs01/ [ken@bastion ~]$ sudo vim /etc/exports /nfs01 192.168.1.0/24(rw,no_root_squash) [ken@bastion ~]$ sudo systemctl enable --now rpcbind nfs-server Created symlink /etc/systemd/system/multi-user.target.wants/nfs-server.service → /usr/lib/systemd/system/nfs-server.service. [ken@bastion ~]$ 防火牆關閉或是開nfs service Karbon (Airgap) https://portal.nutanix.com/page/documents/details?targetId=Nutanix-Kubernetes-Engine-v2_6:top-airgap-deploy-t.html https://next.nutanix.com/nutanix-kubernetes-engine-30/karbon-install-40404 https://portal.nutanix.com/page/documents/details?targetId=Nutanix-Kubernetes-Engine-v2_6:Nutanix-Kubernetes-Engine-v2_6\n安裝httpd 安裝httpd\n1 [ken@bastion ~]$ sudo yum install httpd 新增資料夾並複製Karbon檔案\u0026amp;解壓縮\n1 2 3 4 5 6 7 8 [ken@bastion ~]$ sudo mkdir /var/www/html/ntnx-2.6.0 [sudo] password for ken: scp airgap-* root@172.16.90.205:/var/www/html/ntnx-2.6.0 airgap-ntnx-2.6.0.tar.gz airgap-manifest.json [ken@bastion ~]$ sudo tar xvzf /var/www/html/ntnx-2.6.0/airgap-ntnx-2.6.0.tar.gz 啟動httpd,確認可以瀏覽至該目錄\n1 2 [ken@bastion ~]$ sudo systemctl enable httpd --now Created symlink /etc/systemd/system/multi-user.target.wants/httpd.service → /usr/lib/systemd/system/httpd.service. 安裝Karbon https://portal.nutanix.com/page/documents/details?targetId=Nutanix-Kubernetes-Engine-v2_6:Nutanix-Kubernetes-Engine-v2_6 https://portal.nutanix.com/page/documents/details?targetId=Nutanix-Kubernetes-Engine-v2_6:top-airgap-c.html\nPC Enable Karbon\nLCM確認Karbon版本為最新 (2.6.0)\nPC 加一張網卡(連192.168.1.0的網段)\n1 2 3 4 5 6 7 8 9 10 nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ sudo ifconfig eth1 192.168.1.2 netmask 255.255.255.0 nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ sudo ip link set eth1 down nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ sudo ip link set eth1 up nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.16.90.254 0.0.0.0 UG 0 0 0 eth0 169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0 172.16.90.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 192.168.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1 用nutanix登入進prism central, cd 進/home/nutanix/karbon\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 wangken@wangken-MAC % ssh -m hmac-sha2-256 nutanix@172.16.90.219 Prism Central VM nutanix@172.16.90.219\u0026#39;s password: nutanix/4u nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ pwd /home/nutanix/karbon nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ ls karbonctl 登入prism central nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ ./karbonctl login --pc-ip 172.16.90.219 \\ --pc-username admin \\ --pc-password Nutanix@20230109 Login successful 填入資訊並執行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 nutanix@pcvm$ /home/nutanix/karbon/karbonctl airgap enable \\ --webserver-url http://webserver_url/ntnx-version-number/ \\ --vlan-name network_name --static-ip static ip-address \\ --storage-container storage_container_name \\ --pe-cluster-name PE_cluster_name \\ –-pe-username user ---- 執行安裝指令 (若出現pe-username的error可以把它調到最前面) nutanix@pcvm$ ./karbonctl airgap enable \\ –-pe-username admin \\ --webserver-url http://172.16.90.205/ntnx-2.6.0 \\ --vlan-name 192.168.1.0 --static-ip 192.168.1.5 \\ --storage-container VM \\ --pe-cluster-name NX3360NG8-Cluster nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ /home/nutanix/karbon/karbonctl airgap enable --pe-username admin --webserver-url http://172.16.90.205/ntnx-2.6.0/ --vlan-name 192.168.1.0 --static-ip 192.168.1.5 --storage-container VM --pe-cluster-name NX3360NG8-Cluster Please enter the password for the PE user: admin Successfully submitted request to enable airgap: [POST /airgap][202] postAirgapAccepted \u0026amp;{TaskUUID:0xc000370140} ---- 確認執行進度 nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ ./karbonctl airgap list Airgap UUID VM IP URL Status Package Upload Status Registry Status Status Message 33844fb8-a677-4412-5440-b099f4a5289d :0 Deploying host OS images Not running check ok\n1 2 3 nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ ./karbonctl airgap list Airgap UUID VM IP URL Status Package Upload Status Registry Status Status Message 00e01072-b5a6-4055-4cfd-f55d78a8d57b 192.168.1.5 airgap-0:5000 Airgap registry is running Healthy 在Web UI新增Production Cluster 連線方式用External Loadbalancer: 192.168.1.10 \u0026ndash;\u0026gt; 要額外設定LoadBalancer control plane: 192.168.1.6-8\n改用Active-Passive Mode \u0026ndash;\u0026gt; VIP 192.168.1.9\n抽換為 Active-Passive Mode\n安裝完畢 連線測試NKE 下載kubeconfig linux 安裝kubectl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [ken@bastion ~]$ curl -LO https://dl.k8s.io/release/v1.23.0/bin/linux/amd64/kubectl % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 138 100 138 0 0 380 0 --:--:-- --:--:-- --:--:-- 379 100 44.4M 100 44.4M 0 0 8858k 0 0:00:05 0:00:05 --:--:-- 10.6M [ken@bastion ~]$ sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl [ken@bastion ~]$ kubectl version --client Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;23\u0026#34;, GitVersion:\u0026#34;v1.23.0\u0026#34;, GitCommit:\u0026#34;ab69524f795c42094a6630298ff53f3c3ebab7f4\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-12-07T18:16:20Z\u0026#34;, GoVersion:\u0026#34;go1.17.3\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} ###將kubeconfig放到.kube/config [ken@bastion ~]$ cp nke-kubectl.cfg .kube/config [ken@bastion ~]$ rm .kube/kubeconfig.cfg [ken@bastion ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION nke-34f192-master-0 Ready master 25m v1.23.11 nke-34f192-master-1 Ready master 25m v1.23.11 nke-34f192-worker-0 Ready node 23m v1.23.11 nke-34f192-worker-1 Ready node 23m v1.23.11 nke-34f192-worker-2 Ready node 23m v1.23.11 [ken@bastion redmine]$ kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME nke-34f192-master-0 Ready master 17h v1.23.11 192.168.1.27 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-1160.71.1.el7.x86_64 containerd://1.6.6 nke-34f192-master-1 Ready master 17h v1.23.11 192.168.1.13 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-1160.71.1.el7.x86_64 containerd://1.6.6 nke-34f192-worker-0 Ready node 17h v1.23.11 192.168.1.84 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-1160.71.1.el7.x86_64 containerd://1.6.6 nke-34f192-worker-1 Ready node 17h v1.23.11 192.168.1.15 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-1160.71.1.el7.x86_64 containerd://1.6.6 nke-34f192-worker-2 Ready node 17h v1.23.11 192.168.1.82 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-1160.71.1.el7.x86_64 containerd://1.6.6 ### 新增測試pod [ken@bastion ~]$ kubectl run -i -t busybox --image=radial/busyboxplus:curl --restart=Never 新增測試服務-redmine postgresql的yaml檔-pvc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kind: PersistentVolumeClaim apiVersion: v1 metadata: namespace: redmine name: redmine-postgres-pv-claim labels: app: postgres spec: storageClassName: default-storageclass accessModes: - ReadWriteOnce resources: requests: storage: 20Gi postgresql的yaml檔-configmap\n1 2 3 4 5 6 7 8 9 10 11 12 13 [ken@bastion redmine]$ cat pgsql-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: redmine-postgres-config labels: app: postgres namespace: redmine data: POSTGRES_USER: postgres POSTGRES_PASSWORD: P@55w.rd PGDATA: /var/lib/postgresql/pgdata POSTGRES_DB: redmine postgresql的yaml檔-deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 [ken@bastion redmine]$ cat pgsql-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: redmine-postgres-deployment namespace: redmine spec: strategy: type: Recreate selector: matchLabels: app: postgres replicas: 1 template: metadata: labels: app: postgres spec: containers: - name: postgres image: postgres:12 imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; ports: - containerPort: 5432 envFrom: - configMapRef: name: redmine-postgres-config volumeMounts: - mountPath: /var/lib/postgresql/data name: redmine-postgredb subPath: redmine-pgsql/data - mountPath: /etc/postgresql/pg_hba.conf name: redmine-postgredb subPath: redmine-pgsql/pghba.conf - mountPath: /etc/postgresql/postgresql.conf name: redmine-postgredb subPath: redmine-pgsql/postgresql.conf volumes: - name: redmine-postgredb persistentVolumeClaim: claimName: redmine-postgres-pv-claim postgresql的yaml檔-svc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [ken@bastion redmine]$ cat pgsql-svc.yaml apiVersion: v1 kind: Service metadata: name: redmine-postgres-service namespace: redmine labels: app: postgres spec: type: NodePort ports: - port: 5432 nodePort: 31432 selector: app: postgres deploy pgsql\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 [ken@bastion redmine]$ kubectl apply -f pgsql-pvc.yaml -n redmine persistentvolumeclaim/redmine-postgres-pv-claim created [ken@bastion redmine]$ kubectl apply -f pgsql-config.yaml -f pgsql-deploy.yaml -f pgsql-svc.yaml -n redmine configmap/redmine-postgres-config created deployment.apps/redmine-postgres-deployment created service/redmine-postgres-service created ### 確認PV,PVC [ken@bastion redmine]$ kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-6e4760a1-6884-492f-8ecb-b2dc89ddfdde 30Gi RWO Delete Bound ntnx-system/prometheus-k8s-db-prometheus-k8s-0 default-storageclass 17h persistentvolume/pvc-7875f29e-0b6e-409f-bcc2-d03b315ff2ac 20Gi RWO Delete Bound redmine/redmine-postgres-pv-claim default-storageclass 4s persistentvolume/pvc-d5e769a0-3c74-4e99-b401-2deb620f90c0 30Gi RWO Delete Bound ntnx-system/prometheus-k8s-db-prometheus-k8s-1 default-storageclass 17h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/redmine-postgres-pv-claim Bound pvc-7875f29e-0b6e-409f-bcc2-d03b315ff2ac 20Gi RWO default-storageclass 7s ### 確認pgsql status [ken@bastion redmine]$ kubectl get all -n redmine NAME READY STATUS RESTARTS AGE pod/redmine-postgres-deployment-6f6f66f9cd-4ph9q 1/1 Running 0 6m4s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/redmine-postgres-service NodePort 172.19.202.4 \u0026lt;none\u0026gt; 5432:31432/TCP 6m4s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/redmine-postgres-deployment 1/1 1 1 6m4s NAME DESIRED CURRENT READY AGE replicaset.apps/redmine-postgres-deployment-6f6f66f9cd 1 1 1 6m4s ### Test node port 31432 [ken@bastion redmine]$ telnet 192.168.1.84 31432 Trying 192.168.1.84... Connected to 192.168.1.84. Escape character is \u0026#39;^]\u0026#39;. ^CConnection closed by foreign host. redmine的yaml檔-config\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 apiVersion: v1 data: configuration.yml: | # = Redmine configuration file # # Each environment has its own configuration options. If you are only # running in production, only the production block needs to be configured. # Environment specific configuration options override the default ones. # # Note that this file needs to be a valid YAML file. # DO NOT USE TABS! Use 2 spaces instead of tabs for indentation. # default configuration options for all environments default: email_delivery: autologin_cookie_name: autologin_cookie_path: scm_mercurial_command: scm_git_command: scm_cvs_command: scm_bazaar_command: scm_subversion_path_regexp: scm_mercurial_path_regexp: scm_git_path_regexp: scm_cvs_path_regexp: scm_bazaar_path_regexp: scm_filesystem_path_regexp: scm_stderr_log_file: database_cipher_key: minimagick_font_path: production: development: kind: ConfigMap metadata: name: redmine-config namespace: redmine redmine的yaml檔-deploy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 [ken@bastion ~]$ cat redmine-deploy.yaml apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redmine namespace: redmine spec: replicas: 1 selector: matchLabels: app: redmine template: metadata: labels: app: redmine spec: initContainers: # 修正configmap所需的讀寫權限 - name: fix-permission-for-configfile image: busybox:1.33.1 imagePullPolicy: IfNotPresent command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;cat /tmp/configuration.yml \u0026amp;\u0026amp; cp /tmp/configuration.yml /config-tmp-dir/configuration.yml \u0026amp;\u0026amp; chmod -R 777 /config-tmp-dir\u0026#39;] # 同時掛載兩個空間(一個有redmine要得檔案, 另外一個有可自由讀寫的空間) volumeMounts: - name: redmine-config-vol mountPath: /tmp/configuration.yml subPath: configuration.yml - name: redmine-config-tmp-dir mountPath: /config-tmp-dir containers: - name: redmine image: redmine:4.1.5 env: - name: REDMINE_DB_POSTGRES value: redmine-postgres-service - name: REDMINE_DB_USER value: postgres - name: REDMINE_DB_PASSWORD value: P@55w.rd - name: REDMINE_DB_DATABASE value: redmine # default password admin/admin volumeMounts: - name: redmine-config-tmp-dir mountPath: /usr/src/redmine/config/configuration.yml subPath: configuration.yml - name: redmine-files-dir mountPath: /usr/src/redmine/files - name: redmine-plugins mountPath: /usr/src/redmine/plugins - name: redmine-themes mountPath: /usr/src/redmine/public/themes ports: - containerPort: 3000 name: ui # 向k8s取得redmine的yaml檔案 volumes: - name: redmine-config-vol configMap: name: redmine-config items: - key: configuration.yml path: configuration.yml # 暫存給redmine與初始化需要的儲存空間(會隨刪除消失) - name: redmine-config-tmp-dir emptyDir: {} - name: redmine-files-dir nfs: path: /nfs01/redmine/ server: 192.168.1.1 - name: redmine-themes nfs: path: /nfs01/redmine/ server: 192.168.1.1 - name: redmine-plugins nfs: path: /nfs01/redmine/ server: 192.168.1.1 redmine的yaml檔-service (NodePort)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 [ken@bastion ~]$ cat redmine-svc.yaml apiVersion: v1 kind: Service metadata: name: redmine-service labels: app: redmine spec: type: NodePort selector: app: redmine ports: - name: ui protocol: TCP port: 3000 nodePort: 32748 ----------------- ClusterIP apiVersion: v1 kind: Service metadata: name: redmine-service labels: app: redmine spec: type: ClusterIP selector: app: redmine externalIPs: - 172.16.90.213 ports: - name: ui protocol: TCP port: 80 targetPort: 3000 部署redmine\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [ken@bastion redmine]$ kubectl apply -f redmine-config.yaml -f redmine-deploy.yaml -f redmine-svc.yaml configmap/redmine-config created deployment.apps/redmine created service/redmine-service created [ken@bastion redmine]$ kubectl get all -n redmine NAME READY STATUS RESTARTS AGE pod/redmine-54b45b447c-89rm4 1/1 Running 0 51s pod/redmine-postgres-deployment-6f6f66f9cd-6ngtz 1/1 Running 0 86s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/redmine-postgres-service NodePort 172.19.202.4 \u0026lt;none\u0026gt; 5432:31432/TCP 172m service/redmine-service NodePort 172.19.220.153 \u0026lt;none\u0026gt; 3000:32748/TCP 51s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/redmine 1/1 1 1 52s deployment.apps/redmine-postgres-deployment 1/1 1 1 172m NAME DESIRED CURRENT READY AGE replicaset.apps/redmine-54b45b447c 1 1 1 52s replicaset.apps/redmine-postgres-deployment-6f6f66f9cd 1 1 1 172m 預設帳密 admin/admin , 服務皆走node port , 確認連線OK Enable Advance k8s https://portal.nutanix.com/page/documents/details?targetId=Nutanix-Kubernetes-Engine-v2_5:top-install-advanced-t.html\n登入Prism Central的VM\n1 2 3 wangken@wangken-MAC ~ % ssh nutanix@172.16.90.219 Prism Central VM nutanix@172.16.90.219\u0026#39;s password: karbonctl 登入PC\n1 2 3 4 5 6 7 8 9 nutanix@NTNX-172-16-90-219-A-PCVM:~$ cd karbon/ nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ ll total 66020 -rwxr-x---. 1 nutanix nutanix 53464168 Oct 14 11:04 karbonctl -rw-------. 1 nutanix nutanix 14136153 Jan 11 00:56 kps-cloud-deployer-pc.tar.gz nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ ./karbonctl login --pc-username admin Please enter the password for the PC user: admin Login successful Enable Advance K8s Management \u0026ndash; airgap mode 不能啟用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 nutanix@NTNX-172-16-90-219-A-PCVM:~/karbon$ ./karbonctl karbon-management enable --cluster-name nke Advanced Kubernetes Managment can not be enabled in airgap mode. panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x68 pc=0x155813e] goroutine 1 [running]: github.com/nutanix-core/acs-karbonctl/cmd.executeK8sRESTAPI({0x1b93ae8, 0xc00013f1c0}, {0x0, 0x0}, {0x190b901, 0x5}, {0xc000148200, 0xc000523510, 0xc000356880}, {0x28bff08, ...}, ...) /home/circleci/go/src/github.com/nutanix-core/acs-karbonctl/cmd/karbonmgmt_enable.go:140 +0x5e github.com/nutanix-core/acs-karbonctl/cmd.actionKarbonMgmtYaml({0x1b93ae8, 0xc00013f1c0}, {0x0, 0x0}, {0xc0003ec0c0, 0x7, 0x0}, {0x28bff08, 0x0, 0x0}, ...) /home/circleci/go/src/github.com/nutanix-core/acs-karbonctl/cmd/karbonmgmt.go:213 +0x191 github.com/nutanix-core/acs-karbonctl/cmd.processKarbonMgmtYaml({0x1b93ae8, 0xc00013f1c0}, {0x190e83e, 0x16eeb40}, 0x1b29ac0, {0x0, 0x32}, {0x0, 0x2b}, {0xc0004dcea0, ...}, ...) /home/circleci/go/src/github.com/nutanix-core/acs-karbonctl/cmd/karbonmgmt_enable.go:483 +0x165 github.com/nutanix-core/acs-karbonctl/cmd.glob..func64(0x286ba80, {0xc00013d060, 0x2, 0x2}) /home/circleci/go/src/github.com/nutanix-core/acs-karbonctl/cmd/karbonmgmt_enable.go:426 +0x1c5 github.com/spf13/cobra.(*Command).execute(0x286ba80, {0xc00013d040, 0x2, 0x2}) /home/circleci/go/src/github.com/nutanix-core/acs-karbonctl/vendor/github.com/spf13/cobra/command.go:860 +0x5f8 github.com/spf13/cobra.(*Command).ExecuteC(0x2870580) /home/circleci/go/src/github.com/nutanix-core/acs-karbonctl/vendor/github.com/spf13/cobra/command.go:974 +0x3bc github.com/spf13/cobra.(*Command).Execute(...) /home/circleci/go/src/github.com/nutanix-core/acs-karbonctl/vendor/github.com/spf13/cobra/command.go:902 github.com/nutanix-core/acs-karbonctl/cmd.Execute() /home/circleci/go/src/github.com/nutanix-core/acs-karbonctl/cmd/root.go:44 +0x25 main.main() /home/circleci/go/src/github.com/nutanix-core/acs-karbonctl/main.go:19 +0x17 Rancher install 下載安裝cert-manager\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [ken@bastion rancher]$ wget https://github.com/jetstack/cert-manager/releases/download/v1.0.4/cert-manager.yaml [ken@bastion rancher]$ kubectl create ns cert-manager [ken@bastion rancher]$ kubectl apply -f cert-manager.yaml [ken@bastion rancher]$ kubectl get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-7455bdc9bd-jxjrs 1/1 Running 0 109s cert-manager-cainjector-5bcbb87487-j27cl 0/1 Error 4 (60s ago) 109s cert-manager-webhook-55b97fbb44-5xbsd 1/1 Running 0 109s 複製檔案過來 [ken@bastion rancher]$ ll total 3324 -rw-r--r--. 1 ken ken 1679688 Jan 16 09:44 cert-manager.crds.yaml -rw-r--r--. 1 ken ken 1701573 Dec 8 2021 cert-manager.yaml -rw-r--r--. 1 ken ken 262 Jan 16 09:44 rancher-service.yaml [ken@bastion rancher]$ kubectl apply --validate=false -f cert-manager.crds.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io configured customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io configured customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io configured customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io configured customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io configured customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io configured 安裝rancher\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 [ken@bastion rancher]$ kubectl create namespace cattle-system namespace/cattle-system created [ken@bastion rancher]$ kubectl apply -f rancher-service.yaml service/rancher-service created ### helm install [ken@bastion rancher]$ wget https://get.helm.sh/helm-v3.11.0-rc.2-linux-amd64.tar.gz [ken@bastion rancher]$ tar xvf helm-v3.10.3-linux-amd64.tar.gz linux-amd64/ linux-amd64/helm linux-amd64/LICENSE linux-amd64/README.md [ken@bastion linux-amd64]$ sudo cp helm /usr/local/bin/ [sudo] password for ken: [ken@bastion linux-amd64]$ helm version WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/ken/nke.cfg WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/ken/nke.cfg version.BuildInfo{Version:\u0026#34;v3.10.3\u0026#34;, GitCommit:\u0026#34;835b7334cfe2e5e27870ab3ed4135f136eecc704\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.18.9\u0026#34;} ##rancher install [ken@bastion rancher]$ helm repo add rancher-stable https://releases.rancher.com/server-charts/stable [ken@bastion rancher]$ helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=rancher.netfos.ken.lab --set replicas=1 [ken@bastion rancher]$ kubectl get all -n cattle-system NAME READY STATUS RESTARTS AGE pod/helm-operation-9bhx6 0/2 Completed 0 72s pod/helm-operation-h7rnz 0/2 Completed 0 25s pod/helm-operation-j8wx2 0/2 Completed 0 6s pod/rancher-687bc6596d-xf4pw 1/1 Running 0 2m38s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/rancher ClusterIP 172.19.196.248 \u0026lt;none\u0026gt; 80/TCP,443/TCP 2m39s service/rancher-service NodePort 172.19.29.29 \u0026lt;none\u0026gt; 443:31443/TCP 36m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/rancher 1/1 1 1 2m39s NAME DESIRED CURRENT READY AGE replicaset.apps/rancher-687bc6596d 1 1 1 2m39s browser to https://192.168.1.15:31443 改密碼 即可透過Rancher來查看叢集Status FIles https://portal.nutanix.com/page/documents/details?targetId=Files-v4_2:fil-file-server-create-wc-t.html#ntask_dgl_l3j_j5 https://portal.nutanix.com/page/documents/details?targetId=Files-v4_2:Files-v4_2\n安裝設定AD (optional 如要使用SMB就需有AD)\nDomain: netfos.ken.lab\n分為Storage Network (172.16.90.0)、Client Network (192.168.1.0)\nClient Network不能與CVM同網段\nVirtual IP用在Storage Network, Client 的IP, FSVM會自動HA\n先用LCM將File升級到最新\n網路Port圖 https://portal.nutanix.com/page/documents/details?targetId=Port-Reference:Files_port_auto_r.html\n架構 安裝AD 安裝File Server on PE https://portal.nutanix.com/page/documents/details?targetId=Files-v4_2:fil-file-server-create-wc-t.html#ntask_dgl_l3j_j5\n在PE的 File Server ADD,先確認有達到最低需求 設定FQDN、大小\n設定Client Network (192.168.1.0 要與CVM的網段不同) 設定DNS\u0026amp;NTP\n設定Storage Network (172.16.90.206-209) 設定目錄服務 SMB、NFS (SMB要AD) Summary \u0026amp; Protection Domain (Snapshot) 去PE的Setting查看fs被分配到什麼IP (192.168.1.26,81,97)\n並在DNS上註記 \u0026ndash; 使用auto update\n查看Takss 安裝完畢 可以點選console來查看File Server的狀態 有出現無法連到AD 需查找問題\u0026ndash; 將兩台DNS Server的註記都設定上去後即正常 新增Share資料夾 (SMB) 測試連線,點進去file-test即可看到mount path 在一台Windows上連線測試 ( 走的是192.168.1.0網段 )\n測試資料夾有出現，但要開權限\nhttps://portal.nutanix.com/page/documents/details?targetId=Files-v4_2:fil-fs-rbac-workflow-c.html![image-20230112112909758](https://kenkenny.synology.me:5543/images/2023/09/image-20230112112909758.png)\n查詢 https://portal.nutanix.com/page/documents/kbs/details?targetId=kA032000000TWZMCA4\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 nutanix@NTNX-22SH6M380125-C-CVM:172.16.90.213:~$ ncli fs ls Uuid : dcb7f7d3-1beb-423e-92d8-0004978542c8 Name : fs01 DNS Domain Name : netfos.ken.lab Version : 4.2.1 Container ID : 0005f1d0-ce0d-147b-16c3-7cc25505da5a::169299 Container Uuid : 5d3fd980-8192-4a8c-ab18-5fb605f58bae Virtual IP address : 172.16.90.206 DNS Server IP Addresses : 192.168.1.1, 192.168.1.100 NTP Servers : clock.stdtime.gov.tw, tick.stdtime.gov.tw, time.stdtime.gov.tw, tock.stdtime.gov.tw, watch.stdtime.gov.tw File server PD name : NTNX-fs01 File server PD status : Active File server status : Active AD Domain Name : netfos.ken.lab AD Username : administrator Preferred Domain Contr... : true Overwrite User Account : false AD Protocol Type : SMB Local Protocol Type : NFS Nfs version : NFSV3 Total Snapshot Space Used : 0 bytes Total Space Used : 808 KiB (827,392 bytes) Total Space Available : 1024 GiB (1,099,510,800,384 bytes) Total SMB Connections : 1 Storage User Capacity : 23.31 TiB (25,633,686,087,723 bytes) Size : 1 TiB (1,099,511,627,776 bytes) Data savings : 0 bytes Data reduction ratio : 1:1 Controller Num IOPs : 100 Controller IO Bandwidth : 751 Controller Avg IO Latency : 867 Network UUID : c5b9824f-4fba-4c01-9fb2-d02a88daf933 Default Gateway : 172.16.90.254 Subnet Mask : 255.255.255.0 Network Type : Internal IP Pool : 172.16.90.206 - 172.16.90.209 Network UUID : ac09299f-322e-4750-9183-6d106be2a0df Default Gateway : 192.168.1.1 Subnet Mask : 255.255.255.0 Network Type : External Nvm UUID : 5a7aef2d-2052-4a6b-935a-bfa46b493b98 Nvm ID : 0005f1d0-ce0d-147b-16c3-7cc25505da5a::0749859d-c8d7-4199-876f-92ee9215ba26 Nvm Name : NTNX-fs01-3 Memory In GiB : 12 Virtual CPUs : 4 Nvm IP Addresses : 192.168.1.26, 172.16.90.209 Nvm UUID : 275adefc-0e3f-4f26-8680-9dd7ff7223b4 Nvm ID : 0005f1d0-ce0d-147b-16c3-7cc25505da5a::a0a930a7-cafd-4dd3-b4f9-559a48a6db00 Nvm Name : NTNX-fs01-1 Memory In GiB : 12 Virtual CPUs : 4 Nvm IP Addresses : 192.168.1.97, 172.16.90.207 Nvm UUID : 3eec8305-8d7c-41ec-804d-9f217b41669c Nvm ID : 0005f1d0-ce0d-147b-16c3-7cc25505da5a::d3b41d19-1bf1-4c4b-8ece-e09d2b9dac2d Nvm Name : NTNX-fs01-2 Memory In GiB : 12 Virtual CPUs : 4 Nvm IP Addresses : 192.168.1.81, 172.16.90.208 最後問題點是FQDN帶了多個IP出來，直接用IP就可以連線\n並且資料會同步到其他FSVM上 可以針對檔案副檔名封鎖存取\nDashboard查看目前Share Folder的狀態\n測試NFS,新增NFS Share\n在Linux上安裝nfs-utils後mount測試\n1 2 3 4 5 [ken@bastion ~]$ sudo yum install nfs-utils [ken@bastion ~]$ sudo mount -t nfs 192.168.1.26:/nfs01 /var/nfs Created symlink /run/systemd/system/remote-fs.target.wants/rpc-statd.service → /usr/lib/systemd/system/rpc-statd.service. mount.nfs: access denied by server while mounting 192.168.1.26:/nfs01 出現 Access Denied 需要調整權限,Authentication調整為none,mount時要加sec=none 測試連線\n1 2 3 4 5 6 7 8 9 10 [ken@bastion ~]$ sudo mount -t nfs 192.168.1.26:/nfs01 /var/nfs -o sec=none [ken@bastion ~]$ cd /var/nfs/ [ken@bastion nfs]$ mkdir test [ken@bastion nfs]$ touch 123 [ken@bastion nfs]$ touch 321.txt [ken@bastion nfs]$ ll total 2 -rw-r--r--. 1 4294967294 4294967294 0 Jan 12 2023 123 -rw-r--r--. 1 4294967294 4294967294 0 Jan 12 2023 321.txt drwxr-xr-x. 2 4294967294 4294967294 2 Jan 12 15:10 test 測試阻擋副檔名*.txt 1 2 3 4 5 6 [ken@bastion nfs]$ touch 333.txt touch: cannot touch \u0026#39;333.txt\u0026#39;: Permission denied [ken@bastion nfs]$ rm 321.txt rm: cannot remove \u0026#39;321.txt\u0026#39;: Operation not permitted [ken@bastion nfs]$ rm 123 rm: cannot remove \u0026#39;123\u0026#39;: Operation not permitted 若新增時選擇的是Use \u0026ldquo;Distributed\u0026rdquo; share type instead of \u0026ldquo;Standard\u0026rdquo;\n提示要安裝MMC Plugin\n可以連線至該資料夾，但無法新增檔案 可以新增資料夾並在子資料夾內可新增檔案\nStandard vs Distributed\n1 2 Standard: 一般的Share Folder,實測資料都會同步到其他FSVM上,第一層資料夾即可開始使用。 Distributed: 多用來當作家目錄資料夾底下會放多個使用者的資料夾，可以有比較好的效能跟分流作用。 Distributed Standard File HA 1 2 3 4 5 6 7 Fail over for file server VMs (FSVMs). High Availability (HA) for Files insures that during a disruption of service a file server VM (FSVM), on clusters of two or more FSVMs, can fail over to another FSVM. High Availability is enabled by default on all clusters of two or more FSVMs. When an FSVM experiences an issue, Files reassigns the IP address of the FSVM to another FSVM in the cluster. The IP address of the out-of-service FSVM remains available. However, the shares and exports on the impacted FSVM are unavailable for several minutes during a failover. Affinity rules do not affect HA; multiple FSVMs can share a single host during a HA event. 情境模擬,目前DNS設定 資料夾連線狀態,兩邊都可以新增檔案 將ntnx-fs01-1關機 同樣路徑下還是可以進行檔案編輯、新增 192.168.1.97 還是ping得通。\nCSI https://github.com/nutanix/csi-plugin/blob/master/README.md\n分為Volume 跟Files Files 分為專用的Share以及Dynamic NFS 先確認worker node 有啟動nfs-server服務\n1 2 3 4 5 6 7 8 9 10 11 12 13 [nutanix@anthos-lab01-anthos-workerVm-0 ~]$ sudo systemctl enable nfs-server --now Created symlink /etc/systemd/system/multi-user.target.wants/nfs-server.service → /usr/lib/systemd/system/nfs-server.service. [nutanix@anthos-lab01-anthos-workerVm-0 ~]$ sudo systemctl status nfs-server ● nfs-server.service - NFS server and services Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled) Active: active (exited) since Fri 2023-07-14 09:00:10 UTC; 12s ago Process: 46413 ExecStart=/bin/sh -c if systemctl -q is-active gssproxy; then systemctl reload gssproxy ; fi (code=exited, status=0/SUCCESS) Process: 46400 ExecStart=/usr/sbin/rpc.nfsd (code=exited, status=0/SUCCESS) Process: 46398 ExecStartPre=/usr/sbin/exportfs -r (code=exited, status=0/SUCCESS) Main PID: 46413 (code=exited, status=0/SUCCESS) 7月 14 09:00:10 anthos-lab01-anthos-workerVm-0 systemd[1]: Starting NFS server and services... 7月 14 09:00:10 anthos-lab01-anthos-workerVm-0 systemd[1]: Started NFS server and services. Files Dynamic Prism 上有建立Files Server即可使用，不用新增Share\n建立Secret\n1 2 3 4 5 6 7 8 9 10 [nutanix@anthos-lab01-anthos-adminVm-0 ~]$ cat ntnx-secret74.yaml apiVersion: v1 kind: Secret metadata: name: ntnx-secret74 namespace: ntnx-system data: # base64 encoded prism-ip:prism-port:admin:password. # E.g.: echo -n \u0026#34;172.16.90.74:9440:admin:mypassword\u0026#34; | base64 key: MTcyLjE2LjkwLjc0Ojk0NDA6YWRtaW46TnV0YW5peC9MYWIxMjM= 建立Dynamic NFS Storageclass\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [nutanix@anthos-lab01-anthos-adminVm-0 ~]$ cat ntnx-secret74.yaml apiVersion: v1 kind: Secret metadata: name: ntnx-secret74 namespace: ntnx-system data: # base64 encoded prism-ip:prism-port:admin:password. # E.g.: echo -n \u0026#34;10.6.47.155:9440:admin:mypassword\u0026#34; | base64 key: MTcyLjE2LjkwLjc0Ojk0NDA6YWRtaW46TnV0YW5peC9MYWIxMjM= [nutanix@anthos-lab01-anthos-adminVm-0 ~]$ cat dyn-files.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ntnx-files provisioner: csi.nutanix.com parameters: dynamicProv: ENABLED nfsServerName: Files csi.storage.k8s.io/provisioner-secret-name: ntnx-secret74 csi.storage.k8s.io/provisioner-secret-namespace: ntnx-system storageType: NutanixFiles 建立測試pvc\n1 2 3 4 5 6 7 8 9 10 11 12 [nutanix@anthos-lab01-anthos-adminVm-0 ~]$ cat test-claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim2 spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: ntnx-files 驗證OK\n1 2 3 [nutanix@anthos-lab01-anthos-adminVm-0 ~]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim2 Bound pvc-a338991c-13f8-4c2f-811b-10911f543e97 10Gi RWX ntnx-files 93m 實際Prism 畫面 Anthos 畫面 Files Share Prism 建立Files Share 建立Storageclass 針對anthos 這個share\n1 2 3 4 5 6 7 8 9 10 11 12 13 [nutanix@anthos-lab01-anthos-adminVm-0 ~]$ cat ntnx-files.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ntnx-files-anthos annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;false\u0026#34; provisioner: csi.nutanix.com parameters: nfsServer: Files.nutanixlab.local nfsPath: anthos storageType: NutanixFiles reclaimPolicy: Delete 確認Storageclass \u0026ldquo;ntnx-files-anthos\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [nutanix@anthos-lab01-anthos-adminVm-0 ~]$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE anthos-system kubernetes.io/no-provisioner Delete WaitForFirstConsumer false 29d local-disks kubernetes.io/no-provisioner Delete WaitForFirstConsumer false 29d local-shared kubernetes.io/no-provisioner Delete WaitForFirstConsumer false 29d ntnx-files csi.nutanix.com Delete Immediate false 108m ntnx-files-anthos csi.nutanix.com Delete Immediate false 6s nutanix-volume (default) csi.nutanix.com Delete Immediate true 29d [nutanix@anthos-lab01-anthos-adminVm-0 ~]$ kubectl describe storageclass ntnx-files-anthos Name: ntnx-files-anthos IsDefaultClass: No Annotations: storageclass.kubernetes.io/is-default-class=false Provisioner: csi.nutanix.com Parameters: nfsPath=anthos,nfsServer=Files.nutanixlab.local,storageType=NutanixFiles AllowVolumeExpansion: \u0026lt;unset\u0026gt; MountOptions: \u0026lt;none\u0026gt; ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u0026lt;none\u0026gt; ","date":"2023-09-23T02:46:09+08:00","permalink":"https://wangken0129.github.io/p/karbonfiles/","title":"Nutanix Karbon\u0026Files"},{"content":"Nutanix上快速部署 Anthos 目標 : 透過 Nutanix 的 Calm 快速部署 Google Anthos Cluster並註冊到雲端，過程約一個小時， 並在Google Cloud Console 上來管理地端Anthos Cluster，達到混合雲的架構。\n除了Nutanix Calm外，同樣也可以使用Terraform來達到相同目的。\n環境準備 : 一座 Nutanix Cluster 含至少192GB 可用記憶體、768GB 硬碟可用空間、 6個 IPAM 給Anthos Cluster VM、3個Kubernetes用的IP\nPrism Central 啟用Calm服務\n一台 Calm DSL VM (啟動Calm DSL Container )\n一組 ssh key 給Anthos 部署的VM 使用\n其他準備：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 Prerequisites Before using any of the automation methods, make sure to meet the following requirements: Automation\tCalm: 3.0.0.2 or later A project with AHV account ~ or ~ Terraform: 0.13.x or later Nutanix provider 1.2.x or later Credentials\t(Calm only) SSH key. It must start with —BEGIN RSA PRIVATE KEY— Prism Element account with User Admin role Prism Central account with CRUD VM permissions Networking\tInternet connectivity AHV IPAM pool with minimum 6 IP addresses Kubernetes: Control plane VIP Ingress VIP Load balancing pool Nutanix\tPrism Element cluster: AHV: 20201105.1045 or later AOS: 5.19.1 or later iSCSI data service IP configured VLAN network with AHV IPAM configured Prism Central: 2020.11.0.1 or later Google Cloud\tA project with Owner role Project must have monitoring enabled (console) A service account (how-to) Role: Project Owner A private key: JSON format 版本資訊 : AOS Version: 6.5.2 Prism Central Version: 2023.1.01 Calm Version: 3.6.2 Calm DSL Version: 3.6.1.2023.03.15.commit.e753678 Anthos Version: 1.15.0 (v1.26.2-gke.1001) CentOS Version: 8.2.2004-20200611.2 參考資料 Anthos on AHV https://www.nutanix.dev/2021/04/26/anthos-clusters-on-ahv-getting-started/#calm-anthos-ahv\nCalm Blueprint https://github.com/nutanixdev/anthos-on-ahv/tree/main/calm\nService Account https://cloud.google.com/anthos/run/docs/securing/service-accounts\n架構圖 : Calm\nAnthos Cluster\nCalm 與 Jenkins整合的流程，日後可做為參考\nLab01 Info\n名稱 IP Prism Central 172.16.90.72 Prism Element 172.16.90.71 Prism iSCSI Data IP 172.16.90.73 Calm DSL VM 172.16.90.210 anthos-lab01-admin-vm 172.16.90.211-216 anthos-lab01-master1 172.16.90.211-216 anthos-lab01-master2 172.16.90.211-216 anthos-lab01-master3 172.16.90.211-216 anthos-lab01-worker1 172.16.90.211-216 anthos-lab01-worker2 172.16.90.211-216 Anthos cluster01 VIP 172.16.90.207 Ingress VIP 172.16.90.205 Loadbalncer IP 172.16.90.205-206 Lab02\n名稱 IP anthos-lab02-admin-vm 172.16.90.211-216 anthos-lab02-master0 172.16.90.217-223 anthos-lab02-master1 172.16.90.217-223 anthos-lab02-master2 172.16.90.217-223 anthos-lab02-worker0 172.16.90.217-223 anthos-lab02-worker1 172.16.90.217-223 anthos-lab02-worker2 172.16.90.217-223 Anthos cluster02 VIP 172.16.90.224 Ingress VIP 172.16.90.208 Loadbalncer IP 172.16.90.208-209 Calm DSL執行步驟 : Google Cloud Console : 登入 Google console https://console.cloud.google.com\n建立 Project \u0026amp; Enable Anthos 建立 Service Account \u0026amp;私密金鑰並上傳 Nutanix Cluster : 登入 Prism Central\n建立 Nutanix Project 並設定使用者、基礎環境\n取得blueprint、設定環境變數 (Calm DSL VM)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ cd ~/nutanix $ git clone https://github.com/mat0606/anthos-on-ahv $ cd anthos-on-ahv/calm/ $ vim blueprint.py Create Variables $ mkdir -p .local/secrets $ ls -l .local/secrets -rw-rw-r--. 1 nutanix nutanix 1 6月 14 07:03 gcloud_account -rw-r--r--. 1 nutanix nutanix 1 6月 14 07:04 gcloud_key -rw-rw-r--. 1 nutanix nutanix 11 6月 6 07:39 os2_key -rw-rw-r--. 1 nutanix nutanix 8 6月 6 07:39 os2_username -rw-------. 1 nutanix nutanix 2455 6月 9 04:19 os_key -rw-rw-r--. 1 nutanix nutanix 8 6月 6 06:57 os_username -rw-rw-r--. 1 nutanix nutanix 15 6月 6 06:57 pc_password -rw-rw-r--. 1 nutanix nutanix 6 6月 9 04:03 pc_username -rw-rw-r--. 1 nutanix nutanix 15 6月 6 06:57 pe_password -rw-rw-r--. 1 nutanix nutanix 6 6月 6 06:58 pe_username 啟動Calm DSL Container\n1 $ sudo docker run -v /home/nutanix/anthos-on-ahv:/home/centos/anthos-on-ahv:z -it ntnx/calm-dsl /bin/bash Calm DSL連線到Prism Central及該Project (Calm DSL Container)\n1 2 3 $ ls -l /home/centos/anthos-on-ahv $ calm init dsl $ calm show config Calm DSL 建立blueprint到Prism Central\n1 2 $ calm compile bp --name esun-bp --file /home/centos/anthos-on-ahv/calm/blueprint.py $ calm create bp --name esun-bp --file /home/centos/anthos-on-ahv/calm/blueprint.py 在Prism Central 的blueprint貼上ssh key\nCalm DSL 利用blueprint啟動Application\n1 $ calm launch bp \u0026#34;Anthos-AHV-DSL\u0026#34; --app_name anthos 等待約30~45分鐘即可看到Cluster建立完成\nAnthos-Cluster： 登入admin-VM取得kubeconfig確認Cluster\n1 2 $ SECRET_NAME=$(kubectl get serviceaccount google-cloud-console -o jsonpath=\u0026#39;{$.secrets[0].name}\u0026#39;) $ kubectl get secret ${SECRET_NAME} -o jsonpath=\u0026#39;{$.data.token}\u0026#39; | base64 --decode 驗證workload\n1 2 3 4 5 6 $ sudo yum -y install git $ git clone https://github.com/mat0606/K8S.git $ kubectl create ns wordpress $ kubectl -n wordpress create secret generic mysql-pass --from-literal=password=\u0026#39;Nutanix/4u\u0026#39; $ kubectl -n wordpress apply -f mysql-deployment.yaml $ kubectl -n wordpress get pvc Google Console 確認Cluster已有註冊\nTerraform 執行步驟： Google Cloud Console： 與Calm 執行步驟相同\n登入 Google console https://console.cloud.google.com\n建立 Project \u0026amp; Enable Anthos 建立 Service Account \u0026amp;私密金鑰並上傳 Nutanix Cluster： 只需準備IPAM的可用IP即可\nTerraform： 建立terraform.tfvars\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 echo \u0026#39;subnet_name = \u0026#34;\u0026lt;subnet_name\u0026gt;\u0026#34; anthos_version = \u0026#34;\u0026lt;anthos_version\u0026gt;\u0026#34; anthos_controlplane_vip = \u0026#34;\u0026lt;anthos_controlplane_vip\u0026gt;\u0026#34; anthos_ingress_vip = \u0026#34;\u0026lt;anthos_ingress_vip\u0026gt;\u0026#34; anthos_lb_addresspool = \u0026#34;\u0026lt;anthos_lb_addresspool\u0026gt;\u0026#34; anthos_cluster_name = \u0026#34;\u0026lt;anthos_cluster_name\u0026gt;\u0026#34; google_application_credentials_path = \u0026#34;\u0026lt;google_application_credentials_path\u0026gt;\u0026#34; amount_of_anthos_worker_vms = \u0026#34;\u0026lt;amount_of_anthos_worker_vms\u0026gt;\u0026#34; ntnx_pc_username = \u0026#34;\u0026lt;ntnx_pc_username\u0026gt;\u0026#34; ntnx_pc_password = \u0026#34;\u0026lt;ntnx_pc_password\u0026gt;\u0026#34; ntnx_pc_ip = \u0026#34;\u0026lt;ntnx_pc_ip\u0026gt;\u0026#34; ntnx_pe_storage_container = \u0026#34;\u0026lt;ntnx_pe_storage_container\u0026gt;\u0026#34; ntnx_pe_username = \u0026#34;\u0026lt;ntnx_pe_username\u0026gt;\u0026#34; ntnx_pe_password = \u0026#34;\u0026lt;ntnx_pe_password\u0026gt;\u0026#34; ntnx_pe_ip = \u0026#34;\u0026lt;ntnx_pe_ip\u0026gt;\u0026#34; ntnx_pe_dataservice_ip = \u0026#34;\u0026lt;ntnx_pe_dataservice_ip\u0026gt;\u0026#34;\u0026#39; \u0026gt; terraform.tfvars $ chmod 0600 terraform.tfvars 執行terraform初始化並產生plan\n1 2 $ terraform init $ terraform plan 部署anthos cluster\n1 $ terraform apply Anthos-Cluster： 部署完成後登入admin-vm確認Cluster\n取得kubeconfig以登入Google Cloud Console\n1 2 $ SECRET_NAME=$(kubectl get serviceaccount google-cloud-console -o jsonpath=\u0026#39;{$.secrets[0].name}\u0026#39;) $ kubectl get secret ${SECRET_NAME} -o jsonpath=\u0026#39;{$.data.token}\u0026#39; | base64 --decode Google Console 確認Cluster已有註冊\nScale out 測試，修改terraform.tfvars的worker數量\n1 $ terraform apply 登入admin-vm 確認Scale out 完成\nGoogle與Calm bp完整步驟 Anthos Create Service Accounts 進入IAM與管理,點選project (Anthos-Test-202303) 點選建立服務帳戶 給擁有者的權限\n郵件地址複製 netfos@anthos-test-202303.iam.gserviceaccount.com\n點選該服務帳戶,新增金鑰 儲存金鑰 Enable Anthos Prism Central \u0026amp; Calm IPAM設定IP Pool\nCalm建立Project 上傳github 上的 blueprint 點開Blueprint\n調整參數可以安裝時再輸入即可 (有藍色的人代表安裝時可以輸入)\nVM網卡要先設定好 Credential 要先輸入 CRED_OS ssh-keygen\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ssh-keygen -t rsa -f ~/.ssh/KEY_FILENAME -C USERNAME -b 2048 wangken@wangken-MAC ~ % ssh-keygen -t rsa -f ~/.ssh/anthos_key -C nutanix -b 2048 Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /Users/wangken/.ssh/anthos_key Your public key has been saved in /Users/wangken/.ssh/anthos_key.pub The key fingerprint is: SHA256:vBzHezjT7zfmG5ZWQX+eS78TLDHmfNj7g1Rki4PxV5Q nutanix The key\u0026#39;s randomart image is: +---[RSA 2048]----+ | oo| | . .Eo| | + +.=| | . . . B =+| | S o + Xoo| | . + + *.==| | o = + +*+| | + oo*+| | .==B| +----[SHA256]-----+ wangken@wangken-MAC .ssh % ls |grep anthos anthos_key.pub anthos_key anthos sa private key\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 wangken@wangken-MAC notes % cat anthos-test-202303-privatekey.json { \u0026#34;type\u0026#34;: \u0026#34;service_account\u0026#34;, \u0026#34;project_id\u0026#34;: \u0026#34;anthos-test-202303\u0026#34;, \u0026#34;private_key_id\u0026#34;: \u0026#34;b76e04e16af19f535e185f2f36bae2be8038c363\u0026#34;, \u0026#34;private_key\u0026#34;: \u0026#34;-----BEGIN PRIVATE KEY----- XXXX\\n-----END PRIVATE KEY-----\\n\u0026#34;, \u0026#34;client_email\u0026#34;: \u0026#34;netfos@anthos-test-202303.iam.gserviceaccount.com\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;110466507071833816563\u0026#34;, \u0026#34;auth_uri\u0026#34;: \u0026#34;https://accounts.google.com/o/oauth2/auth\u0026#34;, \u0026#34;token_uri\u0026#34;: \u0026#34;https://oauth2.googleapis.com/token\u0026#34;, \u0026#34;auth_provider_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/oauth2/v1/certs\u0026#34;, \u0026#34;client_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/robot/v1/metadata/x509/netfos%40anthos-test-202303.iam.gserviceaccount.com\u0026#34; } 輸入完後按save , 執行 填入資訊 IP 資訊 IP 資訊 Ingress VIP 要在load balancing pool 內 Deploy 自動化部屬 Manage可以查看目前的部署狀態 部署完成 登入Admin VM 取得Secret並在Anthos Console登入\n1 2 3 4 5 6 7 8 9 10 11 12 wangken@wangken-MAC ~ % ssh -i .ssh/anthos_key nutanix@172.16.90.206 Activate the web console with: systemctl enable --now cockpit.socket Last login: Tue Mar 14 09:21:38 2023 from 172.16.90.72 [nutanix@anthos-netfos1-anthos-adminVm-0 ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION anthos-netfos1-anthos-controlvm-0 Ready control-plane,master 57m v1.21.5-gke.1200 anthos-netfos1-anthos-controlvm-1 Ready control-plane,master 53m v1.21.5-gke.1200 anthos-netfos1-anthos-controlvm-2 Ready control-plane,master 53m v1.21.5-gke.1200 anthos-netfos1-anthos-workervm-0 Ready \u0026lt;none\u0026gt; 50m v1.21.5-gke.1200 anthos-netfos1-anthos-workervm-1 Ready \u0026lt;none\u0026gt; 50m v1.21.5-gke.1200 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 [nutanix@anthos-netfos1-anthos-adminVm-0 anthos-netfos1]$ pwd /home/nutanix/baremetal/bmctl-workspace/anthos-netfos1 [nutanix@anthos-netfos1-anthos-adminVm-0 anthos-netfos1]$ cat anthos-netfos1-kubeconfig apiVersion: v1 clusters: - cluster: certificate-authority-data: XXXX server: https://172.16.90.212:443 name: anthos-netfos1 contexts: - context: cluster: anthos-netfos1 user: anthos-netfos1-admin name: anthos-netfos1-admin@anthos-netfos1 current-context: anthos-netfos1-admin@anthos-netfos1 kind: Config preferences: {} users: - name: anthos-netfos1-admin user: client-certificate-data: XXX client-key-data: XXXX 登入Anthos Console用憑證下面兩行指令得出的base64 碼\n1 2 $SECRET_NAME=$(kubectl get serviceaccount google-cloud-console -o jsonpath=\u0026#39;{$.secrets[0].name}\u0026#39;) $kubectl get secret ${SECRET_NAME} -o jsonpath=\u0026#39;{$.data.token}\u0026#39; | base64 --decode 登入完成 Workload Marketplace部署測試程式 設定並部署 在地端查看namespace狀態\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 [nutanix@anthos-netfos1-anthos-adminVm-0 ~]$ kubectl get all -n harbor NAME READY STATUS RESTARTS AGE pod/harbor-1-chartmuseum-967bb456b-2vmjk 1/1 Running 0 3m18s pod/harbor-1-core-7956bd9f54-t4jcw 1/1 Running 1 3m18s pod/harbor-1-database-0 1/1 Running 0 3m18s pod/harbor-1-deployer-rq8m5 0/1 Completed 0 3m59s pod/harbor-1-exporter-5c86cc7b6d-s78kb 1/2 CrashLoopBackOff 5 3m18s pod/harbor-1-jobservice-9898c468b-5bhzp 1/1 Running 3 3m18s pod/harbor-1-nginx-66d4ccd87-7p7q2 1/1 Running 0 3m18s pod/harbor-1-notary-server-7d7b4b598d-85nm5 1/1 Running 2 3m18s pod/harbor-1-notary-signer-5d59d79f4-4tkn4 1/1 Running 2 3m18s pod/harbor-1-portal-66f4b7478b-5xv5b 1/1 Running 0 3m18s pod/harbor-1-redis-0 1/1 Running 0 3m18s pod/harbor-1-registry-67bc689779-bvw8m 2/2 Running 0 3m18s pod/harbor-1-trivy-0 1/1 Running 0 3m18s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/harbor ClusterIP 172.31.202.245 \u0026lt;none\u0026gt; 80/TCP,443/TCP,4443/TCP 3m19s service/harbor-1-chartmuseum ClusterIP 172.31.6.127 \u0026lt;none\u0026gt; 80/TCP 3m19s service/harbor-1-core ClusterIP 172.31.17.10 \u0026lt;none\u0026gt; 80/TCP,8001/TCP 3m19s service/harbor-1-database ClusterIP 172.31.204.175 \u0026lt;none\u0026gt; 5432/TCP 3m19s service/harbor-1-exporter ClusterIP 172.31.107.19 \u0026lt;none\u0026gt; 8001/TCP 3m19s service/harbor-1-jobservice ClusterIP 172.31.128.4 \u0026lt;none\u0026gt; 80/TCP,8001/TCP 3m19s service/harbor-1-notary-server ClusterIP 172.31.240.223 \u0026lt;none\u0026gt; 4443/TCP 3m19s service/harbor-1-notary-signer ClusterIP 172.31.221.6 \u0026lt;none\u0026gt; 7899/TCP 3m19s service/harbor-1-portal ClusterIP 172.31.38.67 \u0026lt;none\u0026gt; 80/TCP 3m19s service/harbor-1-redis ClusterIP 172.31.38.202 \u0026lt;none\u0026gt; 6379/TCP 3m19s service/harbor-1-registry ClusterIP 172.31.16.8 \u0026lt;none\u0026gt; 5000/TCP,8080/TCP,8001/TCP 3m18s service/harbor-1-trivy ClusterIP 172.31.70.71 \u0026lt;none\u0026gt; 8080/TCP 3m18s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/harbor-1-chartmuseum 1/1 1 1 3m18s deployment.apps/harbor-1-core 1/1 1 1 3m18s deployment.apps/harbor-1-exporter 0/1 1 0 3m18s deployment.apps/harbor-1-jobservice 1/1 1 1 3m18s deployment.apps/harbor-1-nginx 1/1 1 1 3m18s deployment.apps/harbor-1-notary-server 1/1 1 1 3m18s deployment.apps/harbor-1-notary-signer 1/1 1 1 3m18s deployment.apps/harbor-1-portal 1/1 1 1 3m18s deployment.apps/harbor-1-registry 1/1 1 1 3m18s NAME DESIRED CURRENT READY AGE replicaset.apps/harbor-1-chartmuseum-967bb456b 1 1 1 3m18s replicaset.apps/harbor-1-core-7956bd9f54 1 1 1 3m18s replicaset.apps/harbor-1-exporter-5c86cc7b6d 1 1 0 3m18s replicaset.apps/harbor-1-jobservice-9898c468b 1 1 1 3m18s replicaset.apps/harbor-1-nginx-66d4ccd87 1 1 1 3m18s replicaset.apps/harbor-1-notary-server-7d7b4b598d 1 1 1 3m18s replicaset.apps/harbor-1-notary-signer-5d59d79f4 1 1 1 3m18s replicaset.apps/harbor-1-portal-66f4b7478b 1 1 1 3m18s replicaset.apps/harbor-1-registry-67bc689779 1 1 1 3m18s NAME READY AGE statefulset.apps/harbor-1-database 1/1 3m18s statefulset.apps/harbor-1-redis 1/1 3m18s statefulset.apps/harbor-1-trivy 1/1 3m18s NAME COMPLETIONS DURATION AGE job.batch/harbor-1-deployer 1/1 44s 3m59s 測試redmine\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [nutanix@anthos-netfos1-anthos-adminVm-0 redmine]$ kubectl get all -n redmine NAME READY STATUS RESTARTS AGE pod/redmine-58cfd55655-qthfz 1/1 Running 0 38m pod/redmine-postgres-deployment-6b8f85c97b-72vkl 1/1 Running 0 106m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/redmine-postgres-service NodePort 172.31.163.134 \u0026lt;none\u0026gt; 5432:31432/TCP 106m service/redmine-service ClusterIP 172.31.151.36 172.16.90.213 80/TCP 38m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/redmine 1/1 1 1 38m deployment.apps/redmine-postgres-deployment 1/1 1 1 106m NAME DESIRED CURRENT READY AGE replicaset.apps/redmine-58cfd55655 1 1 1 38m replicaset.apps/redmine-postgres-deployment-6b8f85c97b 1 1 1 106m IngressVIP : 172.16.90.213 anthos畫面 StorageClass \u0026ndash;\u0026gt; default block storage\n預設會是用Nutanix的iSCSI block storage，如果要使用NFS則要另外設定\n","date":"2023-09-23T02:40:52+08:00","permalink":"https://wangken0129.github.io/p/anthos_on_nutanix/","title":"Anthos_on_Nutanix"},{"content":"製作Blog Part2 前篇寫了 Typora + Picgo + NAS 的筆記製作過程，\n接下來要將做的筆記用Hugo + Github Page + Stack Theme 的方式做成一個靜態網站。\n預先準備 Homebrew\nHomebrew 安裝參考：https://brew.sh\nGit\nGit 安裝參考：https://git-scm.com/book/zh-tw/v2/開始-Git-安裝教學\nHugo 簡介 Hugo是一個由Go語言開發的靜態網頁產生器，\n老實說跟另一個叫Hexo的產生器比起來，Hexo的主題、中文文件比較多，\n以部落格的好看性來說應該是Hexo較優，而且是由台灣人開發出來的，\n但Hexo的安裝可能還要考慮Node.js的相依性等，我在安裝時遇到了一些問題懶得去解決，\n所以最後就選擇Hugo，相對來說安裝上就簡單很多，反正簡單、好用就好，\n可以參考下方的主題來選擇自己要的產生器。\nHugo主題: https://themes.gohugo.io/tags/blog\nHexo主題: https://hexo.io/themes/index.html\nInstall Hugo 官方文件：https://gohugo.io/installation/macos/\n確認Homebrew版本，推薦Mac用戶都要安裝這個套件軟體\n1 2 wangken@wangken-MAC ~ % brew -v Homebrew 4.1.11 透過Homebrew安裝Hugo，裝好後確認hugo版本\n1 2 3 4 wangken@wangken-MAC ~ % brew install hugo wangken@wangken-MAC ~ % hugo version hugo v0.118.2-da7983ac4b94d97d776d7c2405040de97e95c03d+extended darwin/arm64 BuildDate=2023-08-31T11:23:51Z VendorInfo=brew ( Option ) 升級 hugo 版本跟部落格版本，遇到版本支援問題時會需要升級 ( 2024-12-13 更新 )\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 wangken@wangken-MAC myblog % brew upgrade hugo Running `brew update --auto-update`... ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:303bed4c7fc431a685db3c3c151d873740114adbdccd23762ea2d1e39ea78f47 ######################################################################### 100.0% ==\u0026gt; Pouring portable-ruby-3.3.6.arm64_big_sur.bottle.tar.gz ==\u0026gt; Auto-updated Homebrew! You have 56 outdated formulae and 1 outdated cask installed. ==\u0026gt; Migrating formula python-certifi to certifi ==\u0026gt; Unlinking python-certifi ==\u0026gt; Moving python-certifi versions to /opt/homebrew/Cellar/certifi ==\u0026gt; Relinking certifi ... wangken@wangken-MAC myblog % hugo version hugo v0.139.4+extended+withdeploy darwin/arm64 BuildDate=2024-12-09T17:45:23Z VendorInfo=brew 模板升級 wangken@wangken-MAC hugo-blog % hugo mod get -u github.com/CaiJimmy/hugo-theme-stack/v3 WARN deprecated: site config key paginate was deprecated in Hugo v0.128.0 and will be removed in a future release. Use pagination.pagerSize instead. hugo: downloading modules … hugo: collected modules in 3638 ms go: downloading github.com/CaiJimmy/hugo-theme-stack/v3 v3.29.0 go: downloading github.com/CaiJimmy/hugo-theme-stack v2.6.0+incompatible go: upgraded github.com/CaiJimmy/hugo-theme-stack/v3 v3.21.0 =\u0026gt; v3.29.0 wangken@wangken-MAC hugo-blog % hugo mod tidy WARN deprecated: site config key paginate was deprecated in Hugo v0.128.0 and will be removed in a future release. Use pagination.pagerSize instead. 遇到的錯誤訊息 ( 在 Github 的 Action 內，Build 的時候發生錯誤，只能等原模板的作者更新，或自行修改 ）\nTest Hugo 開啟終端機，新增一個hugo的網站目錄\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 wangken@wangken-MAC ~ % hugo new site test-hugo Congratulations! Your new Hugo site was created in /Users/wangken/test-hugo. Just a few more steps... 1. Change the current directory to /Users/wangken/test-hugo. 2. Create or install a theme: - Create a new theme with the command \u0026#34;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026#34; - Install a theme from https://themes.gohugo.io/ 3. Edit hugo.toml, setting the \u0026#34;theme\u0026#34; property to the theme name. 4. Create new content with the command \u0026#34;hugo new content \u0026lt;SECTIONNAME\u0026gt;/\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026#34;. 5. Start the embedded web server with the command \u0026#34;hugo server --buildDrafts\u0026#34;. See documentation at https://gohugo.io/. cd進入專案目錄 test-hugo\n1 2 3 4 5 6 7 8 9 10 11 12 13 wangken@wangken-MAC ~ % cd test-hugo wangken@wangken-MAC test-hugo % ls -l total 8 drwxr-xr-x 3 wangken staff 96 9 20 12:05 archetypes drwxr-xr-x 2 wangken staff 64 9 20 12:05 assets drwxr-xr-x 2 wangken staff 64 9 20 12:05 content drwxr-xr-x 2 wangken staff 64 9 20 12:05 data -rw-r--r-- 1 wangken staff 83 9 20 12:05 hugo.toml drwxr-xr-x 2 wangken staff 64 9 20 12:05 i18n drwxr-xr-x 2 wangken staff 64 9 20 12:05 layouts drwxr-xr-x 2 wangken staff 64 9 20 12:05 static drwxr-xr-x 2 wangken staff 64 9 20 12:05 themes 啟動hugo，輸入hugo serve\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 wangken@wangken-MAC test-hugo % hugo serve Watching for changes in /Users/wangken/test-hugo/{archetypes,assets,content,data,i18n,layouts,static} Watching for config changes in /Users/wangken/test-hugo/hugo.toml Start building sites … hugo v0.118.2-da7983ac4b94d97d776d7c2405040de97e95c03d+extended darwin/arm64 BuildDate=2023-08-31T11:23:51Z VendorInfo=brew WARN found no layout file for \u0026#34;html\u0026#34; for kind \u0026#34;taxonomy\u0026#34;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. WARN found no layout file for \u0026#34;html\u0026#34; for kind \u0026#34;home\u0026#34;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. | EN -------------------+----- Pages | 3 Paginator pages | 0 Non-page files | 0 Static files | 0 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Built in 6 ms Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop 將 http://localhost:1313 貼到瀏覽器上，確認有顯示出 Page Not Found\n表示Hugo運作正常，會顯示這個訊息是因為Hugo會需要主題來運作，而且也沒有任何的文章發布\n終端機輸入Control+C 結束hugo網站，到此hugo的安裝就完成了，\n後續我會直接套用Stack主題的模板，如要其他模板可以參考其他文章來建立。\nGithub Pages 申請Github帳號 進入Github官網，點右上角的Sign up，接著輸入必要的資訊即可\nhttps://github.com/\n套用模板與基本設定 點開Stack 主題模板的連結，下方有說明可以參考\nhttps://github.com/CaiJimmy/hugo-theme-stack-starter\n點擊 Use this template \u0026ndash;\u0026gt; Create a new repository\n在Repository name的地方要輸入 『 github帳號.github.io 』，\n這樣github page的連結才會是根網站像 https://wangken0129.github.io\n如果輸入其他名稱連結會變成 https://wangken0129.github.io/其他名稱\n匯入完成後會看到有兩個branch，master跟gh-pages，\n其他兩個是如果上一步驟有勾選 Include all branches就會複製過來\n複製過來後原文件說明是可以直接在綠色 Code地方新增一個codespace，這是可以直接在github上面直接去編輯配置文件\n但為了後續管理文件方便，我選擇回到筆電終端機上建立一個資料夾來當作工作目錄。\n再來要設定好 github page 所用的branch，最點選右邊的Settings \u0026ndash;\u0026gt; Pages\n根據下圖的設定，Branch那邊要選擇 gh-pages\n接下來可以在github上先新增一個token，用來在筆電上跟github做溝通，\n點選右上角的圓圈\u0026ndash;\u0026gt; Settings\n拉到最下面有個Developer settings\nPersonal access tokens \u0026ndash;\u0026gt; Tokens (classic) 新增一組Token，\n這組Token請小心保存，而且只會出現一次，可以根據需求設定到期時間、名稱等\n設定模板 回到剛剛的Github Repository上面，點選 Code \u0026ndash;\u0026gt; SSH 並複製下方那串SSH\n開啟筆電終端機，新增一個資料夾作為工作目錄，並進入此資料夾\n1 2 3 4 wangken@wangken-MAC ~ % mkdir workspace wangken@wangken-MAC ~ % cd workspace wangken@wangken-MAC workspace % git 登入，最後面使用 authentication token，就是上面複製的token\n1 2 3 4 5 6 7 8 9 wangken@wangken-MAC workspace % gh auth login ? What account do you want to log into? GitHub.com ? You\u0026#39;re already logged into github.com. Do you want to re-authenticate? Yes ? What is your preferred protocol for Git operations? HTTPS ? Authenticate Git with your GitHub credentials? Yes ? How would you like to authenticate GitHub CLI? [Use arrows to move, type to filter] Login with a web browser \u0026gt; Paste an authentication token 用git初始化工作目錄，並用 -b 指定branch為master\n1 2 wangken@wangken-MAC workspace % git init -b master 已初始化空的 Git 版本庫於 /Users/wangken/workspace/.git/ git 新增遠端Github的Repository，並用 git remote -v 確認\n1 2 3 4 5 wangken@wangken-MAC workspace % git remote add origin git@github.com:wangken0129/wangken0129.github.io.git wangken@wangken-MAC workspace % git remote -v origin\tgit@github.com:wangken0129/wangken0129.github.io.git (fetch) origin\tgit@github.com:wangken0129/wangken0129.github.io.git (push) 將遠端的branch 拉下來，此時就會看到github上有的資料夾以及檔案\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 wangken@wangken-MAC workspace % git pull git@github.com:wangken0129/wangken0129.github.io.git remote: Enumerating objects: 247, done. remote: Counting objects: 100% (247/247), done. remote: Compressing objects: 100% (128/128), done. remote: Total 247 (delta 109), reused 195 (delta 68), pack-reused 0 接收物件中: 100% (247/247), 141.87 KiB | 470.00 KiB/s, 完成. 處理 delta 中: 100% (109/109), 完成. 來自 github.com:wangken0129/wangken0129.github.io * branch HEAD -\u0026gt; FETCH_HEAD wangken@wangken-MAC workspace % ls -l total 32 -rw-r--r-- 1 wangken staff 1066 9 20 15:16 LICENSE -rw-r--r-- 1 wangken staff 2874 9 20 15:16 README.md drwxr-xr-x 4 wangken staff 128 9 20 15:16 assets drwxr-xr-x 3 wangken staff 96 9 20 15:16 config drwxr-xr-x 6 wangken staff 192 9 20 15:16 content -rw-r--r-- 1 wangken staff 130 9 20 15:16 go.mod -rw-r--r-- 1 wangken staff 199 9 20 15:16 go.sum drwxr-xr-x 3 wangken staff 96 9 20 15:16 static 在config/_default/config.toml內的有幾項需要先設定\nbaseurl = ”https://Github帳號名稱.github.io“\ntitle = \u0026ldquo;自定義\u0026rdquo;\ndefaultContentLanguage = \u0026ldquo;zh-tw\u0026rdquo;\ndisqusShortname = \u0026ldquo;disqus建立的Shortname\u0026rdquo; (這是部落格底下的留言板功能，可先忽略)\n可以去官網註冊帳號，並建立一個Site，官網： https://disqus.com/profile/signup/intent/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 wangken@wangken-MAC workspace % vi config/_default/config.toml # Change baseurl before deploy baseurl = \u0026#34;https://wangken0129.github.io\u0026#34; languageCode = \u0026#34;en-us\u0026#34; paginate = 5 title = \u0026#34;Ken\u0026#39;s blog\u0026#34; # Theme i18n support # Available values: en, fr, id, ja, ko, pt-br, zh-cn, zh-tw, es, de, nl, it, th, el, uk, ar defaultContentLanguage = \u0026#34;zh-tw\u0026#34; # Set hasCJKLanguage to true if DefaultContentLanguage is in [zh-cn ja ko] # This will make .Summary and .WordCount behave correctly for CJK languages. hasCJKLanguage = false # Change it to your Disqus shortname before using disqusShortname = \u0026#34;wangken0129\u0026#34; 基本設定這些就可以建立網站了，後續的一些設定可以參考官方文件，幾乎都可以在config/_default 目錄底下去設定\nStack主題的官方文件：https://stack.jimmycai.com/\n後續如果有文章想新增的話，根據此主題的架構要新增目錄在content/post裡面\n並依照下方的目錄結構去新增，並且md檔案前面會需要加上front matter\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 文章結構，每篇都會有index.md content └── post └── my-first-post ├── index.md ├── image1.png └── image2.png # front matter範例，以此文章為範例，放在md檔案最上面前後都要加 ``` ``` # 文章標題 title: 製作Blog Part2 # 文章描述 description: Hugo + Github Page + Stack Theme # slug slug: blog-part2 # 時間，固定格式UTC+08:00 date: 2023-09-20T07:15:00+08:00 # 文章分類 categories: - Knowledge Base Category # 文章標籤 tags: - Blog - KB weight: 1 # You can add weight to some posts to override the default sorting (date descending) ``` 設定完成後即可push到github上面，指令如下：\n1 2 3 git add . git commit -m \u0026#34;msg\u0026#34; git push --set-upstream origin master 成功push後可以點進Github 上的Repository \u0026ndash;\u0026gt; Actions\n套用此模板的好處之一，就是已經自動幫你建立好workflows，\n只要在電腦編輯好後再push上去，有任何更動都會幫你套用到網站\n同時也會自動更新主題的一些Bug\n連線確認網站OK\n後記 完成部落格的設定後，再來就要把之前做的筆記套用Front Matter放上來跟之後的撰寫了，\n針對Hugo的一些配置檔的調整，我會使用Visual Studio Code來做編輯跟調整，\n因為針對.toml檔案的編輯還是比較友善，\n像下方的params.toml，我有去修改[sidebar]下方的值，\n其他筆記的撰寫我就還是用Typora，左邊可以分為檔案的目錄檢視，\n或是切換到大綱檢視，分為文章裡面的大小標題等等，\n最後因為會頻繁地做git push的動作，所以我也參考網路的Script來自動化的push，\n希望最後是能變成從寫完後就全部都自動化，有的話再寫上來分享。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/bin/sh #Put this script outside the workspace folder # If a command fails then the deploy stops set -e printf \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\u0026#34; # Go To workspace folder cd workspace # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site $(date)\u0026#34; if [ -n \u0026#34;$*\u0026#34; ]; then msg=\u0026#34;$*\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master # come back cd .. ","date":"2023-09-20T08:15:00+08:00","permalink":"https://wangken0129.github.io/p/blog-part2/","title":"製作Blog Part2"},{"content":"製作Blog Part1 把過程記錄下來，未來若有更換電腦還可以照著操作復原回來。\n原本我只有使用Typora做本地紀錄，有需要再上傳Github或是匯出成 PDF or Word 分享給別人，\n後來覺得有時候要用手機查看不太方便，有嘗試使用Notion因為還有手機App可使用，\n但是匯入文件的時候出現不少問題故作罷，幾經測試，覺得Github Page+Hugo看起來是比較簡單好上手。\nPart1 是Typora作為筆記軟體，並用Picgo上傳圖片到自架的NAS當作圖床空間。\nPart2 是透過Hugo + Github Page來做靜態網站，以存放Typora的筆記。\nPart3 是自動化腳本。\nTypora 我使用Typora作為我的Markdown筆記軟體，操作簡單而且可以匯出很多格式。\nInstall Typora 從官網下載下來安裝即可\n官網: https://typora.io/\nSetting Typora 有三個步驟:\n輸入授權 or 破解(?)\n針對圖片的存放設定，一開始我是使用如下圖的設定，\n複製圖片至Typora時會存放到上層路徑的assets資料夾，後來因為方便性改為上傳到自架的NAS上作為圖床。 外觀配置，官網有很多主題可以使用，只要把下載下來的主題複製到主題資料夾即可。\n完成後就可以開始使用Typora了。\nNAS自架圖床 這步驟如果沒有NAS或是使用其他圖床空間可以省略，\n會使用NAS自架圖床也是剛好有NAS，而且Imgur前陣子在大掃除，\n其他的圖床空間也難免會有大掃除或是停止的狀況，自架也只建議小型的Blog使用，\n如果有大量的圖片需求，還是建議用雲端的像Amazon S3等等。\nNAS設定 DDNS OR DNS 還有憑證網路上有很多參考文件此處就不多加贅述，此處使用的是Synology DDNS 憑證自動更新\nWeb Station Install 新增共用資料夾 /www/wwwroot/blog/images 建議再新增一個使用者設定權限專門使用此資料夾 安裝Web Station 網頁服務 \u0026ndash;\u0026gt; 新增靜態網站，指定主目錄為剛剛新增的資料夾 新增網頁入口，對應剛剛新增的網頁服務，並設定好Port號，建議使用HTTPS 丟一張圖片到 /www/wwwroot/blog/images 用網頁連線測試看是否可以讀取 FTP 設定 開啟FTP服務，設定FTPS、Port號，安全性考量不勾選FTP 設定指定使用者根目錄，安全考量不啟用匿名登入 Picgo 配置文件\n參考原文: https://zhuanlan.zhihu.com/p/382702959\nPicgo: https://picgo.github.io/PicGo-Core-Doc/zh/guide/config.html\nftp-uploader: https://github.com/imba97/picgo-plugin-ftp-uploader/tree/master\nftp-basic: https://www.npmjs.com/package/basic-ftp\nInstall Picgo 首先要先安裝Node.js，下載安裝或是用brew install node\nhttps://nodejs.org/en/download\n安裝好後確認版本\n1 2 3 4 wangken@wangken-MAC ~ % npm -v 9.6.7 wangken@wangken-MAC ~ % node -v v18.17.1 使用npm安裝Picgo\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 wangken@wangken-MAC ~ % npm install picgo -g wangken@wangken-MAC ~ % picgo -v 1.5.6 wangken@wangken-MAC ~ % picgo --help Usage: picgo [options] [command] Options: -v, --version output the version number -d, --debug debug mode -s, --silent silent mode -c, --config \u0026lt;path\u0026gt; set config path -p, --proxy \u0026lt;url\u0026gt; set proxy for uploading -h, --help display help for command Commands: install|add [options] \u0026lt;plugins...\u0026gt; install picgo plugin uninstall|rm \u0026lt;plugins...\u0026gt; uninstall picgo plugin update [options] \u0026lt;plugins...\u0026gt; update picgo plugin set|config \u0026lt;module\u0026gt; [name] configure config of picgo modules upload|u [input...] upload, go go go use [module] use modules of picgo init [options] \u0026lt;template\u0026gt; [project] create picgo plugin\u0026#39;s development templates i18n [lang] change picgo language help [command] display help for command 安裝好後再新增ftp-uploader\n1 2 3 4 wangken@wangken-MAC ~ % picgo install picgo-plugin-ftp-uploader # added 3 packages in 1s # [PicGo SUCCESS]: 插件安装成功 Setting Picgo 修改Picgo配置文件，預設在~/.picgo/config.json\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 wangken@wangken-MAC ~ % picgo set uploader ? Choose a(n) uploader ftp-uploader ? imba97 kenkenny ? D:/ftpUploaderConfig.json /Users/wangken/.picgo/ftpUploaderConfig.json [PicGo SUCCESS]: Configure config successfully! wangken@wangken-MAC ~ % cat ~/.picgo/config.json { \u0026#34;picBed\u0026#34;: { \u0026#34;uploader\u0026#34;: \u0026#34;ftp-uploader\u0026#34;, \u0026#34;current\u0026#34;: \u0026#34;ftp-uploader\u0026#34;, \u0026#34;ftp-uploader\u0026#34;: { \u0026#34;site\u0026#34;: \u0026#34;kenkenny\u0026#34;, \u0026#34;configFile\u0026#34;: \u0026#34;/Users/wangken/.picgo/ftpUploaderConfig.json\u0026#34; } }, \u0026#34;picgoPlugins\u0026#34;: { \u0026#34;picgo-plugin-ftp-uploader\u0026#34;: true } }% 新增~/.picgo/ftpUploaderConfig.json 這裡我多了一個secure的選項，安裝好後預設是沒有這個參數 因為我是使用FTPS，所以後面的配置檔案也要新增secure的參數，其餘請自行修改NAS對應的路徑及連結\n1 2 3 4 5 6 7 8 9 10 11 12 13 wangken@wangken-MAC .picgo % cat ftpUploaderConfig.json { \u0026#34;kenkenny\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;https://nas.domain:xxxx\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/images/{year}/{month}/{fullName}\u0026#34;, \u0026#34;uploadPath\u0026#34;: \u0026#34;/wwwroot/blog/images/{year}/{month}/{fullName}\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;nas.domain\u0026#34;, \u0026#34;port\u0026#34;: portnumber, \u0026#34;username\u0026#34;: \u0026#34;ftp-user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;ftp-user-password\u0026#34;, \u0026#34;secure\u0026#34;: true } } 修改 ~/.picgo/node_modules/picgo-plugin-ftp-uploader/dist/index.js\n在await client.access的地方新增 secure: config.secure\n詳細配置可以Google \u0026ldquo;ftp-basic\u0026rdquo; 就會有文件可以參考了\n1 2 3 4 5 6 7 8 9 10 wangken@wangken-MAC dist % vim index.js await client.access({ host: config.host, port: config.port, user: config.username, password: config.password, secure: config.secure }); 以上配置完成，不用重開機直接進入Typora修改圖片上傳的設定\n驗證Typora + Picgo + NAS 首先修改Typora的圖片設定，改為自訂命令，輸入 picgo upload 接著新增一個md檔案，複製一張圖片進去即可看到圖片上傳成功\nTypora: 點擊圖片即可看到上傳後的路徑\nNAS: 點選連結或是進入NAS的File Station確認圖片 以上就是Typora+Picgo上傳圖片到NAS上的設定，下一篇再來進入部落格的製作。\n","date":"2023-09-19T07:10:00+08:00","permalink":"https://wangken0129.github.io/p/blog-part1/","title":"製作Blog Part1"},{"content":"Hello World Description This is a blog for my labs and knowledge bases.\nHappy engineering.\n這是Ken的部落格，用來放我的Lab跟知識庫。\n當個快樂的工程師。\nThis is a test shell script Generate certificate script\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash # Create certificate given a specific url name if [ $# -ne 1 ] then echo \u0026#34;No arguments supplied or too many arguments\u0026#34; echo \u0026#34;Usage: $0 domain_name\u0026#34; exit 1 fi MYNAME=$1 echo \u0026#34;Generating a private key...for $MYNAME\u0026#34; openssl genrsa -out $MYNAME.key 2048 echo \u0026#34;Generating a CSR...for $MYNAME\u0026#34; openssl req -new -key $MYNAME.key \\ -out $MYNAME.csr \\ -subj \u0026#34;/C=US/ST=NC/L=Raleigh/O=RedHat/OU=RHT/CN=$MYNAME\u0026#34; echo \u0026#34;Generating a certificate...for $MYNAME\u0026#34; openssl x509 -req -days 366 -in \\ $MYNAME.csr -signkey \\ $MYNAME.key \\ -out $MYNAME.crt ","date":"2023-09-19T05:10:00+08:00","permalink":"https://wangken0129.github.io/p/hello-world/","title":"Hello World"}]